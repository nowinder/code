{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db3153e7-9f29-4d3d-9c2b-1f04c83a2da2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 提取图片到list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8acdf0ae-198a-4554-a4c4-6cd058fef0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取图片到list\n",
    "import os\n",
    "import numpy as np\n",
    "import imageio.v2 as imageio\n",
    "def getimg(dir, j):\n",
    "    input_dir = dir\n",
    "    img_list = []\n",
    "    for i in range(1,1201):\n",
    "        filename = f'{i+j}.tif'\n",
    "        img_path = os.path.join(input_dir,filename)\n",
    "        img = imageio.imread(img_path)\n",
    "        img = img.astype(np.float32)\n",
    "        img = img / 255.0\n",
    "        img_list.append(img)\n",
    "        # print(i)\n",
    "    return img_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d706665-25d0-4ffa-aac0-82d619ef6cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '/data/caizhizheng/2D/v2/data1/data/'\n",
    "img1_list = getimg(img_path, j=0)\n",
    "img_path = '/data/caizhizheng/2D/v2/data2/data/'\n",
    "img2_list = getimg(img_path, j=900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "374e3017-c46c-4e52-b3e9-d1dec058b23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_path = '/data/caizhizheng/2D/v2/label/label/'\n",
    "label_list = getimg(label_path,j=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f131c9-64fb-44fe-876d-777c000cb0c7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### onehot处理，整理slices到数据集并保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63c6de61-8998-497e-bd99-237bcd1d1fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def onehot(img):\n",
    "    img = tf.cast(img, tf.int32)\n",
    "    img_one_hot = tf.one_hot(img, depth=2, on_value=1.0, off_value=0.0, axis=-1)\n",
    "    return img_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "867a02bd-8fab-4d15-8d06-74a63b3004de",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = np.stack((img1_list,img2_list),axis = -1)\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(img_list)\n",
    "label_dataset = tf.data.Dataset.from_tensor_slices(label_list)\n",
    "label_dataset = label_dataset.map(onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59aba1da-c0b7-48d9-a93d-20b809e33f12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# image_dataset.cardinality()\n",
    "label_dataset.cardinality().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1e5d7d3-0654-46c1-bd26-a46d66bef0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data to save time when train\n",
    "tf.data.experimental.save(image_dataset, './dataset/image_exped/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8be3e1d-62fd-4dcb-af97-75991bb67bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.data.experimental.save(label_dataset, './dataset/label_exped/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73207b65-7c89-40a7-8d3b-04a881b71d60",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cd844f4-bbaf-476d-ba83-3322ff940425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model predict OK\n",
      "平均汉明距离为：0.3258\n",
      "平均杰卡德相似系数为：0.9987\n",
      "平均Dice相似系数为：0.9993\n",
      "平均误差为：0.5986%\n"
     ]
    }
   ],
   "source": [
    "# %load unet_exped.py\n",
    "import os\n",
    "import shutil\n",
    "from unittest import result\n",
    "import skimage.io as io\n",
    "import skimage.transform as trans\n",
    "from keras.optimizer_v2.adam import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as keras\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.saving.utils_v1.mode_keys import is_train\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import imageio.v2 as imageio\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.layers import BatchNormalization, Conv2D, Activation, Dropout, AveragePooling2D, concatenate, \\\n",
    "    GlobalAveragePooling2D, MaxPooling2D, Dense, Input\n",
    "from keras.regularizers import l2\n",
    "import keras.backend as K\n",
    "import zipfile\n",
    "import evalu\n",
    "import time\n",
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "# tf.config.experimental.set_memory_growth(gpus[1], True)\n",
    "# tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "# Read datasets\n",
    "def getimg(dir,j):\n",
    "    input_dir = dir\n",
    "    img_list = []\n",
    "    for i in range(1+j,1201+j):\n",
    "        filename = f'{i}.tif'\n",
    "        img_path = os.path.join(input_dir,filename)\n",
    "        img = imageio.imread(img_path)\n",
    "        img = img.astype(np.float32)\n",
    "        img = img / 255.0\n",
    "        img_list.append(img)\n",
    "    return img_list\n",
    "\n",
    "def onehot(img):\n",
    "    img = tf.cast(img, tf.int32)\n",
    "    img_one_hot = tf.one_hot(img, depth=2, on_value=1.0, off_value=0.0, axis=-1)\n",
    "    return img_one_hot\n",
    "\n",
    "# def add_channel(img): \n",
    "#     img = tf.expand_dims(img, axis=-1) \n",
    "#     return img\n",
    "# Draw loss curve\n",
    "def plot_history(history, result_dir, prefix):\n",
    "    \"\"\"\n",
    "    将训练与验证的accuracy与loss画出来\n",
    "    \"\"\"\n",
    "    plt.plot(history.history['accuracy'], marker='.')\n",
    "    plt.plot(history.history['val_accuracy'], marker='.')\n",
    "    plt.title('model accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.grid()\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend(['acc', 'val_acc'], loc='upper right')\n",
    "\t# plt.show()\n",
    "    plt.savefig(result_dir + 'unet_ace' + prefix + '.png')\n",
    "    plt.close()\n",
    "\n",
    "    plt.plot(history.history['loss'], marker='.')\n",
    "    plt.plot(history.history['val_loss'], marker='.')\n",
    "    plt.title('model loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.yscale(\"log\")\n",
    "    plt.grid()\n",
    "    plt.legend(['loss', 'val_loss'], loc='upper right')\n",
    "    # plt.show()\n",
    "    plt.savefig(result_dir + 'unet_loss'+ prefix +'.png')\n",
    "    plt.close()\n",
    "\n",
    "    # x = history.history['loss']\n",
    "    # np.savetxt('D:/pycharm/up_down_code/loss_picture/unet_train_loss.txt', x, fmt='%f')\n",
    "    # y = history.history['val_loss']\n",
    "    # np.savetxt('D:/pycharm/up_down_code/loss_picture/unet_val_loss.txt', y, fmt='%f')hb\n",
    "\n",
    "\n",
    "def Conv_Block(input_tensor, filters, bottleneck=False, weight_decay=1e-4):\n",
    "    \"\"\"    封装卷积层\n",
    "\n",
    "    :param input_tensor: 输入张量\n",
    "    :param filters: 卷积核数目\n",
    "    :param bottleneck: 是否使用bottleneck\n",
    "    :param dropout_rate: dropout比率\n",
    "    :param weight_decay: 权重衰减率\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    concat_axis = 1 if K.image_data_format() == 'channel_first' else -1  # 确定格式\n",
    "\n",
    "    x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5)(input_tensor)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # if bottleneck:\n",
    "    #     # 使用bottleneck进行降维\n",
    "    #     inter_channel = filters\n",
    "    #     x = Conv2D(inter_channel, (1, 1),\n",
    "    #                kernel_initializer='he_normal',\n",
    "    #                padding='same', use_bias=False,\n",
    "    #                kernel_regularizer=l2(weight_decay))(x)\n",
    "    #     x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5)(x)\n",
    "    #     x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters, (3, 3), kernel_initializer='he_normal', padding='same', use_bias=False)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def dens_block(input_tensor, nb_filter):\n",
    "    x1 = Conv_Block(input_tensor, nb_filter)\n",
    "    add1 = concatenate([x1, input_tensor], axis=-1)\n",
    "    x2 = Conv_Block(add1, nb_filter)\n",
    "    add2 = concatenate([x1, input_tensor, x2], axis=-1)\n",
    "    x3 = Conv_Block(add2, nb_filter)\n",
    "    return x3\n",
    "\n",
    "\n",
    "from keras.regularizers import l2\n",
    "\n",
    "\n",
    "# model definition\n",
    "def unet(input_shape=(128, 128, 2)):\n",
    "    # tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "    inputs = Input(input_shape)\n",
    "    # input1=Input(shape=(128,128,1))\n",
    "    # input2=Input(shape=(128,128,1))\n",
    "    # inputs=Concatenate(axis=-1)([input1,input2])\n",
    "    # inputs = Input(shape\n",
    "    # x  = Conv2D(32, 1, strides=1, activation='relu', padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    x = Conv2D(32, 7, kernel_initializer='he_normal', padding='same', strides=1, use_bias=False,\n",
    "               kernel_regularizer=l2(1e-4))(inputs)\n",
    "    # down first\n",
    "    down1 = dens_block(x, nb_filter=64)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(down1)  # 256\n",
    "    # down second\n",
    "    down2 = dens_block(pool1, nb_filter=64)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(down2)  # 128\n",
    "    # down third\n",
    "    down3 = dens_block(pool2, nb_filter=128)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(down3)  # 64\n",
    "    # down four\n",
    "    down4 = dens_block(pool3, nb_filter=256)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(down4)  # 32\n",
    "    # center\n",
    "    conv5 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool4)\n",
    "    conv5 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    # up first\n",
    "    up6 = UpSampling2D(size=(2, 2))(drop5)\n",
    "    # up6 = UpSampling2D(size=(2, 2))(drop5)\n",
    "    add6 = concatenate([down4, up6], axis=3)\n",
    "    up6 = dens_block(add6, nb_filter=256)\n",
    "    # up second\n",
    "    up7 = UpSampling2D(size=(2, 2))(up6)\n",
    "    # up7 = UpSampling2D(size=(2, 2))(conv6)\n",
    "    add7 = concatenate([down3, up7], axis=3)\n",
    "    up7 = dens_block(add7, nb_filter=128)\n",
    "    # up third\n",
    "    up8 = UpSampling2D(size=(2, 2))(up7)\n",
    "    # up8 = UpSampling2D(size=(2, 2))(conv7)\n",
    "    add8 = concatenate([down2, up8], axis=-1)\n",
    "    up8 = dens_block(add8, nb_filter=64)\n",
    "    # up four\n",
    "    up9 = UpSampling2D(size=(2, 2))(up8)\n",
    "    add9 = concatenate([down1, up9], axis=-1)\n",
    "    up9 = dens_block(add9, nb_filter=64)\n",
    "    # output\n",
    "    conv10 = Conv2D(32, 7, strides=1, activation='relu', padding='same', kernel_initializer='he_normal')(up9)\n",
    "    conv10 = Conv2D(2, 1, activation='softmax')(conv10)\n",
    "    # model = Model(inputs=[input1,input2], outputs=conv10)\n",
    "    model = Model(inputs=inputs, outputs=conv10)\n",
    "    # print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "# define Huber loss\n",
    "def huber_loss(y_true, y_pred):\n",
    "    return tf.losses.huber_loss(y_true, y_pred, delta=0.01)\n",
    "\n",
    "\n",
    "def simm_loss(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_pred - y_true)) + 0.01 * K.mean(K.abs(y_pred))\n",
    "    # return tf.abs(tf.norm(y_pred - y_true))/tf.norm(y_true)\n",
    "    \n",
    "# smooth = 1. # 用于防止分母为0.\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true) # 将 y_true 拉伸为一维.\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection) / (K.sum(y_true_f * y_true_f) + K.sum(y_pred_f * y_pred_f))\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1. - dice_coef(y_true, y_pred)\n",
    "\n",
    "# Define the learning rate attenuation value\n",
    "def scheduler(epoch):\n",
    "    if epoch % 10 == 0 and epoch != 0:\n",
    "        lr = K.get_value(model.optimizer.lr)\n",
    "        K.set_value(model.optimizer.lr, lr * 0.1)\n",
    "        print(\"lr change to {}\".format(lr * 0.1))\n",
    "    return K.get_value(model.optimizer.lr)\n",
    "\n",
    "import random\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "  0.001,\n",
    "  decay_steps=100,\n",
    "  decay_rate=1,\n",
    "  staircase=False)\n",
    "        \n",
    "def make_zip(source_dir, output_name):\n",
    "    zipf = zipfile.ZipFile(output_name, 'w')\n",
    "    prelen = len(os.path.dirname(source_dir))\n",
    "    for parent, _, filenames in os.walk(source_dir):\n",
    "        for filename in filenames:\n",
    "            pathfile = os.path.join(parent, filename)\n",
    "            arcname = pathfile[prelen:].strip(os.path.sep)     #相对路径\n",
    "            zipf.write(pathfile, arcname)\n",
    "        zipf.close()\n",
    "\n",
    "def train(model, model_savename, train_dataset, validation_dataset):\n",
    "    model.compile(optimizer=Adam(learning_rate=lr_schedule), loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "    # reduce_lr = LearningRateScheduler(scheduler)\n",
    "    # reduce_lr = LearningRateScheduler(lschedule)\n",
    "    model_checkpoint = ModelCheckpoint('/code/save_model/exped/'+model_savename, monitor='loss', verbose=1,\n",
    "                                       save_best_only=True)\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=100)\n",
    "    Board = tf.keras.callbacks.TensorBoard(log_dir=\"/output/logs\")\n",
    "    history = model.fit(train_dataset.repeat(),\n",
    "                                  steps_per_epoch=100,\n",
    "                                  epochs=1000,\n",
    "                                  validation_data=validation_dataset,\n",
    "                                #   validation_steps=50,\n",
    "                                  callbacks=[model_checkpoint,\n",
    "                                             early_stop,Board\n",
    "                                             # Board\n",
    "                                             ])\n",
    "    plot_history(history, '/code/loss_picture/extended/',prefix='_exped')\n",
    "    return model\n",
    "    \n",
    "def test(model, test_dataset, output_path):\n",
    "    mask = model.predict(test_dataset)\n",
    "    mask = tf.argmax(mask, axis=-1)\n",
    "    mask = tf.keras.backend.eval(mask)\n",
    "    mask = (mask * 255).astype(np.uint8)\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    for j in range(0, 1200):\n",
    "        cv2.imwrite(output_path+ '%d.tif' %(j+1), mask[j])\n",
    "    print('model predict OK')\n",
    "        \n",
    "BATCH_SIZE=64\n",
    "plot_path = '/code/loss_picture/extended/'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # is_train = False # you can change this to False if you want to test only\n",
    "    output_path = '/data/caizhizheng/2D/v2/exped_output/'\n",
    "    label_path = '/data/caizhizheng/2D/v2/label/label/'\n",
    "    model = unet(input_shape=(128, 128, 2))\n",
    "    img_path = '/data/caizhizheng/2D/v2/data1/data/'\n",
    "    img1_list = getimg(img_path, j=0)\n",
    "    img_path = '/data/caizhizheng/2D/v2/data2/data/'\n",
    "    img2_list = getimg(img_path, j=900)\n",
    "    # label_list = getimg(label_path,j=0)\n",
    "    img_list = np.stack((img1_list,img2_list),axis = -1)\n",
    "    image_dataset = tf.data.Dataset.from_tensor_slices(img_list)\n",
    "    # label_dataset = tf.data.Dataset.from_tensor_slices(label_list)\n",
    "    # label_dataset = label_dataset.map(onehot)\n",
    "    # 使用事先保存的数据集会造成内存泄漏？？这是为什么，以前使用并不会\n",
    "    # image_dataset = tf.data.experimental.load(\"dataset/image_exped\")\n",
    "    # label_dataset = tf.data.experimental.load(\"dataset/label_exped\")\n",
    "    # x = next(iter(image_dataset.batch(image_dataset.cardinality().numpy())))\n",
    "    # y = next(iter(label_dataset.batch(label_dataset.cardinality().numpy())))\n",
    "    # train_dataset = tf.data.Dataset.zip((image_dataset, label_dataset))\n",
    "    # train_dataset = train_dataset.cache().shuffle(train_dataset.cardinality())\n",
    "\n",
    "    # validation_dataset = train_dataset.take(50).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    # train_dataset = train_dataset.skip(50).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    model_savename = 'exped.keras'\n",
    "    # train = train(model, model_savename, train_dataset, validation_dataset)\n",
    "    model.load_weights('./save_model/exped/exped.keras')\n",
    "    \n",
    "    image_dataset = image_dataset.batch(BATCH_SIZE)\n",
    "    test(model, image_dataset, output_path)\n",
    "    name = 'error2' + '_postCrV' + '.csv'\n",
    "    error_path = './errors/'\n",
    "    name = 'error_exped' + '.csv'\n",
    "    name = os.path.join(error_path, name)\n",
    "    evalu.main(output_dir=output_path, label_dir=label_path, name=name, oder=np.array(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7e7a553-cfda-497e-9851-948a47656e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (mask[1199]==mask[1]).all()\n",
    "image_dataset.cardinality().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e2d711a-b17d-43e1-86d3-776973872997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平均汉明距离为：5283.4650\n",
      "平均杰卡德相似系数为：0.0646\n",
      "平均Dice相似系数为：0.0987\n",
      "平均误差为：188.3010%\n"
     ]
    }
   ],
   "source": [
    "import evalu\n",
    "import numpy as np\n",
    "import os\n",
    "error_path = './errors/'\n",
    "name = 'error_exped' + '.csv'\n",
    "name = os.path.join(error_path, name)\n",
    "output_path = '/data/caizhizheng/2D/v2/exped_output/'\n",
    "label_path = '/data/caizhizheng/2D/v2/label/label/'\n",
    "evalu.main(output_dir=output_path, label_dir=label_path, name=name, oder=np.array(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15db6acb-8422-4f50-a70d-700e262dba98",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = model.predict(image_dataset)\n",
    "mask = tf.argmax(mask, axis=-1)\n",
    "mask = tf.keras.backend.eval(mask)\n",
    "mask = (mask * 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bc2aae-32c8-480e-a92f-48002adf8024",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(1,1201,1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c08c9df6-ad85-43dd-80c7-55def6eb32ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/data/caizhizheng/2D/validataset2/'\n",
    "label_path = save_path + 'label/'\n",
    "label = os.listdir(label_path)\n",
    "number = []\n",
    "for f in label:\n",
    "     number.append(f.split('.')[0])\n",
    "number = np.array(number, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82dfc9cf-8fa8-402e-9358-ea4f150093a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e26c20c-a418-46e5-ae74-a592f7daead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "oder = np.array([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f3518cd-0fa9-43cc-ac0d-c3a59f8d7cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oder.all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6179b30c-8f5c-4bea-993b-6450db685de0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05c69c0d-7920-43da-876b-71ab6156b177",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model predict OK\n",
      "平均汉明距离为：264.6000\n",
      "平均杰卡德相似系数为：0.9038\n",
      "平均Dice相似系数为：0.9215\n",
      "平均误差为：18.5349%\n"
     ]
    }
   ],
   "source": [
    "# %load unet_exped.py\n",
    "import os\n",
    "import shutil\n",
    "from unittest import result\n",
    "import skimage.io as io\n",
    "import skimage.transform as trans\n",
    "from keras.optimizer_v2.adam import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as keras\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.saving.utils_v1.mode_keys import is_train\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import imageio.v2 as imageio\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.layers import BatchNormalization, Conv2D, Activation, Dropout, AveragePooling2D, concatenate, \\\n",
    "    GlobalAveragePooling2D, MaxPooling2D, Dense, Input\n",
    "from keras.regularizers import l2\n",
    "import keras.backend as K\n",
    "import zipfile\n",
    "import evalu\n",
    "import time\n",
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "# tf.config.experimental.set_memory_growth(gpus[1], True)\n",
    "# tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "# Read datasets\n",
    "def getimg(dir,j):\n",
    "    input_dir = dir\n",
    "    img_list = []\n",
    "    for i in range(1+j,101+j):\n",
    "        filename = f'{i}.tif'\n",
    "        img_path = os.path.join(input_dir,filename)\n",
    "        img = imageio.imread(img_path)\n",
    "        img = img.astype(np.float32)\n",
    "        img = img / 255.0\n",
    "        img_list.append(img)\n",
    "    return img_list\n",
    "\n",
    "def onehot(img):\n",
    "    img = tf.cast(img, tf.int32)\n",
    "    img_one_hot = tf.one_hot(img, depth=2, on_value=1.0, off_value=0.0, axis=-1)\n",
    "    return img_one_hot\n",
    "\n",
    "# def add_channel(img): \n",
    "#     img = tf.expand_dims(img, axis=-1) \n",
    "#     return img\n",
    "# Draw loss curve\n",
    "def plot_history(history, result_dir, prefix):\n",
    "    \"\"\"\n",
    "    将训练与验证的accuracy与loss画出来\n",
    "    \"\"\"\n",
    "    plt.plot(history.history['accuracy'], marker='.')\n",
    "    plt.plot(history.history['val_accuracy'], marker='.')\n",
    "    plt.title('model accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.grid()\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend(['acc', 'val_acc'], loc='upper right')\n",
    "\t# plt.show()\n",
    "    plt.savefig(result_dir + 'unet_ace' + prefix + '.png')\n",
    "    plt.close()\n",
    "\n",
    "    plt.plot(history.history['loss'], marker='.')\n",
    "    plt.plot(history.history['val_loss'], marker='.')\n",
    "    plt.title('model loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.yscale(\"log\")\n",
    "    plt.grid()\n",
    "    plt.legend(['loss', 'val_loss'], loc='upper right')\n",
    "    # plt.show()\n",
    "    plt.savefig(result_dir + 'unet_loss'+ prefix +'.png')\n",
    "    plt.close()\n",
    "\n",
    "    # x = history.history['loss']\n",
    "    # np.savetxt('D:/pycharm/up_down_code/loss_picture/unet_train_loss.txt', x, fmt='%f')\n",
    "    # y = history.history['val_loss']\n",
    "    # np.savetxt('D:/pycharm/up_down_code/loss_picture/unet_val_loss.txt', y, fmt='%f')hb\n",
    "\n",
    "\n",
    "def Conv_Block(input_tensor, filters, bottleneck=False, weight_decay=1e-4):\n",
    "    \"\"\"    封装卷积层\n",
    "\n",
    "    :param input_tensor: 输入张量\n",
    "    :param filters: 卷积核数目\n",
    "    :param bottleneck: 是否使用bottleneck\n",
    "    :param dropout_rate: dropout比率\n",
    "    :param weight_decay: 权重衰减率\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    concat_axis = 1 if K.image_data_format() == 'channel_first' else -1  # 确定格式\n",
    "\n",
    "    x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5)(input_tensor)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # if bottleneck:\n",
    "    #     # 使用bottleneck进行降维\n",
    "    #     inter_channel = filters\n",
    "    #     x = Conv2D(inter_channel, (1, 1),\n",
    "    #                kernel_initializer='he_normal',\n",
    "    #                padding='same', use_bias=False,\n",
    "    #                kernel_regularizer=l2(weight_decay))(x)\n",
    "    #     x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5)(x)\n",
    "    #     x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters, (3, 3), kernel_initializer='he_normal', padding='same', use_bias=False)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def dens_block(input_tensor, nb_filter):\n",
    "    x1 = Conv_Block(input_tensor, nb_filter)\n",
    "    add1 = concatenate([x1, input_tensor], axis=-1)\n",
    "    x2 = Conv_Block(add1, nb_filter)\n",
    "    add2 = concatenate([x1, input_tensor, x2], axis=-1)\n",
    "    x3 = Conv_Block(add2, nb_filter)\n",
    "    return x3\n",
    "\n",
    "\n",
    "from keras.regularizers import l2\n",
    "\n",
    "\n",
    "# model definition\n",
    "def unet(input_shape=(128, 128, 2)):\n",
    "    # tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "    inputs = Input(input_shape)\n",
    "    # input1=Input(shape=(128,128,1))\n",
    "    # input2=Input(shape=(128,128,1))\n",
    "    # inputs=Concatenate(axis=-1)([input1,input2])\n",
    "    # inputs = Input(shape\n",
    "    # x  = Conv2D(32, 1, strides=1, activation='relu', padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    x = Conv2D(32, 7, kernel_initializer='he_normal', padding='same', strides=1, use_bias=False,\n",
    "               kernel_regularizer=l2(1e-4))(inputs)\n",
    "    # down first\n",
    "    down1 = dens_block(x, nb_filter=64)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(down1)  # 256\n",
    "    # down second\n",
    "    down2 = dens_block(pool1, nb_filter=64)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(down2)  # 128\n",
    "    # down third\n",
    "    down3 = dens_block(pool2, nb_filter=128)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(down3)  # 64\n",
    "    # down four\n",
    "    down4 = dens_block(pool3, nb_filter=256)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(down4)  # 32\n",
    "    # center\n",
    "    conv5 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool4)\n",
    "    conv5 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    # up first\n",
    "    up6 = UpSampling2D(size=(2, 2))(drop5)\n",
    "    # up6 = UpSampling2D(size=(2, 2))(drop5)\n",
    "    add6 = concatenate([down4, up6], axis=3)\n",
    "    up6 = dens_block(add6, nb_filter=256)\n",
    "    # up second\n",
    "    up7 = UpSampling2D(size=(2, 2))(up6)\n",
    "    # up7 = UpSampling2D(size=(2, 2))(conv6)\n",
    "    add7 = concatenate([down3, up7], axis=3)\n",
    "    up7 = dens_block(add7, nb_filter=128)\n",
    "    # up third\n",
    "    up8 = UpSampling2D(size=(2, 2))(up7)\n",
    "    # up8 = UpSampling2D(size=(2, 2))(conv7)\n",
    "    add8 = concatenate([down2, up8], axis=-1)\n",
    "    up8 = dens_block(add8, nb_filter=64)\n",
    "    # up four\n",
    "    up9 = UpSampling2D(size=(2, 2))(up8)\n",
    "    add9 = concatenate([down1, up9], axis=-1)\n",
    "    up9 = dens_block(add9, nb_filter=64)\n",
    "    # output\n",
    "    conv10 = Conv2D(32, 7, strides=1, activation='relu', padding='same', kernel_initializer='he_normal')(up9)\n",
    "    conv10 = Conv2D(2, 1, activation='softmax')(conv10)\n",
    "    # model = Model(inputs=[input1,input2], outputs=conv10)\n",
    "    model = Model(inputs=inputs, outputs=conv10)\n",
    "    # print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "# define Huber loss\n",
    "def huber_loss(y_true, y_pred):\n",
    "    return tf.losses.huber_loss(y_true, y_pred, delta=0.01)\n",
    "\n",
    "\n",
    "def simm_loss(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_pred - y_true)) + 0.01 * K.mean(K.abs(y_pred))\n",
    "    # return tf.abs(tf.norm(y_pred - y_true))/tf.norm(y_true)\n",
    "    \n",
    "# smooth = 1. # 用于防止分母为0.\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true) # 将 y_true 拉伸为一维.\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection) / (K.sum(y_true_f * y_true_f) + K.sum(y_pred_f * y_pred_f))\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1. - dice_coef(y_true, y_pred)\n",
    "\n",
    "# Define the learning rate attenuation value\n",
    "def scheduler(epoch):\n",
    "    if epoch % 10 == 0 and epoch != 0:\n",
    "        lr = K.get_value(model.optimizer.lr)\n",
    "        K.set_value(model.optimizer.lr, lr * 0.1)\n",
    "        print(\"lr change to {}\".format(lr * 0.1))\n",
    "    return K.get_value(model.optimizer.lr)\n",
    "\n",
    "import random\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "  0.001,\n",
    "  decay_steps=100,\n",
    "  decay_rate=1,\n",
    "  staircase=False)\n",
    "        \n",
    "def make_zip(source_dir, output_name):\n",
    "    zipf = zipfile.ZipFile(output_name, 'w')\n",
    "    prelen = len(os.path.dirname(source_dir))\n",
    "    for parent, _, filenames in os.walk(source_dir):\n",
    "        for filename in filenames:\n",
    "            pathfile = os.path.join(parent, filename)\n",
    "            arcname = pathfile[prelen:].strip(os.path.sep)     #相对路径\n",
    "            zipf.write(pathfile, arcname)\n",
    "        zipf.close()\n",
    "\n",
    "def train(model, model_savename, train_dataset, validation_dataset):\n",
    "    model.compile(optimizer=Adam(learning_rate=lr_schedule), loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "    # reduce_lr = LearningRateScheduler(scheduler)\n",
    "    # reduce_lr = LearningRateScheduler(lschedule)\n",
    "    model_checkpoint = ModelCheckpoint('/code/save_model/exped/'+model_savename, monitor='loss', verbose=1,\n",
    "                                       save_best_only=True)\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=100)\n",
    "    Board = tf.keras.callbacks.TensorBoard(log_dir=\"/output/logs\")\n",
    "    history = model.fit(train_dataset.repeat(),\n",
    "                                  steps_per_epoch=100,\n",
    "                                  epochs=1000,\n",
    "                                  validation_data=validation_dataset,\n",
    "                                #   validation_steps=50,\n",
    "                                  callbacks=[model_checkpoint,\n",
    "                                             early_stop,Board\n",
    "                                             # Board\n",
    "                                             ])\n",
    "    plot_history(history, '/code/loss_picture/extended/',prefix='_exped')\n",
    "    return model\n",
    "    \n",
    "def test(model, test_dataset, output_path):\n",
    "    mask = model.predict(test_dataset)\n",
    "    mask = tf.argmax(mask, axis=-1)\n",
    "    mask = tf.keras.backend.eval(mask)\n",
    "    mask = (mask * 255).astype(np.uint8)\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    for j in range(0, 100):\n",
    "        cv2.imwrite(output_path+ '%d.tif' %(j+1), mask[j])\n",
    "    print('model predict OK')\n",
    "        \n",
    "BATCH_SIZE=64\n",
    "plot_path = '/code/loss_picture/extended/'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # is_train = False # you can change this to False if you want to test only\n",
    "    # output_path = '/data/caizhizheng/2D/v2/exped_output/'\n",
    "    # label_path = '/data/caizhizheng/2D/v2/label/label/'\n",
    "    model = unet(input_shape=(128, 128, 2))\n",
    "    model.load_weights('./save_model/exped/exped.keras')\n",
    "    # img_path = '/data/caizhizheng/2D/v2/data1/data/'\n",
    "    # img1_list = getimg(img_path, j=0)\n",
    "    # img_path = '/data/caizhizheng/2D/v2/data2/data/'\n",
    "    # img2_list = getimg(img_path, j=900)\n",
    "    # label_list = getimg(label_path,j=0)\n",
    "    # img_list = np.stack((img1_list,img2_list),axis = -1)\n",
    "    # image_dataset = tf.data.Dataset.from_tensor_slices(img_list)\n",
    "    # label_dataset = tf.data.Dataset.from_tensor_slices(label_list)\n",
    "    # label_dataset = label_dataset.map(onehot)\n",
    "    test_path = '/data/caizhizheng/2D/v2/test/predicted_input_data/'\n",
    "    label_path = '/data/caizhizheng/2D/v2/test/predicted_label/'\n",
    "    output_path = '/data/caizhizheng/2D/v2/test/output/'\n",
    "    img1_list = getimg(test_path, j =0)\n",
    "    img2_list = getimg(test_path, j =100)\n",
    "    img_list = np.stack((img1_list,img2_list),axis = -1)\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices(img_list)\n",
    "    # 使用事先保存的数据集会造成内存泄漏？？这是为什么，以前使用并不会\n",
    "    # image_dataset = tf.data.experimental.load(\"dataset/image_exped\")\n",
    "    # label_dataset = tf.data.experimental.load(\"dataset/label_exped\")\n",
    "    # x = next(iter(image_dataset.batch(image_dataset.cardinality().numpy())))\n",
    "    # y = next(iter(label_dataset.batch(label_dataset.cardinality().numpy())))\n",
    "    # train_dataset = tf.data.Dataset.zip((image_dataset, label_dataset))\n",
    "#     train_dataset = train_dataset.cache().shuffle(train_dataset.cardinality())\n",
    "\n",
    "#     validation_dataset = train_dataset.take(50).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "#     train_dataset = train_dataset.skip(50).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    # model_savename = 'exped.keras'\n",
    "    # train = train(model, model_savename, train_dataset, validation_dataset)\n",
    "    \n",
    "    test_dataset = test_dataset.batch(BATCH_SIZE)\n",
    "    test(model, test_dataset, output_path)\n",
    "    # name = 'error2' + '_postCrV' + '.csv'\n",
    "    error_path = './errors/'\n",
    "    name = 'error_exped_test' + '.csv'\n",
    "    name = os.path.join(error_path, name)\n",
    "    evalu.main(output_dir=output_path, label_dir=label_path, name=name, oder=np.array(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79cc9f3e-0dea-4e92-acc3-081879386ab5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "test_path = '/data/caizhizheng/2D/v2/predicted_label/'\n",
    "test_path1 = '/data/caizhizheng/2D/v2/test/predicted_label/'\n",
    "os.rename(test_path, test_path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89656624-bcaa-4779-9799-a8c58940c077",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    }
   ],
   "source": [
    "# %load unet_exped.py\n",
    "import os\n",
    "import shutil\n",
    "from unittest import result\n",
    "import skimage.io as io\n",
    "import skimage.transform as trans\n",
    "from keras.optimizer_v2.adam import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as keras\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.saving.utils_v1.mode_keys import is_train\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import imageio.v2 as imageio\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.layers import BatchNormalization, Conv2D, Activation, Dropout, AveragePooling2D, concatenate, \\\n",
    "    GlobalAveragePooling2D, MaxPooling2D, Dense, Input\n",
    "from keras.regularizers import l2\n",
    "import keras.backend as K\n",
    "import zipfile\n",
    "import evalu\n",
    "import time\n",
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "# tf.config.experimental.set_memory_growth(gpus[1], True)\n",
    "# tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "# Read datasets\n",
    "def getimg(dir,num,j):\n",
    "    input_dir = dir\n",
    "    img_list = []\n",
    "    for i in range(1+j,num+j):\n",
    "        filename = f'{i}.tif'\n",
    "        img_path = os.path.join(input_dir,filename)\n",
    "        img = imageio.imread(img_path)\n",
    "        img = img.astype(np.float32)\n",
    "        img = img / 255.0\n",
    "        img_list.append(img)\n",
    "    return img_list\n",
    "\n",
    "def onehot(img):\n",
    "    img = tf.cast(img, tf.int32)\n",
    "    img_one_hot = tf.one_hot(img, depth=2, on_value=1.0, off_value=0.0, axis=-1)\n",
    "    return img_one_hot\n",
    "\n",
    "# def add_channel(img): \n",
    "#     img = tf.expand_dims(img, axis=-1) \n",
    "#     return img\n",
    "# Draw loss curve\n",
    "def plot_history(history, result_dir, prefix):\n",
    "    \"\"\"\n",
    "    将训练与验证的accuracy与loss画出来\n",
    "    \"\"\"\n",
    "    plt.plot(history.history['accuracy'], marker='.')\n",
    "    plt.plot(history.history['val_accuracy'], marker='.')\n",
    "    plt.title('model accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.grid()\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend(['acc', 'val_acc'], loc='upper right')\n",
    "\t# plt.show()\n",
    "    plt.savefig(result_dir + 'unet_ace' + prefix + '.png')\n",
    "    plt.close()\n",
    "\n",
    "    plt.plot(history.history['loss'], marker='.')\n",
    "    plt.plot(history.history['val_loss'], marker='.')\n",
    "    plt.title('model loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.yscale(\"log\")\n",
    "    plt.grid()\n",
    "    plt.legend(['loss', 'val_loss'], loc='upper right')\n",
    "    # plt.show()\n",
    "    plt.savefig(result_dir + 'unet_loss'+ prefix +'.png')\n",
    "    plt.close()\n",
    "\n",
    "    # x = history.history['loss']\n",
    "    # np.savetxt('D:/pycharm/up_down_code/loss_picture/unet_train_loss.txt', x, fmt='%f')\n",
    "    # y = history.history['val_loss']\n",
    "    # np.savetxt('D:/pycharm/up_down_code/loss_picture/unet_val_loss.txt', y, fmt='%f')hb\n",
    "\n",
    "\n",
    "def Conv_Block(input_tensor, filters, bottleneck=False, weight_decay=1e-4):\n",
    "    \"\"\"    封装卷积层\n",
    "\n",
    "    :param input_tensor: 输入张量\n",
    "    :param filters: 卷积核数目\n",
    "    :param bottleneck: 是否使用bottleneck\n",
    "    :param dropout_rate: dropout比率\n",
    "    :param weight_decay: 权重衰减率\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    concat_axis = 1 if K.image_data_format() == 'channel_first' else -1  # 确定格式\n",
    "\n",
    "    x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5)(input_tensor)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # if bottleneck:\n",
    "    #     # 使用bottleneck进行降维\n",
    "    #     inter_channel = filters\n",
    "    #     x = Conv2D(inter_channel, (1, 1),\n",
    "    #                kernel_initializer='he_normal',\n",
    "    #                padding='same', use_bias=False,\n",
    "    #                kernel_regularizer=l2(weight_decay))(x)\n",
    "    #     x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5)(x)\n",
    "    #     x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters, (3, 3), kernel_initializer='he_normal', padding='same', use_bias=False)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def dens_block(input_tensor, nb_filter):\n",
    "    x1 = Conv_Block(input_tensor, nb_filter)\n",
    "    add1 = concatenate([x1, input_tensor], axis=-1)\n",
    "    x2 = Conv_Block(add1, nb_filter)\n",
    "    add2 = concatenate([x1, input_tensor, x2], axis=-1)\n",
    "    x3 = Conv_Block(add2, nb_filter)\n",
    "    return x3\n",
    "\n",
    "\n",
    "from keras.regularizers import l2\n",
    "\n",
    "\n",
    "# model definition\n",
    "def unet(input_shape=(128, 128, 2)):\n",
    "    # tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "    inputs = Input(input_shape)\n",
    "    # input1=Input(shape=(128,128,1))\n",
    "    # input2=Input(shape=(128,128,1))\n",
    "    # inputs=Concatenate(axis=-1)([input1,input2])\n",
    "    # inputs = Input(shape\n",
    "    # x  = Conv2D(32, 1, strides=1, activation='relu', padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    x = Conv2D(32, 7, kernel_initializer='he_normal', padding='same', strides=1, use_bias=False,\n",
    "               kernel_regularizer=l2(1e-4))(inputs)\n",
    "    # down first\n",
    "    down1 = dens_block(x, nb_filter=64)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(down1)  # 256\n",
    "    # down second\n",
    "    down2 = dens_block(pool1, nb_filter=64)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(down2)  # 128\n",
    "    # down third\n",
    "    down3 = dens_block(pool2, nb_filter=128)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(down3)  # 64\n",
    "    # down four\n",
    "    down4 = dens_block(pool3, nb_filter=256)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(down4)  # 32\n",
    "    # center\n",
    "    conv5 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool4)\n",
    "    conv5 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    # up first\n",
    "    up6 = UpSampling2D(size=(2, 2))(drop5)\n",
    "    # up6 = UpSampling2D(size=(2, 2))(drop5)\n",
    "    add6 = concatenate([down4, up6], axis=3)\n",
    "    up6 = dens_block(add6, nb_filter=256)\n",
    "    # up second\n",
    "    up7 = UpSampling2D(size=(2, 2))(up6)\n",
    "    # up7 = UpSampling2D(size=(2, 2))(conv6)\n",
    "    add7 = concatenate([down3, up7], axis=3)\n",
    "    up7 = dens_block(add7, nb_filter=128)\n",
    "    # up third\n",
    "    up8 = UpSampling2D(size=(2, 2))(up7)\n",
    "    # up8 = UpSampling2D(size=(2, 2))(conv7)\n",
    "    add8 = concatenate([down2, up8], axis=-1)\n",
    "    up8 = dens_block(add8, nb_filter=64)\n",
    "    # up four\n",
    "    up9 = UpSampling2D(size=(2, 2))(up8)\n",
    "    add9 = concatenate([down1, up9], axis=-1)\n",
    "    up9 = dens_block(add9, nb_filter=64)\n",
    "    # output\n",
    "    conv10 = Conv2D(32, 7, strides=1, activation='relu', padding='same', kernel_initializer='he_normal')(up9)\n",
    "    conv10 = Conv2D(2, 1, activation='softmax')(conv10)\n",
    "    # model = Model(inputs=[input1,input2], outputs=conv10)\n",
    "    model = Model(inputs=inputs, outputs=conv10)\n",
    "    # print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "# define Huber loss\n",
    "def huber_loss(y_true, y_pred):\n",
    "    return tf.losses.huber_loss(y_true, y_pred, delta=0.01)\n",
    "\n",
    "\n",
    "def simm_loss(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_pred - y_true)) + 0.01 * K.mean(K.abs(y_pred))\n",
    "    # return tf.abs(tf.norm(y_pred - y_true))/tf.norm(y_true)\n",
    "    \n",
    "# smooth = 1. # 用于防止分母为0.\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true) # 将 y_true 拉伸为一维.\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection) / (K.sum(y_true_f * y_true_f) + K.sum(y_pred_f * y_pred_f))\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1. - dice_coef(y_true, y_pred)\n",
    "\n",
    "# Define the learning rate attenuation value\n",
    "def scheduler(epoch):\n",
    "    if epoch % 10 == 0 and epoch != 0:\n",
    "        lr = K.get_value(model.optimizer.lr)\n",
    "        K.set_value(model.optimizer.lr, lr * 0.1)\n",
    "        print(\"lr change to {}\".format(lr * 0.1))\n",
    "    return K.get_value(model.optimizer.lr)\n",
    "\n",
    "import random\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "  0.001,\n",
    "  decay_steps=100000,\n",
    "  decay_rate=1,\n",
    "  staircase=False)\n",
    "        \n",
    "def make_zip(source_dir, output_name):\n",
    "    zipf = zipfile.ZipFile(output_name, 'w')\n",
    "    prelen = len(os.path.dirname(source_dir))\n",
    "    for parent, _, filenames in os.walk(source_dir):\n",
    "        for filename in filenames:\n",
    "            pathfile = os.path.join(parent, filename)\n",
    "            arcname = pathfile[prelen:].strip(os.path.sep)     #相对路径\n",
    "            zipf.write(pathfile, arcname)\n",
    "        zipf.close()\n",
    "\n",
    "def train(model, model_savename, train_dataset, validation_dataset):\n",
    "    model.compile(optimizer=Adam(learning_rate=lr_schedule), loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "    # reduce_lr = LearningRateScheduler(scheduler)\n",
    "    # reduce_lr = LearningRateScheduler(lschedule)\n",
    "    model_checkpoint = ModelCheckpoint('/code/save_model/exped/'+model_savename, monitor='loss', verbose=1,\n",
    "                                       save_best_only=True)\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=100)\n",
    "    Board = tf.keras.callbacks.TensorBoard(log_dir=\"/output/logs\")\n",
    "    history = model.fit(train_dataset.repeat(),\n",
    "                                  steps_per_epoch=100,\n",
    "                                  epochs=1000,\n",
    "                                  validation_data=validation_dataset,\n",
    "                                #   validation_steps=50,\n",
    "                                  callbacks=[model_checkpoint,\n",
    "                                             early_stop,Board\n",
    "                                             # Board\n",
    "                                             ])\n",
    "    plot_history(history, '/code/loss_picture/extended/',prefix='_exped')\n",
    "    return model\n",
    "    \n",
    "def test(model, test_dataset, output_path):\n",
    "    mask = model.predict(test_dataset)\n",
    "    mask = tf.argmax(mask, axis=-1)\n",
    "    mask = tf.keras.backend.eval(mask)\n",
    "    mask = (mask * 255).astype(np.uint8)\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    for j in range(0, 1200):\n",
    "        cv2.imwrite(output_path+ '%d.tif' %(j+1), mask[j])\n",
    "    print('model predict OK')\n",
    "        \n",
    "BATCH_SIZE=128\n",
    "plot_path = '/code/loss_picture/extended/'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # is_train = False # you can change this to False if you want to test only\n",
    "    output_path = '/data/caizhizheng/2D/v2/exped_output/'\n",
    "    label_path = '/data/caizhizheng/2D/v2/label/label/'\n",
    "    model = unet(input_shape=(128, 128, 2))\n",
    "    img_path = '/data/caizhizheng/2D/v2/data1/data/'\n",
    "    img1_list = getimg(img_path,num=1201, j=0)\n",
    "    img_path = '/data/caizhizheng/2D/v2/data2/data/'\n",
    "    img2_list = getimg(img_path,num=1201, j=900)\n",
    "    label_list = getimg(label_path,num=1201, j=0)\n",
    "    img_list = np.stack((img1_list,img2_list),axis = -1)\n",
    "    image_dataset = tf.data.Dataset.from_tensor_slices(img_list)\n",
    "    label_dataset = tf.data.Dataset.from_tensor_slices(label_list)\n",
    "    label_dataset = label_dataset.map(onehot)\n",
    "    # 使用事先保存的数据集会造成内存泄漏？？这是为什么，以前使用并不会\n",
    "    # image_dataset = tf.data.experimental.load(\"dataset/image_exped\")\n",
    "    # label_dataset = tf.data.experimental.load(\"dataset/label_exped\")\n",
    "    # x = next(iter(image_dataset.batch(image_dataset.cardinality().numpy())))\n",
    "    # y = next(iter(label_dataset.batch(label_dataset.cardinality().numpy())))\n",
    "    train_dataset = tf.data.Dataset.zip((image_dataset, label_dataset))\n",
    "    train_dataset = train_dataset.cache().shuffle(train_dataset.cardinality())\n",
    "\n",
    "    validation_dataset = train_dataset.take(50).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    train_dataset = train_dataset.skip(50).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    model_savename = 'expedv2.keras'\n",
    "    train = train(model, model_savename, train_dataset, validation_dataset)\n",
    "    \n",
    "    image_dataset = image_dataset.batch(BATCH_SIZE)\n",
    "    test(model, image_dataset, output_path)\n",
    "    # name = 'error2' + '_postCrV' + '.csv'\n",
    "    error_path = './errors/'\n",
    "    name = 'error_expedv2' + '.csv'\n",
    "    name = os.path.join(error_path, name)\n",
    "    evalu.main(output_dir=output_path, label_dir=label_path, name=name, oder=np.array(0))\n",
    "    \n",
    "#     测试集\n",
    "    test_path = '/data/caizhizheng/2D/v2/test/predicted_input_data/'\n",
    "    label_path = '/data/caizhizheng/2D/v2/test/predicted_label/'\n",
    "    output_path = '/data/caizhizheng/2D/v2/test/outputv2/'\n",
    "    img1_list = getimg(test_path,num=101, j =0)\n",
    "    img2_list = getimg(test_path,num=101, j =100)\n",
    "    img_list = np.stack((img1_list,img2_list),axis = -1)\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices(img_list)\n",
    "    model.load_weights('/code/save_model/exped/'+model_savename)\n",
    "    test(model, test_dataset, output_path)\n",
    "    name = 'error_exped_testv2.csv'\n",
    "    name = os.path.join(error_path, name)\n",
    "    evalu.main(output_dir=output_path, label_dir=label_path, name=name, oder=np.array(0))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a2f613f-41e7-47a6-92e9-f50b0076baf4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model predict OK\n",
      "平均汉明距离为：138.0800\n",
      "平均杰卡德相似系数为：0.6413\n",
      "平均Dice相似系数为：0.6942\n",
      "平均误差为：53.2805%\n"
     ]
    }
   ],
   "source": [
    "# %load unet_alltrain.py\n",
    "# 测试集\n",
    "import os\n",
    "import shutil\n",
    "from unittest import result\n",
    "import skimage.io as io\n",
    "import skimage.transform as trans\n",
    "from keras.optimizer_v2.adam import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as keras\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.saving.utils_v1.mode_keys import is_train\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import imageio.v2 as imageio\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.layers import BatchNormalization, Conv2D, Activation, Dropout, AveragePooling2D, concatenate, \\\n",
    "    GlobalAveragePooling2D, MaxPooling2D, Dense, Input\n",
    "from keras.regularizers import l2\n",
    "import keras.backend as K\n",
    "import zipfile\n",
    "import evalu\n",
    "import time\n",
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "# tf.config.experimental.set_memory_growth(gpus[1], True)\n",
    "# tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "# Read datasets\n",
    "def getimg(dir,num,j):\n",
    "    input_dir = dir\n",
    "    img_list = []\n",
    "    for i in range(1+j,num+j):\n",
    "        filename = f'{i}.tif'\n",
    "        img_path = os.path.join(input_dir,filename)\n",
    "        img = imageio.imread(img_path)\n",
    "        img = img.astype(np.float32)\n",
    "        img = img / 255.0\n",
    "        img_list.append(img)\n",
    "    return img_list\n",
    "\n",
    "def onehot(img):\n",
    "    img = tf.cast(img, tf.int32)\n",
    "    img_one_hot = tf.one_hot(img, depth=2, on_value=1.0, off_value=0.0, axis=-1)\n",
    "    return img_one_hot\n",
    "\n",
    "# def add_channel(img): \n",
    "#     img = tf.expand_dims(img, axis=-1) \n",
    "#     return img\n",
    "# Draw loss curve\n",
    "def plot_history(history, result_dir, prefix):\n",
    "    \"\"\"\n",
    "    将训练与验证的accuracy与loss画出来\n",
    "    \"\"\"\n",
    "    plt.plot(history.history['accuracy'], marker='.')\n",
    "    plt.plot(history.history['val_accuracy'], marker='.')\n",
    "    plt.title('model accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.grid()\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend(['acc', 'val_acc'], loc='upper right')\n",
    "\t# plt.show()\n",
    "    plt.savefig(result_dir + 'unet_ace' + prefix + '.png')\n",
    "    plt.close()\n",
    "\n",
    "    plt.plot(history.history['loss'], marker='.')\n",
    "    plt.plot(history.history['val_loss'], marker='.')\n",
    "    plt.title('model loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.yscale(\"log\")\n",
    "    plt.grid()\n",
    "    plt.legend(['loss', 'val_loss'], loc='upper right')\n",
    "    # plt.show()\n",
    "    plt.savefig(result_dir + 'unet_loss'+ prefix +'.png')\n",
    "    plt.close()\n",
    "\n",
    "    # x = history.history['loss']\n",
    "    # np.savetxt('D:/pycharm/up_down_code/loss_picture/unet_train_loss.txt', x, fmt='%f')\n",
    "    # y = history.history['val_loss']\n",
    "    # np.savetxt('D:/pycharm/up_down_code/loss_picture/unet_val_loss.txt', y, fmt='%f')hb\n",
    "\n",
    "\n",
    "def Conv_Block(input_tensor, filters, bottleneck=False, weight_decay=1e-4):\n",
    "    \"\"\"    封装卷积层\n",
    "\n",
    "    :param input_tensor: 输入张量\n",
    "    :param filters: 卷积核数目\n",
    "    :param bottleneck: 是否使用bottleneck\n",
    "    :param dropout_rate: dropout比率\n",
    "    :param weight_decay: 权重衰减率\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    concat_axis = 1 if K.image_data_format() == 'channel_first' else -1  # 确定格式\n",
    "\n",
    "    x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5)(input_tensor)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # if bottleneck:\n",
    "    #     # 使用bottleneck进行降维\n",
    "    #     inter_channel = filters\n",
    "    #     x = Conv2D(inter_channel, (1, 1),\n",
    "    #                kernel_initializer='he_normal',\n",
    "    #                padding='same', use_bias=False,\n",
    "    #                kernel_regularizer=l2(weight_decay))(x)\n",
    "    #     x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5)(x)\n",
    "    #     x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters, (3, 3), kernel_initializer='he_normal', padding='same', use_bias=False)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def dens_block(input_tensor, nb_filter):\n",
    "    x1 = Conv_Block(input_tensor, nb_filter)\n",
    "    add1 = concatenate([x1, input_tensor], axis=-1)\n",
    "    x2 = Conv_Block(add1, nb_filter)\n",
    "    add2 = concatenate([x1, input_tensor, x2], axis=-1)\n",
    "    x3 = Conv_Block(add2, nb_filter)\n",
    "    return x3\n",
    "\n",
    "\n",
    "from keras.regularizers import l2\n",
    "\n",
    "\n",
    "# model definition\n",
    "def unet(input_shape=(128, 128, 2)):\n",
    "    # tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "    inputs = Input(input_shape)\n",
    "    # input1=Input(shape=(128,128,1))\n",
    "    # input2=Input(shape=(128,128,1))\n",
    "    # inputs=Concatenate(axis=-1)([input1,input2])\n",
    "    # inputs = Input(shape\n",
    "    # x  = Conv2D(32, 1, strides=1, activation='relu', padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    x = Conv2D(32, 7, kernel_initializer='he_normal', padding='same', strides=1, use_bias=False,\n",
    "               kernel_regularizer=l2(1e-4))(inputs)\n",
    "    # down first\n",
    "    down1 = dens_block(x, nb_filter=64)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(down1)  # 256\n",
    "    # down second\n",
    "    down2 = dens_block(pool1, nb_filter=64)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(down2)  # 128\n",
    "    # down third\n",
    "    down3 = dens_block(pool2, nb_filter=128)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(down3)  # 64\n",
    "    # down four\n",
    "    down4 = dens_block(pool3, nb_filter=256)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(down4)  # 32\n",
    "    # center\n",
    "    conv5 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool4)\n",
    "    conv5 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    # up first\n",
    "    up6 = UpSampling2D(size=(2, 2))(drop5)\n",
    "    # up6 = UpSampling2D(size=(2, 2))(drop5)\n",
    "    add6 = concatenate([down4, up6], axis=3)\n",
    "    up6 = dens_block(add6, nb_filter=256)\n",
    "    # up second\n",
    "    up7 = UpSampling2D(size=(2, 2))(up6)\n",
    "    # up7 = UpSampling2D(size=(2, 2))(conv6)\n",
    "    add7 = concatenate([down3, up7], axis=3)\n",
    "    up7 = dens_block(add7, nb_filter=128)\n",
    "    # up third\n",
    "    up8 = UpSampling2D(size=(2, 2))(up7)\n",
    "    # up8 = UpSampling2D(size=(2, 2))(conv7)\n",
    "    add8 = concatenate([down2, up8], axis=-1)\n",
    "    up8 = dens_block(add8, nb_filter=64)\n",
    "    # up four\n",
    "    up9 = UpSampling2D(size=(2, 2))(up8)\n",
    "    add9 = concatenate([down1, up9], axis=-1)\n",
    "    up9 = dens_block(add9, nb_filter=64)\n",
    "    # output\n",
    "    conv10 = Conv2D(32, 7, strides=1, activation='relu', padding='same', kernel_initializer='he_normal')(up9)\n",
    "    conv10 = Conv2D(2, 1, activation='softmax')(conv10)\n",
    "    # model = Model(inputs=[input1,input2], outputs=conv10)\n",
    "    model = Model(inputs=inputs, outputs=conv10)\n",
    "    # print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "# define Huber loss\n",
    "def huber_loss(y_true, y_pred):\n",
    "    return tf.losses.huber_loss(y_true, y_pred, delta=0.01)\n",
    "\n",
    "\n",
    "def simm_loss(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_pred - y_true)) + 0.01 * K.mean(K.abs(y_pred))\n",
    "    # return tf.abs(tf.norm(y_pred - y_true))/tf.norm(y_true)\n",
    "    \n",
    "# smooth = 1. # 用于防止分母为0.\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true) # 将 y_true 拉伸为一维.\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection) / (K.sum(y_true_f * y_true_f) + K.sum(y_pred_f * y_pred_f))\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1. - dice_coef(y_true, y_pred)\n",
    "\n",
    "# Define the learning rate attenuation value\n",
    "def scheduler(epoch):\n",
    "    if epoch % 10 == 0 and epoch != 0:\n",
    "        lr = K.get_value(model.optimizer.lr)\n",
    "        K.set_value(model.optimizer.lr, lr * 0.1)\n",
    "        print(\"lr change to {}\".format(lr * 0.1))\n",
    "    return K.get_value(model.optimizer.lr)\n",
    "\n",
    "import random\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "  0.001,\n",
    "  decay_steps=1000,\n",
    "  decay_rate=1,\n",
    "  staircase=False)\n",
    "        \n",
    "def make_zip(source_dir, output_name):\n",
    "    zipf = zipfile.ZipFile(output_name, 'w')\n",
    "    prelen = len(os.path.dirname(source_dir))\n",
    "    for parent, _, filenames in os.walk(source_dir):\n",
    "        for filename in filenames:\n",
    "            pathfile = os.path.join(parent, filename)\n",
    "            arcname = pathfile[prelen:].strip(os.path.sep)     #相对路径\n",
    "            zipf.write(pathfile, arcname)\n",
    "        zipf.close()\n",
    "\n",
    "def train(model, model_savename, train_dataset, validation_dataset):\n",
    "    model.compile(optimizer=Adam(learning_rate=lr_schedule), loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "    # reduce_lr = LearningRateScheduler(scheduler)\n",
    "    # reduce_lr = LearningRateScheduler(lschedule)\n",
    "    model_checkpoint = ModelCheckpoint('/code/save_model/exped/'+model_savename, monitor='loss', verbose=1,\n",
    "                                       save_best_only=True)\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=100)\n",
    "    Board = tf.keras.callbacks.TensorBoard(log_dir=\"/output/logs\")\n",
    "    history = model.fit(train_dataset.repeat(),\n",
    "                                  steps_per_epoch=100,\n",
    "                                  epochs=1000,\n",
    "                                  validation_data=validation_dataset,\n",
    "                                #   validation_steps=50,\n",
    "                                  callbacks=[model_checkpoint,\n",
    "                                             early_stop,Board\n",
    "                                             # Board\n",
    "                                             ])\n",
    "    plot_history(history, '/code/loss_picture/extended/',prefix='_exped')\n",
    "    return model\n",
    "    \n",
    "def test(model, test_dataset, output_path, length):\n",
    "    mask = model.predict(test_dataset)\n",
    "    mask = tf.argmax(mask, axis=-1)\n",
    "    mask = tf.keras.backend.eval(mask)\n",
    "    mask = (mask * 255).astype(np.uint8)\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    for j in range(0, length):\n",
    "        cv2.imwrite(output_path+ '%d.tif' %(j+1), mask[j])\n",
    "    print('model predict OK')\n",
    "        \n",
    "BATCH_SIZE=128\n",
    "plot_path = '/code/loss_picture/extended/'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # is_train = False # you can change this to False if you want to test only\n",
    "    # output_path = '/data/caizhizheng/2D/v2/test/outputv2/'\n",
    "    # label_path = '/data/caizhizheng/2D/v2/label/label/'\n",
    "    model = unet(input_shape=(128, 128, 2))\n",
    "    # img_path = '/data/caizhizheng/2D/v2/data1/data/'\n",
    "    # img1_list = getimg(img_path,num=1201, j=0)\n",
    "    # img_path = '/data/caizhizheng/2D/v2/data2/data/'\n",
    "    # img2_list = getimg(img_path,num=1201, j=900)\n",
    "    # label_list = getimg(label_path,num=1201, j=0)\n",
    "    # img_list = np.stack((img1_list,img2_list),axis = -1)\n",
    "    \n",
    "    test_path = '/data/caizhizheng/2D/v2/test/test2/data/'\n",
    "    test_label_path = '/data/caizhizheng/2D/v2/test/test2/label/'\n",
    "    output_path = '/data/caizhizheng/2D/v2/test/outputv2/'\n",
    "    img1_list = getimg(test_path,num=201, j =0)\n",
    "    img2_list = getimg(test_path,num=201, j =200)\n",
    "    test_img_list = np.stack((img1_list,img2_list),axis = -1)\n",
    "    # test_label_list = getimg(test_label_path, num=201, j=0)\n",
    "    \n",
    "    # img_list = np.vstack((img_list, test_img_list))\n",
    "    # label_list = np.vstack((label_list, test_label_list))\n",
    "    \n",
    "    image_dataset = tf.data.Dataset.from_tensor_slices(test_img_list)\n",
    "    # label_dataset = tf.data.Dataset.from_tensor_slices(label_list)\n",
    "    # label_dataset = label_dataset.map(onehot)\n",
    "    # 使用事先保存的数据集会造成内存泄漏？？这是为什么，以前使用并不会\n",
    "    # image_dataset = tf.data.experimental.load(\"dataset/image_exped\")\n",
    "    # label_dataset = tf.data.experimental.load(\"dataset/label_exped\")\n",
    "    # x = next(iter(image_dataset.batch(image_dataset.cardinality().numpy())))\n",
    "    # y = next(iter(label_dataset.batch(label_dataset.cardinality().numpy())))\n",
    "    # train_dataset = tf.data.Dataset.zip((image_dataset, label_dataset))\n",
    "    # train_dataset = train_dataset.cache().shuffle(train_dataset.cardinality())\n",
    "\n",
    "    # validation_dataset = train_dataset.take(50).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    # train_dataset = train_dataset.skip(50).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    model_savename = 'expedv2.keras'\n",
    "    model.load_weights('/code/save_model/exped/'+model_savename)\n",
    "    # train = train(model, model_savename, train_dataset, validation_dataset)\n",
    "    \n",
    "    image_dataset = image_dataset.batch(BATCH_SIZE)\n",
    "    test(model, image_dataset, output_path, 200)\n",
    "    # name = 'error2' + '_postCrV' + '.csv'\n",
    "    error_path = './errors/'\n",
    "    name = 'error_exped_testv2' + '.csv'\n",
    "    name = os.path.join(error_path, name)\n",
    "    evalu.main(output_dir=output_path, label_dir=test_label_path, name=name, oder=np.array(0))\n",
    "    \n",
    "# #     测试集\n",
    "#     test_path = '/data/caizhizheng/2D/v2/test/predicted_input_data/'\n",
    "#     label_path = '/data/caizhizheng/2D/v2/test/predicted_label/'\n",
    "#     output_path = '/data/caizhizheng/2D/v2/test/outputv2/'\n",
    "#     img1_list = getimg(test_path,num=101, j =0)\n",
    "#     img2_list = getimg(test_path,num=101, j =100)\n",
    "#     img_list = np.stack((img1_list,img2_list),axis = -1)\n",
    "#     test_dataset = tf.data.Dataset.from_tensor_slices(img_list)\n",
    "#     test_dataset = test_datset.batch(1)\n",
    "#     model.load_weights('/code/save_model/exped/'+model_savename)\n",
    "#     test(model, test_dataset, output_path)\n",
    "#     name = 'error_exped_testv2.csv'\n",
    "#     name = os.path.join(error_path, name)\n",
    "#     evalu.main(output_dir=output_path, label_dir=label_path, name=name, oder=np.array(0))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bb32d280-63d5-4bd2-a1cd-1bc0bb1c74c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1300, 128, 128, 2)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f94cd6db-8cca-4f88-a3e8-3601173afa61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1300, 128, 128)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0fd7b1d8-c38b-4e8c-a64a-4af2774f2a8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.vstack((img_list, test_img_list))[1200:]==test_img_list).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8682a0-f801-493d-b12e-a7161756a2da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
