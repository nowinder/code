{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "140f87a5-86d9-4206-bc19-b974e57e1219",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 测试tvt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7013ad47-0231-4a15-b9a7-fe50ed5d1efa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %load unet_tvt\n",
    "import os\n",
    "# import shutil\n",
    "# import skimage.io as io\n",
    "# import skimage.transform as trans\n",
    "from keras.optimizer_v2.adam import Adam\n",
    "# from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "# from keras import backend as keras\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import imageio.v2 as imageio\n",
    "# from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.layers import BatchNormalization, Conv2D, Activation, Dropout, AveragePooling2D, concatenate, \\\n",
    "    GlobalAveragePooling2D, MaxPooling2D, Dense, Input\n",
    "from keras.regularizers import l2\n",
    "import keras.backend as K\n",
    "import zipfile\n",
    "import evalu\n",
    "from keras.regularizers import l2\n",
    "# tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "# Read datasets\n",
    "def getimg(dir,num,j):\n",
    "    input_dir = dir\n",
    "    img_list = []\n",
    "    for i in range(1+j,num+j):\n",
    "        filename = f'{i}.tif'\n",
    "        img_path = os.path.join(input_dir,filename)\n",
    "        img = imageio.imread(img_path)\n",
    "        img = img.astype(np.float32)\n",
    "        img = img / 255.0\n",
    "        img_list.append(img)\n",
    "    return img_list\n",
    "\n",
    "def onehot(img):\n",
    "    img = tf.cast(img, tf.int32)\n",
    "    img_one_hot = tf.one_hot(img, depth=2, on_value=1.0, off_value=0.0, axis=-1)\n",
    "    return img_one_hot\n",
    "\n",
    "# def add_channel(img): \n",
    "#     img = tf.expand_dims(img, axis=-1) \n",
    "#     return img\n",
    "# Draw loss curve\n",
    "def plot_history(history, result_dir, prefix):\n",
    "    \"\"\"\n",
    "    将训练与验证的accuracy与loss画出来\n",
    "    \"\"\"\n",
    "    plt.plot(history.history['accuracy'], marker='.')\n",
    "    plt.plot(history.history['val_accuracy'], marker='.')\n",
    "    plt.title('model accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.grid()\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend(['acc', 'val_acc'], loc='upper right')\n",
    "    # plt.show()\n",
    "    plt.savefig(result_dir + 'unet_ace' + prefix + '.png')\n",
    "    plt.close()\n",
    "\n",
    "    plt.plot(history.history['loss'], marker='.')\n",
    "    plt.plot(history.history['val_loss'], marker='.')\n",
    "    plt.title('model loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.yscale(\"log\")\n",
    "    plt.grid()\n",
    "    plt.legend(['loss', 'val_loss'], loc='upper right')\n",
    "    # plt.show()\n",
    "    plt.savefig(result_dir + 'unet_loss'+ prefix +'.png')\n",
    "    plt.close()\n",
    "\n",
    "    # x = history.history['loss']\n",
    "    # np.savetxt('D:/pycharm/up_down_code/loss_picture/unet_train_loss.txt', x, fmt='%f')\n",
    "    # y = history.history['val_loss']\n",
    "    # np.savetxt('D:/pycharm/up_down_code/loss_picture/unet_val_loss.txt', y, fmt='%f')hb\n",
    "\n",
    "\n",
    "def Conv_Block(input_tensor, filters, bottleneck=False, weight_decay=1e-4):\n",
    "    \"\"\"    封装卷积层\n",
    "\n",
    "    :param input_tensor: 输入张量\n",
    "    :param filters: 卷积核数目\n",
    "    :param bottleneck: 是否使用bottleneck\n",
    "    :param dropout_rate: dropout比率\n",
    "    :param weight_decay: 权重衰减率\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    concat_axis = 1 if K.image_data_format() == 'channel_first' else -1  # 确定格式\n",
    "\n",
    "    x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5)(input_tensor)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # if bottleneck:\n",
    "    #     # 使用bottleneck进行降维\n",
    "    #     inter_channel = filters\n",
    "    #     x = Conv2D(inter_channel, (1, 1),\n",
    "    #                kernel_initializer='he_normal',\n",
    "    #                padding='same', use_bias=False,\n",
    "    #                kernel_regularizer=l2(weight_decay))(x)\n",
    "    #     x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5)(x)\n",
    "    #     x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters, (3, 3), kernel_initializer='he_normal', padding='same', use_bias=False)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def dens_block(input_tensor, nb_filter):\n",
    "    x1 = Conv_Block(input_tensor, nb_filter)\n",
    "    add1 = concatenate([x1, input_tensor], axis=-1)\n",
    "    x2 = Conv_Block(add1, nb_filter)\n",
    "    add2 = concatenate([x1, input_tensor, x2], axis=-1)\n",
    "    x3 = Conv_Block(add2, nb_filter)\n",
    "    return x3\n",
    "\n",
    "\n",
    "\n",
    "# model definition\n",
    "def unet(input_shape=(128, 128, 2)):\n",
    "    # tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "    inputs = Input(input_shape)\n",
    "    # input1=Input(shape=(128,128,1))\n",
    "    # input2=Input(shape=(128,128,1))\n",
    "    # inputs=Concatenate(axis=-1)([input1,input2])\n",
    "    # inputs = Input(shape\n",
    "    # x  = Conv2D(32, 1, strides=1, activation='relu', padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    x = Conv2D(32, 7, kernel_initializer='he_normal', padding='same', strides=1, use_bias=False,\n",
    "               kernel_regularizer=l2(1e-4))(inputs)\n",
    "    # down first\n",
    "    down1 = dens_block(x, nb_filter=64)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(down1)  # 256\n",
    "    # down second\n",
    "    down2 = dens_block(pool1, nb_filter=64)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(down2)  # 128\n",
    "    # down third\n",
    "    down3 = dens_block(pool2, nb_filter=128)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(down3)  # 64\n",
    "    # down four\n",
    "    down4 = dens_block(pool3, nb_filter=256)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(down4)  # 32\n",
    "    # center\n",
    "    conv5 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool4)\n",
    "    conv5 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    # up first\n",
    "    up6 = UpSampling2D(size=(2, 2))(drop5)\n",
    "    # up6 = UpSampling2D(size=(2, 2))(drop5)\n",
    "    add6 = concatenate([down4, up6], axis=3)\n",
    "    up6 = dens_block(add6, nb_filter=256)\n",
    "    # up second\n",
    "    up7 = UpSampling2D(size=(2, 2))(up6)\n",
    "    # up7 = UpSampling2D(size=(2, 2))(conv6)\n",
    "    add7 = concatenate([down3, up7], axis=3)\n",
    "    up7 = dens_block(add7, nb_filter=128)\n",
    "    # up third\n",
    "    up8 = UpSampling2D(size=(2, 2))(up7)\n",
    "    # up8 = UpSampling2D(size=(2, 2))(conv7)\n",
    "    add8 = concatenate([down2, up8], axis=-1)\n",
    "    up8 = dens_block(add8, nb_filter=64)\n",
    "    # up four\n",
    "    up9 = UpSampling2D(size=(2, 2))(up8)\n",
    "    add9 = concatenate([down1, up9], axis=-1)\n",
    "    up9 = dens_block(add9, nb_filter=64)\n",
    "    # output\n",
    "    conv10 = Conv2D(32, 7, strides=1, activation='relu', padding='same', kernel_initializer='he_normal')(up9)\n",
    "    conv10 = Conv2D(2, 1, activation='softmax')(conv10)\n",
    "    # model = Model(inputs=[input1,input2], outputs=conv10)\n",
    "    model = Model(inputs=inputs, outputs=conv10)\n",
    "    # print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "# define Huber loss\n",
    "def huber_loss(y_true, y_pred):\n",
    "    return tf.losses.huber_loss(y_true, y_pred, delta=0.01)\n",
    "\n",
    "\n",
    "def simm_loss(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_pred - y_true)) + 0.01 * K.mean(K.abs(y_pred))\n",
    "    # return tf.abs(tf.norm(y_pred - y_true))/tf.norm(y_true)\n",
    "    \n",
    "# smooth = 1. # 用于防止分母为0.\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true) # 将 y_true 拉伸为一维.\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection) / (K.sum(y_true_f * y_true_f) + K.sum(y_pred_f * y_pred_f))\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1. - dice_coef(y_true, y_pred)\n",
    "\n",
    "# Define the learning rate attenuation value\n",
    "def scheduler(epoch):\n",
    "    if epoch % 10 == 0 and epoch != 0:\n",
    "        lr = K.get_value(model.optimizer.lr)\n",
    "        K.set_value(model.optimizer.lr, lr * 0.1)\n",
    "        print(\"lr change to {}\".format(lr * 0.1))\n",
    "    return K.get_value(model.optimizer.lr)\n",
    "\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "  0.001,\n",
    "  decay_steps=2000,\n",
    "  decay_rate=1,\n",
    "  staircase=False)\n",
    "        \n",
    "def make_zip(source_dir, output_name):\n",
    "    zipf = zipfile.ZipFile(output_name, 'w')\n",
    "    prelen = len(os.path.dirname(source_dir))\n",
    "    for parent, _, filenames in os.walk(source_dir):\n",
    "        for filename in filenames:\n",
    "            pathfile = os.path.join(parent, filename)\n",
    "            arcname = pathfile[prelen:].strip(os.path.sep)     #相对路径\n",
    "            zipf.write(pathfile, arcname)\n",
    "        zipf.close()\n",
    "\n",
    "def train(model, model_savename, train_dataset, validation_dataset):\n",
    "    # model.compile(optimizer=Adam(learning_rate=lr_schedule), loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "    # reduce_lr = LearningRateScheduler(scheduler)\n",
    "    # reduce_lr = LearningRateScheduler(lschedule)\n",
    "    # model.load_weights('/code/save_model/exped/'+model_savename)\n",
    "    model_checkpoint = ModelCheckpoint('/code/save_model/exped/'+model_savename, monitor='loss', verbose=0,\n",
    "                                       save_best_only=True)\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=200)\n",
    "    Board = tf.keras.callbacks.TensorBoard(log_dir=\"/output/logs\")\n",
    "    \n",
    "    history = model.fit(train_dataset.repeat(),\n",
    "                                  steps_per_epoch=100,\n",
    "                                  epochs=3000,\n",
    "                                  validation_data=validation_dataset,\n",
    "                                #   validation_steps=50,\n",
    "                                  callbacks=[model_checkpoint,\n",
    "                                             early_stop,Board\n",
    "                                             ])\n",
    "    plot_history(history, '/code/loss_picture/extended/',prefix='_exped')\n",
    "    return model\n",
    "    \n",
    "def test(model, test_dataset, output_path, length):\n",
    "    mask = model.predict(test_dataset)\n",
    "    mask = tf.argmax(mask, axis=-1)\n",
    "    mask = tf.keras.backend.eval(mask)\n",
    "    mask = (mask * 255).astype(np.uint8)\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    for j in range(0, length):\n",
    "        cv2.imwrite(output_path+ '%d.tif' %(j+1), mask[j])\n",
    "    print('model predict OK')\n",
    "        \n",
    "BATCH_SIZE=128\n",
    "plot_path = '/code/loss_picture/extended/'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    output_path = '/data/caizhizheng/2D/v2/exped_output/'\n",
    "    label_path = '/data/caizhizheng/2D/v2/label/label/'\n",
    "    img_path = '/data/caizhizheng/2D/v2/data1/data/'\n",
    "    img1_list = getimg(img_path,num=1901, j=0)\n",
    "    img_path = '/data/caizhizheng/2D/v2/data2/data/'\n",
    "    img2_list = getimg(img_path,num=1901, j=900)\n",
    "    label_list = getimg(label_path,num=1901, j=0)\n",
    "    img_list = np.stack((img1_list,img2_list),axis = -1)\n",
    "    \n",
    "    vali_path = '/data/caizhizheng/2D/v2/test/predicted_input_data/'\n",
    "    vali_label_path = '/data/caizhizheng/2D/v2/test/predicted_label/'\n",
    "    # output_path = '/data/caizhizheng/2D/v2/test/outputv2/'\n",
    "    img1_list = getimg(vali_path,num=101, j =0)\n",
    "    img2_list = getimg(vali_path,num=101, j =100)\n",
    "    vali_list = np.stack((img1_list,img2_list),axis = -1)\n",
    "    vali_label_list = getimg(vali_label_path, num=101, j=0)\n",
    "    \n",
    "    # img_list = np.vstack((img_list, vali_list))\n",
    "    # label_list = np.vstack((label_list, vali_label_list))\n",
    "    \n",
    "    image_dataset = tf.data.Dataset.from_tensor_slices(img_list)\n",
    "    label_dataset = tf.data.Dataset.from_tensor_slices(label_list)\n",
    "    label_dataset = label_dataset.map(onehot)\n",
    "    train_dataset = tf.data.Dataset.zip((image_dataset, label_dataset))\n",
    "    train_dataset = train_dataset.cache().shuffle(train_dataset.cardinality()).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    vali_img_dataset = tf.data.Dataset.from_tensor_slices(vali_list)\n",
    "    vali_label_dataset = tf.data.Dataset.from_tensor_slices(vali_label_list)\n",
    "    vali_label_dataset = vali_label_dataset.map(onehot)\n",
    "    vali_dataset = tf.data.Dataset.zip((vali_img_dataset, vali_label_dataset))\n",
    "    vali_dataset = vali_dataset.cache().shuffle(vali_dataset.cardinality()).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    # 使用事先保存的数据集会造成内存泄漏？？\n",
    "    # image_dataset = tf.data.experimental.load(\"dataset/image_exped\")\n",
    "    # label_dataset = tf.data.experimental.load(\"dataset/label_exped\")\n",
    "    # x = next(iter(image_dataset.batch(image_dataset.cardinality().numpy())))\n",
    "    # y = next(iter(label_dataset.batch(label_dataset.cardinality().numpy())))\n",
    "    \n",
    "    # validation_dataset = train_dataset.take(50).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    # train_dataset = train_dataset.skip(50).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "    with mirrored_strategy.scope():\n",
    "        model = unet(input_shape=(128, 128, 2))\n",
    "        model.compile(optimizer=Adam(learning_rate=lr_schedule), loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "        # model.load_weights('/code/save_model/exped/cross_ini_277.keras')\n",
    "    model_savename = 'tvt_1900.keras'\n",
    "    train = train(model, model_savename, train_dataset, vali_dataset)\n",
    "    \n",
    "    # image_dataset = image_dataset.batch(BATCH_SIZE)\n",
    "    test(model, vali_dataset, output_path, 100)\n",
    "    # name = 'error2' + '_postCrV' + '.csv'\n",
    "    error_path = './errors/'\n",
    "    name = 'error_vali100' + '.csv'\n",
    "    name = os.path.join(error_path, name)\n",
    "    evalu.main(output_dir=output_path, label_dir=vali_label_path, name=name, oder=np.array(0))\n",
    "    \n",
    "#     测试集\n",
    "    test_path = '/data/caizhizheng/2D/v2/test/test2/data/'\n",
    "    test_label_path = '/data/caizhizheng/2D/v2/test/test2/label/'\n",
    "    output_path = '/data/caizhizheng/2D/v2/test/outputv2/'\n",
    "    img1_list = getimg(test_path,num=201, j =0)\n",
    "    img2_list = getimg(test_path,num=201, j =200)\n",
    "    img_list = np.stack((img1_list,img2_list),axis = -1)\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices(img_list)\n",
    "    test_dataset = test_dataset.batch(1)\n",
    "    model.load_weights('/code/save_model/exped/'+model_savename)\n",
    "    test(model, test_dataset, output_path,200)\n",
    "    name = 'error_test200.csv'\n",
    "    name = os.path.join(error_path, name)\n",
    "    evalu.main(output_dir=output_path, label_dir=test_label_path, name=name, oder=np.array(0))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5db9de7a-3389-4a55-a465-7477450f8bf0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "model predict OK\n",
      "平均汉明距离为：110.4600\n",
      "平均杰卡德相似系数为：0.6740\n",
      "平均Dice相似系数为：0.7224\n",
      "平均误差为：48.0609%\n"
     ]
    }
   ],
   "source": [
    "# %load unet_testtv.py\n",
    "import os\n",
    "# import shutil\n",
    "# import skimage.io as io\n",
    "# import skimage.transform as trans\n",
    "from keras.optimizer_v2.adam import Adam\n",
    "# from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "# from keras import backend as keras\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import imageio.v2 as imageio\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.layers import BatchNormalization, Conv2D, Activation, Dropout, AveragePooling2D, concatenate, \\\n",
    "    GlobalAveragePooling2D, MaxPooling2D, Dense, Input\n",
    "from keras.regularizers import l2\n",
    "import keras.backend as K\n",
    "import zipfile\n",
    "import evalu\n",
    "# import time\n",
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "# tf.config.experimental.set_memory_growth(gpus[1], True)\n",
    "# tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "# Read datasets\n",
    "def getimg(dir,num,j):\n",
    "    input_dir = dir\n",
    "    img_list = []\n",
    "    for i in range(1+j,num+j):\n",
    "        filename = f'{i}.tif'\n",
    "        img_path = os.path.join(input_dir,filename)\n",
    "        img = imageio.imread(img_path)\n",
    "        img = img.astype(np.float32)\n",
    "        img = img / 255.0\n",
    "        img_list.append(img)\n",
    "    return img_list\n",
    "\n",
    "def onehot(img):\n",
    "    img = tf.cast(img, tf.int32)\n",
    "    img_one_hot = tf.one_hot(img, depth=2, on_value=1.0, off_value=0.0, axis=-1)\n",
    "    return img_one_hot\n",
    "\n",
    "# def add_channel(img): \n",
    "#     img = tf.expand_dims(img, axis=-1) \n",
    "#     return img\n",
    "# Draw loss curve\n",
    "def plot_history(history, result_dir, prefix):\n",
    "    \"\"\"\n",
    "    将训练与验证的accuracy与loss画出来\n",
    "    \"\"\"\n",
    "    plt.plot(history.history['accuracy'], marker='.')\n",
    "    plt.plot(history.history['val_accuracy'], marker='.')\n",
    "    plt.title('model accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.grid()\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend(['acc', 'val_acc'], loc='upper right')\n",
    "    # plt.show()\n",
    "    plt.savefig(result_dir + 'unet_ace' + prefix + '.png')\n",
    "    plt.close()\n",
    "\n",
    "    plt.plot(history.history['loss'], marker='.')\n",
    "    plt.plot(history.history['val_loss'], marker='.')\n",
    "    plt.title('model loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.yscale(\"log\")\n",
    "    plt.grid()\n",
    "    plt.legend(['loss', 'val_loss'], loc='upper right')\n",
    "    # plt.show()\n",
    "    plt.savefig(result_dir + 'unet_loss'+ prefix +'.png')\n",
    "    plt.close()\n",
    "\n",
    "    # x = history.history['loss']\n",
    "    # np.savetxt('D:/pycharm/up_down_code/loss_picture/unet_train_loss.txt', x, fmt='%f')\n",
    "    # y = history.history['val_loss']\n",
    "    # np.savetxt('D:/pycharm/up_down_code/loss_picture/unet_val_loss.txt', y, fmt='%f')hb\n",
    "\n",
    "\n",
    "def Conv_Block(input_tensor, filters, bottleneck=False, weight_decay=1e-4):\n",
    "    \"\"\"    封装卷积层\n",
    "\n",
    "    :param input_tensor: 输入张量\n",
    "    :param filters: 卷积核数目\n",
    "    :param bottleneck: 是否使用bottleneck\n",
    "    :param dropout_rate: dropout比率\n",
    "    :param weight_decay: 权重衰减率\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    concat_axis = 1 if K.image_data_format() == 'channel_first' else -1  # 确定格式\n",
    "\n",
    "    x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5)(input_tensor)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # if bottleneck:\n",
    "    #     # 使用bottleneck进行降维\n",
    "    #     inter_channel = filters\n",
    "    #     x = Conv2D(inter_channel, (1, 1),\n",
    "    #                kernel_initializer='he_normal',\n",
    "    #                padding='same', use_bias=False,\n",
    "    #                kernel_regularizer=l2(weight_decay))(x)\n",
    "    #     x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5)(x)\n",
    "    #     x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters, (3, 3), kernel_initializer='he_normal', padding='same', use_bias=False)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def dens_block(input_tensor, nb_filter):\n",
    "    x1 = Conv_Block(input_tensor, nb_filter)\n",
    "    add1 = concatenate([x1, input_tensor], axis=-1)\n",
    "    x2 = Conv_Block(add1, nb_filter)\n",
    "    add2 = concatenate([x1, input_tensor, x2], axis=-1)\n",
    "    x3 = Conv_Block(add2, nb_filter)\n",
    "    return x3\n",
    "\n",
    "# model definition\n",
    "def unet(input_shape=(128, 128, 2)):\n",
    "    # tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "    inputs = Input(input_shape)\n",
    "    # input1=Input(shape=(128,128,1))\n",
    "    # input2=Input(shape=(128,128,1))\n",
    "    # inputs=Concatenate(axis=-1)([input1,input2])\n",
    "    # inputs = Input(shape\n",
    "    # x  = Conv2D(32, 1, strides=1, activation='relu', padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    x = Conv2D(32, 7, kernel_initializer='he_normal', padding='same', strides=1, use_bias=False,\n",
    "               kernel_regularizer=l2(1e-4))(inputs)\n",
    "    # down first\n",
    "    down1 = dens_block(x, nb_filter=64)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(down1)  # 256\n",
    "    # down second\n",
    "    down2 = dens_block(pool1, nb_filter=64)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(down2)  # 128\n",
    "    # down third\n",
    "    down3 = dens_block(pool2, nb_filter=128)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(down3)  # 64\n",
    "    # down four\n",
    "    down4 = dens_block(pool3, nb_filter=256)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(down4)  # 32\n",
    "    # center\n",
    "    conv5 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool4)\n",
    "    conv5 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    # up first\n",
    "    up6 = UpSampling2D(size=(2, 2))(drop5)\n",
    "    # up6 = UpSampling2D(size=(2, 2))(drop5)\n",
    "    add6 = concatenate([down4, up6], axis=3)\n",
    "    up6 = dens_block(add6, nb_filter=256)\n",
    "    # up second\n",
    "    up7 = UpSampling2D(size=(2, 2))(up6)\n",
    "    # up7 = UpSampling2D(size=(2, 2))(conv6)\n",
    "    add7 = concatenate([down3, up7], axis=3)\n",
    "    up7 = dens_block(add7, nb_filter=128)\n",
    "    # up third\n",
    "    up8 = UpSampling2D(size=(2, 2))(up7)\n",
    "    # up8 = UpSampling2D(size=(2, 2))(conv7)\n",
    "    add8 = concatenate([down2, up8], axis=-1)\n",
    "    up8 = dens_block(add8, nb_filter=64)\n",
    "    # up four\n",
    "    up9 = UpSampling2D(size=(2, 2))(up8)\n",
    "    add9 = concatenate([down1, up9], axis=-1)\n",
    "    up9 = dens_block(add9, nb_filter=64)\n",
    "    # output\n",
    "    conv10 = Conv2D(32, 7, strides=1, activation='relu', padding='same', kernel_initializer='he_normal')(up9)\n",
    "    conv10 = Conv2D(2, 1, activation='softmax')(conv10)\n",
    "    # model = Model(inputs=[input1,input2], outputs=conv10)\n",
    "    model = Model(inputs=inputs, outputs=conv10)\n",
    "    # print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "# define Huber loss\n",
    "def huber_loss(y_true, y_pred):\n",
    "    return tf.losses.huber_loss(y_true, y_pred, delta=0.01)\n",
    "\n",
    "\n",
    "def simm_loss(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_pred - y_true)) + 0.01 * K.mean(K.abs(y_pred))\n",
    "    # return tf.abs(tf.norm(y_pred - y_true))/tf.norm(y_true)\n",
    "    \n",
    "# smooth = 1. # 用于防止分母为0.\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true) # 将 y_true 拉伸为一维.\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection) / (K.sum(y_true_f * y_true_f) + K.sum(y_pred_f * y_pred_f))\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1. - dice_coef(y_true, y_pred)\n",
    "\n",
    "# Define the learning rate attenuation value\n",
    "def scheduler(epoch):\n",
    "    if epoch % 10 == 0 and epoch != 0:\n",
    "        lr = K.get_value(model.optimizer.lr)\n",
    "        K.set_value(model.optimizer.lr, lr * 0.1)\n",
    "        print(\"lr change to {}\".format(lr * 0.1))\n",
    "    return K.get_value(model.optimizer.lr)\n",
    "\n",
    "import random\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "  0.001,\n",
    "  decay_steps=1000,\n",
    "  decay_rate=1,\n",
    "  staircase=False)\n",
    "        \n",
    "def make_zip(source_dir, output_name):\n",
    "    zipf = zipfile.ZipFile(output_name, 'w')\n",
    "    prelen = len(os.path.dirname(source_dir))\n",
    "    for parent, _, filenames in os.walk(source_dir):\n",
    "        for filename in filenames:\n",
    "            pathfile = os.path.join(parent, filename)\n",
    "            arcname = pathfile[prelen:].strip(os.path.sep)     #相对路径\n",
    "            zipf.write(pathfile, arcname)\n",
    "        zipf.close()\n",
    "\n",
    "def train(model, model_savename, train_dataset, validation_dataset):\n",
    "    # model.compile(optimizer=Adam(learning_rate=lr_schedule), loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "    # reduce_lr = LearningRateScheduler(scheduler)\n",
    "    # reduce_lr = LearningRateScheduler(lschedule)\n",
    "    # model.load_weights('/code/save_model/exped/'+model_savename)\n",
    "    model_checkpoint = ModelCheckpoint('/code/save_model/exped/'+model_savename, monitor='loss', verbose=0,\n",
    "                                       save_best_only=True)\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=200)\n",
    "    Board = tf.keras.callbacks.TensorBoard(log_dir=\"/output/logs\")\n",
    "    \n",
    "    history = model.fit(train_dataset,\n",
    "                                  steps_per_epoch=100,\n",
    "                                  epochs=500,\n",
    "                                  validation_data=validation_dataset,\n",
    "                                #   validation_steps=50,\n",
    "                                  callbacks=[model_checkpoint,\n",
    "                                             early_stop,Board\n",
    "                                             # Board\n",
    "                                             ])\n",
    "    plot_history(history, '/code/loss_picture/extended/',prefix='_exped')\n",
    "    return model\n",
    "    \n",
    "def test(model, test_dataset, output_path, length):\n",
    "    mask = model.predict(test_dataset)\n",
    "    mask = tf.argmax(mask, axis=-1)\n",
    "    mask = tf.keras.backend.eval(mask)\n",
    "    mask = (mask * 255).astype(np.uint8)\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    for j in range(0, length):\n",
    "        cv2.imwrite(output_path+ '%d.tif' %(j+1), mask[j])\n",
    "    print('model predict OK')\n",
    "        \n",
    "BATCH_SIZE=128\n",
    "plot_path = '/code/loss_picture/extended/'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    output_path = '/data/caizhizheng/2D/v2/exped_output/'\n",
    "    label_path = '/data/caizhizheng/2D/v2/label/label/'\n",
    "    img_path = '/data/caizhizheng/2D/v2/data1/data/'\n",
    "    img1_list = getimg(img_path,num=1901, j=0)\n",
    "    img_path = '/data/caizhizheng/2D/v2/data2/data/'\n",
    "    img2_list = getimg(img_path,num=1901, j=900)\n",
    "    label_list = getimg(label_path,num=1901, j=0)\n",
    "    img_list = np.stack((img1_list,img2_list),axis = -1)\n",
    "    \n",
    "    test_path = '/data/caizhizheng/2D/v2/test/predicted_input_data/'\n",
    "    test_label_path = '/data/caizhizheng/2D/v2/test/predicted_label/'\n",
    "    # output_path = '/data/caizhizheng/2D/v2/test/outputv2/'\n",
    "    img1_list = getimg(test_path,num=101, j =0)\n",
    "    img2_list = getimg(test_path,num=101, j =100)\n",
    "    test_img_list = np.stack((img1_list,img2_list),axis = -1)\n",
    "    test_label_list = getimg(test_label_path, num=101, j=0)\n",
    "    \n",
    "    img_list = np.vstack((img_list, test_img_list))\n",
    "    label_list = np.vstack((label_list, test_label_list))\n",
    "    \n",
    "    image_dataset = tf.data.Dataset.from_tensor_slices(img_list)\n",
    "    label_dataset = tf.data.Dataset.from_tensor_slices(label_list)\n",
    "    label_dataset = label_dataset.map(onehot)\n",
    "    # 使用事先保存的数据集会造成内存泄漏？？\n",
    "    # image_dataset = tf.data.experimental.load(\"dataset/image_exped\")\n",
    "    # label_dataset = tf.data.experimental.load(\"dataset/label_exped\")\n",
    "    # x = next(iter(image_dataset.batch(image_dataset.cardinality().numpy())))\n",
    "    # y = next(iter(label_dataset.batch(label_dataset.cardinality().numpy())))\n",
    "    # origin_dataset = tf.data.Dataset.zip((image_dataset, label_dataset))\n",
    "    # origin_dataset = origin_dataset.shuffle(origin_dataset.cardinality())\n",
    "\n",
    "    # validation_dataset = origin_dataset.take(100).batch(BATCH_SIZE)\n",
    "    # validation_dataset = validation_dataset.cache().prefetch(tf.data.AUTOTUNE)\n",
    "    # train_dataset = origin_dataset.skip(100)\n",
    "    # train_dataset = train_dataset.cache().shuffle(train_dataset.cardinality(),reshuffle_each_iteration=True).repeat().batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "    with mirrored_strategy.scope():\n",
    "        model = unet(input_shape=(128, 128, 2))\n",
    "        model.compile(optimizer=Adam(learning_rate=lr_schedule), loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "        # model.load_weights('/code/save_model/exped/cross_ini_277.keras')\n",
    "    model_savename = 'test_tv.keras'\n",
    "    # train = train(model, model_savename, train_dataset, validation_dataset)\n",
    "    \n",
    "    # image_dataset = image_dataset.batch(BATCH_SIZE)\n",
    "    # model.load_weights('/code/save_model/exped/'+model_savename)\n",
    "    # test(model, image_dataset, output_path, 2000)\n",
    "    # # name = 'error2' + '_postCrV' + '.csv'\n",
    "    error_path = './errors/'\n",
    "    # name = 'error_expedv2' + '.csv'\n",
    "    # name = os.path.join(error_path, name)\n",
    "    # evalu.main(output_dir=output_path, label_dir=label_path, name=name, oder=np.array(0))\n",
    "    \n",
    "#     测试集\n",
    "    test_path = '/data/caizhizheng/2D/v2/test/test2/data/'\n",
    "    test_label_path = '/data/caizhizheng/2D/v2/test/test2/label/'\n",
    "    output_path = '/data/caizhizheng/2D/v2/test/outputv2/'\n",
    "    img1_list = getimg(test_path,num=201, j =0)\n",
    "    img2_list = getimg(test_path,num=201, j =200)\n",
    "    img_list = np.stack((img1_list,img2_list),axis = -1)\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices(img_list)\n",
    "    test_dataset = test_dataset.batch(1)\n",
    "    model.load_weights('/code/save_model/exped/'+model_savename)\n",
    "    test(model, test_dataset, output_path,200)\n",
    "    name = 'error_exped_testv2.csv'\n",
    "    name = os.path.join(error_path, name)\n",
    "    evalu.main(output_dir=output_path, label_dir=test_label_path, name=name, oder=np.array(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7609ec7-e6f1-491f-8146-4adfd7e38897",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平均汉明距离为：28.0100\n",
      "平均杰卡德相似系数为：0.9754\n",
      "平均Dice相似系数为：0.9774\n",
      "平均误差为：3.6096%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import evalu\n",
    "import numpy as np\n",
    "output_path = '/data/caizhizheng/2D/v2/exped_output/' \n",
    "test_label_path = '/data/caizhizheng/2D/v2/test/predicted_label/'\n",
    "error_path = './errors/'\n",
    "name = 'error_expedv2_remain' + '.csv'\n",
    "name = os.path.join(error_path, name)\n",
    "evalu.main(output_dir=output_path, label_dir=test_label_path, name=name, oder=np.array(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec77b17-60d8-4c3e-8447-dc8507f6aca2",
   "metadata": {},
   "source": [
    "### testtv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b500b8f8-580f-4d5a-9db0-f67051a73a0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "平均汉明距离为：7.8300\n",
      "平均杰卡德相似系数为：0.9982\n",
      "平均Dice相似系数为：0.9991\n",
      "平均误差为：0.6884%\n"
     ]
    }
   ],
   "source": [
    "# %load unet_testtv\n",
    "import os\n",
    "# import shutil\n",
    "# import skimage.io as io\n",
    "# import skimage.transform as trans\n",
    "from keras.optimizer_v2.adam import Adam\n",
    "# from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "# from keras import backend as keras\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import imageio.v2 as imageio\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.layers import BatchNormalization, Conv2D, Activation, Dropout, AveragePooling2D, concatenate, \\\n",
    "    GlobalAveragePooling2D, MaxPooling2D, Dense, Input\n",
    "from keras.regularizers import l2\n",
    "import keras.backend as K\n",
    "import zipfile\n",
    "import evalu\n",
    "# import time\n",
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "# tf.config.experimental.set_memory_growth(gpus[1], True)\n",
    "# tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "# Read datasets\n",
    "def getimg(dir,num,j):\n",
    "    input_dir = dir\n",
    "    img_list = []\n",
    "    for i in range(1+j,num+j):\n",
    "        filename = f'{i}.tif'\n",
    "        img_path = os.path.join(input_dir,filename)\n",
    "        img = imageio.imread(img_path)\n",
    "        img = img.astype(np.float32)\n",
    "        img = img / 255.0\n",
    "        img_list.append(img)\n",
    "    return img_list\n",
    "\n",
    "def onehot(img):\n",
    "    img = tf.cast(img, tf.int32)\n",
    "    img_one_hot = tf.one_hot(img, depth=2, on_value=1.0, off_value=0.0, axis=-1)\n",
    "    return img_one_hot\n",
    "\n",
    "# def add_channel(img): \n",
    "#     img = tf.expand_dims(img, axis=-1) \n",
    "#     return img\n",
    "# Draw loss curve\n",
    "def plot_history(history, result_dir, prefix):\n",
    "    \"\"\"\n",
    "    将训练与验证的accuracy与loss画出来\n",
    "    \"\"\"\n",
    "    plt.plot(history.history['accuracy'], marker='.')\n",
    "    plt.plot(history.history['val_accuracy'], marker='.')\n",
    "    plt.title('model accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.grid()\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend(['acc', 'val_acc'], loc='upper right')\n",
    "    # plt.show()\n",
    "    plt.savefig(result_dir + 'unet_ace' + prefix + '.png')\n",
    "    plt.close()\n",
    "\n",
    "    plt.plot(history.history['loss'], marker='.')\n",
    "    plt.plot(history.history['val_loss'], marker='.')\n",
    "    plt.title('model loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.yscale(\"log\")\n",
    "    plt.grid()\n",
    "    plt.legend(['loss', 'val_loss'], loc='upper right')\n",
    "    # plt.show()\n",
    "    plt.savefig(result_dir + 'unet_loss'+ prefix +'.png')\n",
    "    plt.close()\n",
    "\n",
    "    # x = history.history['loss']\n",
    "    # np.savetxt('D:/pycharm/up_down_code/loss_picture/unet_train_loss.txt', x, fmt='%f')\n",
    "    # y = history.history['val_loss']\n",
    "    # np.savetxt('D:/pycharm/up_down_code/loss_picture/unet_val_loss.txt', y, fmt='%f')hb\n",
    "\n",
    "\n",
    "def Conv_Block(input_tensor, filters, bottleneck=False, weight_decay=1e-4):\n",
    "    \"\"\"    封装卷积层\n",
    "\n",
    "    :param input_tensor: 输入张量\n",
    "    :param filters: 卷积核数目\n",
    "    :param bottleneck: 是否使用bottleneck\n",
    "    :param dropout_rate: dropout比率\n",
    "    :param weight_decay: 权重衰减率\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    concat_axis = 1 if K.image_data_format() == 'channel_first' else -1  # 确定格式\n",
    "\n",
    "    x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5)(input_tensor)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # if bottleneck:\n",
    "    #     # 使用bottleneck进行降维\n",
    "    #     inter_channel = filters\n",
    "    #     x = Conv2D(inter_channel, (1, 1),\n",
    "    #                kernel_initializer='he_normal',\n",
    "    #                padding='same', use_bias=False,\n",
    "    #                kernel_regularizer=l2(weight_decay))(x)\n",
    "    #     x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5)(x)\n",
    "    #     x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters, (3, 3), kernel_initializer='he_normal', padding='same', use_bias=False)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def dens_block(input_tensor, nb_filter):\n",
    "    x1 = Conv_Block(input_tensor, nb_filter)\n",
    "    add1 = concatenate([x1, input_tensor], axis=-1)\n",
    "    x2 = Conv_Block(add1, nb_filter)\n",
    "    add2 = concatenate([x1, input_tensor, x2], axis=-1)\n",
    "    x3 = Conv_Block(add2, nb_filter)\n",
    "    return x3\n",
    "\n",
    "# model definition\n",
    "def unet(input_shape=(128, 128, 2)):\n",
    "    # tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "    inputs = Input(input_shape)\n",
    "    # input1=Input(shape=(128,128,1))\n",
    "    # input2=Input(shape=(128,128,1))\n",
    "    # inputs=Concatenate(axis=-1)([input1,input2])\n",
    "    # inputs = Input(shape\n",
    "    # x  = Conv2D(32, 1, strides=1, activation='relu', padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    x = Conv2D(32, 7, kernel_initializer='he_normal', padding='same', strides=1, use_bias=False,\n",
    "               kernel_regularizer=l2(1e-4))(inputs)\n",
    "    # down first\n",
    "    down1 = dens_block(x, nb_filter=64)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(down1)  # 256\n",
    "    # down second\n",
    "    down2 = dens_block(pool1, nb_filter=64)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(down2)  # 128\n",
    "    # down third\n",
    "    down3 = dens_block(pool2, nb_filter=128)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(down3)  # 64\n",
    "    # down four\n",
    "    down4 = dens_block(pool3, nb_filter=256)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(down4)  # 32\n",
    "    # center\n",
    "    conv5 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool4)\n",
    "    conv5 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    # up first\n",
    "    up6 = UpSampling2D(size=(2, 2))(drop5)\n",
    "    # up6 = UpSampling2D(size=(2, 2))(drop5)\n",
    "    add6 = concatenate([down4, up6], axis=3)\n",
    "    up6 = dens_block(add6, nb_filter=256)\n",
    "    # up second\n",
    "    up7 = UpSampling2D(size=(2, 2))(up6)\n",
    "    # up7 = UpSampling2D(size=(2, 2))(conv6)\n",
    "    add7 = concatenate([down3, up7], axis=3)\n",
    "    up7 = dens_block(add7, nb_filter=128)\n",
    "    # up third\n",
    "    up8 = UpSampling2D(size=(2, 2))(up7)\n",
    "    # up8 = UpSampling2D(size=(2, 2))(conv7)\n",
    "    add8 = concatenate([down2, up8], axis=-1)\n",
    "    up8 = dens_block(add8, nb_filter=64)\n",
    "    # up four\n",
    "    up9 = UpSampling2D(size=(2, 2))(up8)\n",
    "    add9 = concatenate([down1, up9], axis=-1)\n",
    "    up9 = dens_block(add9, nb_filter=64)\n",
    "    # output\n",
    "    conv10 = Conv2D(32, 7, strides=1, activation='relu', padding='same', kernel_initializer='he_normal')(up9)\n",
    "    conv10 = Conv2D(2, 1, activation='softmax')(conv10)\n",
    "    # model = Model(inputs=[input1,input2], outputs=conv10)\n",
    "    model = Model(inputs=inputs, outputs=conv10)\n",
    "    # print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "# define Huber loss\n",
    "def huber_loss(y_true, y_pred):\n",
    "    return tf.losses.huber_loss(y_true, y_pred, delta=0.01)\n",
    "\n",
    "\n",
    "def simm_loss(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_pred - y_true)) + 0.01 * K.mean(K.abs(y_pred))\n",
    "    # return tf.abs(tf.norm(y_pred - y_true))/tf.norm(y_true)\n",
    "    \n",
    "# smooth = 1. # 用于防止分母为0.\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true) # 将 y_true 拉伸为一维.\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection) / (K.sum(y_true_f * y_true_f) + K.sum(y_pred_f * y_pred_f))\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1. - dice_coef(y_true, y_pred)\n",
    "\n",
    "# Define the learning rate attenuation value\n",
    "def scheduler(epoch):\n",
    "    if epoch % 10 == 0 and epoch != 0:\n",
    "        lr = K.get_value(model.optimizer.lr)\n",
    "        K.set_value(model.optimizer.lr, lr * 0.1)\n",
    "        print(\"lr change to {}\".format(lr * 0.1))\n",
    "    return K.get_value(model.optimizer.lr)\n",
    "\n",
    "import random\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "  0.001,\n",
    "  decay_steps=1000,\n",
    "  decay_rate=1,\n",
    "  staircase=False)\n",
    "        \n",
    "def make_zip(source_dir, output_name):\n",
    "    zipf = zipfile.ZipFile(output_name, 'w')\n",
    "    prelen = len(os.path.dirname(source_dir))\n",
    "    for parent, _, filenames in os.walk(source_dir):\n",
    "        for filename in filenames:\n",
    "            pathfile = os.path.join(parent, filename)\n",
    "            arcname = pathfile[prelen:].strip(os.path.sep)     #相对路径\n",
    "            zipf.write(pathfile, arcname)\n",
    "        zipf.close()\n",
    "\n",
    "def train(model, model_savename, train_dataset, validation_dataset):\n",
    "    # model.compile(optimizer=Adam(learning_rate=lr_schedule), loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "    # reduce_lr = LearningRateScheduler(scheduler)\n",
    "    # reduce_lr = LearningRateScheduler(lschedule)\n",
    "    # model.load_weights('/code/save_model/exped/'+model_savename)\n",
    "    model_checkpoint = ModelCheckpoint('/code/save_model/exped/'+model_savename, monitor='loss', verbose=0,\n",
    "                                       save_best_only=True)\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=200)\n",
    "    Board = tf.keras.callbacks.TensorBoard(log_dir=\"/output/logs\")\n",
    "    \n",
    "    history = model.fit(train_dataset,\n",
    "                                  steps_per_epoch=100,\n",
    "                                  epochs=1000,\n",
    "                                  validation_data=validation_dataset,\n",
    "                                #   validation_steps=50,\n",
    "                                  callbacks=[model_checkpoint,\n",
    "                                             early_stop,Board\n",
    "                                             # Board\n",
    "                                             ])\n",
    "    plot_history(history, '/code/loss_picture/extended/',prefix='_exped')\n",
    "    return model\n",
    "    \n",
    "def test(model, test_dataset, output_path, length):\n",
    "    mask = model.predict(test_dataset)\n",
    "    mask = tf.argmax(mask, axis=-1)\n",
    "    mask = tf.keras.backend.eval(mask)\n",
    "    mask = (mask * 255).astype(np.uint8)\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    for j in range(0, length):\n",
    "        cv2.imwrite(output_path+ '%d.tif' %(j+1), mask[j])\n",
    "    print('model predict OK')\n",
    "        \n",
    "BATCH_SIZE=128\n",
    "plot_path = '/code/loss_picture/extended/'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    output_path = '/data/caizhizheng/2D/v2/exped_output/'\n",
    "    label_path = '/data/caizhizheng/2D/v2/label/label/'\n",
    "    img_path = '/data/caizhizheng/2D/v2/data1/data/'\n",
    "    img1_list = getimg(img_path,num=2901, j=0)\n",
    "    img_path = '/data/caizhizheng/2D/v2/data2/data/'\n",
    "    img2_list = getimg(img_path,num=2901, j=900)\n",
    "    label_list = getimg(label_path,num=2901, j=0)\n",
    "    img_list = np.stack((img1_list,img2_list),axis = -1)\n",
    "    \n",
    "    test_path = '/data/caizhizheng/2D/v2/test/predicted_input_data/'\n",
    "    test_label_path = '/data/caizhizheng/2D/v2/test/predicted_label/'\n",
    "    # output_path = '/data/caizhizheng/2D/v2/test/outputv2/'\n",
    "    img1_list = getimg(test_path,num=101, j =0)\n",
    "    img2_list = getimg(test_path,num=101, j =100)\n",
    "    test_img_list = np.stack((img1_list,img2_list),axis = -1)\n",
    "    test_label_list = getimg(test_label_path, num=101, j=0)\n",
    "    \n",
    "    img_list = np.vstack((img_list, test_img_list))\n",
    "    label_list = np.vstack((label_list, test_label_list))\n",
    "    \n",
    "    image_dataset = tf.data.Dataset.from_tensor_slices(img_list)\n",
    "    label_dataset = tf.data.Dataset.from_tensor_slices(label_list)\n",
    "    label_dataset = label_dataset.map(onehot)\n",
    "    # 使用事先保存的数据集会造成内存泄漏？？\n",
    "    # image_dataset = tf.data.experimental.load(\"dataset/image_exped\")\n",
    "    # label_dataset = tf.data.experimental.load(\"dataset/label_exped\")\n",
    "    # x = next(iter(image_dataset.batch(image_dataset.cardinality().numpy())))\n",
    "    # y = next(iter(label_dataset.batch(label_dataset.cardinality().numpy())))\n",
    "    origin_dataset = tf.data.Dataset.zip((image_dataset, label_dataset))\n",
    "    origin_dataset = origin_dataset.shuffle(origin_dataset.cardinality(), seed=73)\n",
    "\n",
    "    validation_dataset = origin_dataset.take(100).batch(BATCH_SIZE)\n",
    "    validation_dataset = validation_dataset.cache().prefetch(tf.data.AUTOTUNE)\n",
    "    train_dataset = origin_dataset.skip(100)\n",
    "    train_dataset = train_dataset.cache().shuffle(train_dataset.cardinality(),seed=1919810, reshuffle_each_iteration=True).repeat().batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "    with mirrored_strategy.scope():\n",
    "        model = unet(input_shape=(128, 128, 2))\n",
    "        model.compile(optimizer=Adam(learning_rate=lr_schedule), loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "        # model.load_weights('/code/save_model/exped/cross_ini_277.keras')\n",
    "    model_savename = 'test_tv2.keras'\n",
    "    # train = train(model, model_savename, train_dataset, validation_dataset)\n",
    "    \n",
    "    # image_dataset = image_dataset.batch(BATCH_SIZE)\n",
    "    # test(model, image_dataset, output_path, 3000)\n",
    "    # name = 'error2' + '_postCrV' + '.csv'\n",
    "    error_path = './errors/'\n",
    "    name = 'error_expedv2_remain' + '.csv'\n",
    "    name = os.path.join(error_path, name)\n",
    "    evalu.main(output_dir=output_path, label_dir=test_label_path, name=name, oder=np.array(0))\n",
    "    \n",
    "#     测试集\n",
    "    # test_path = '/data/caizhizheng/2D/v2/test/test2/data/'\n",
    "    # test_label_path = '/data/caizhizheng/2D/v2/test/test2/label/'\n",
    "    # output_path = '/data/caizhizheng/2D/v2/test/outputv2/'\n",
    "    # img1_list = getimg(test_path,num=201, j =0)\n",
    "    # img2_list = getimg(test_path,num=201, j =200)\n",
    "    # img_list = np.stack((img1_list,img2_list),axis = -1)\n",
    "    # test_dataset = tf.data.Dataset.from_tensor_slices(img_list)\n",
    "    # test_dataset = test_dataset.batch(1)\n",
    "    # model.load_weights('/code/save_model/exped/'+model_savename)\n",
    "    # test(model, test_dataset, output_path,200)\n",
    "    # name = 'error_exped_testv2.csv'\n",
    "    # name = os.path.join(error_path, name)\n",
    "    # evalu.main(output_dir=output_path, label_dir=test_label_path, name=name, oder=np.array(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bcf222-45bb-46aa-8b2b-c37e2672d8f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "Epoch 1/1000\n"
     ]
    }
   ],
   "source": [
    "# %load unet_tv\n",
    "import os\n",
    "# import shutil\n",
    "# import skimage.io as io\n",
    "# import skimage.transform as trans\n",
    "from keras.optimizer_v2.adam import Adam\n",
    "# from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "# from keras import backend as keras\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import imageio.v2 as imageio\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.layers import BatchNormalization, Conv2D, Activation, Dropout, AveragePooling2D, concatenate, \\\n",
    "    GlobalAveragePooling2D, MaxPooling2D, Dense, Input\n",
    "from keras.regularizers import l2\n",
    "import keras.backend as K\n",
    "import zipfile\n",
    "import evalu\n",
    "# import time\n",
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "# tf.config.experimental.set_memory_growth(gpus[1], True)\n",
    "# tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "# Read datasets\n",
    "def getimg(dir,num,j):\n",
    "    input_dir = dir\n",
    "    img_list = []\n",
    "    for i in range(1+j,num+j):\n",
    "        filename = f'{i}.tif'\n",
    "        img_path = os.path.join(input_dir,filename)\n",
    "        img = imageio.imread(img_path)\n",
    "        img = img.astype(np.float32)\n",
    "        img = img / 255.0\n",
    "        img_list.append(img)\n",
    "    return img_list\n",
    "\n",
    "def onehot(img):\n",
    "    img = tf.cast(img, tf.int32)\n",
    "    img_one_hot = tf.one_hot(img, depth=2, on_value=1.0, off_value=0.0, axis=-1)\n",
    "    return img_one_hot\n",
    "\n",
    "# def add_channel(img): \n",
    "#     img = tf.expand_dims(img, axis=-1) \n",
    "#     return img\n",
    "# Draw loss curve\n",
    "def plot_history(history, result_dir, prefix):\n",
    "    \"\"\"\n",
    "    将训练与验证的accuracy与loss画出来\n",
    "    \"\"\"\n",
    "    plt.plot(history.history['accuracy'], marker='.')\n",
    "    plt.plot(history.history['val_accuracy'], marker='.')\n",
    "    plt.title('model accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.grid()\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend(['acc', 'val_acc'], loc='upper right')\n",
    "    # plt.show()\n",
    "    plt.savefig(result_dir + 'unet_ace' + prefix + '.png')\n",
    "    plt.close()\n",
    "\n",
    "    plt.plot(history.history['loss'], marker='.')\n",
    "    plt.plot(history.history['val_loss'], marker='.')\n",
    "    plt.title('model loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.yscale(\"log\")\n",
    "    plt.grid()\n",
    "    plt.legend(['loss', 'val_loss'], loc='upper right')\n",
    "    # plt.show()\n",
    "    plt.savefig(result_dir + 'unet_loss'+ prefix +'.png')\n",
    "    plt.close()\n",
    "\n",
    "    # x = history.history['loss']\n",
    "    # np.savetxt('D:/pycharm/up_down_code/loss_picture/unet_train_loss.txt', x, fmt='%f')\n",
    "    # y = history.history['val_loss']\n",
    "    # np.savetxt('D:/pycharm/up_down_code/loss_picture/unet_val_loss.txt', y, fmt='%f')hb\n",
    "\n",
    "\n",
    "def Conv_Block(input_tensor, filters, bottleneck=False, weight_decay=1e-4):\n",
    "    \"\"\"    封装卷积层\n",
    "\n",
    "    :param input_tensor: 输入张量\n",
    "    :param filters: 卷积核数目\n",
    "    :param bottleneck: 是否使用bottleneck\n",
    "    :param dropout_rate: dropout比率\n",
    "    :param weight_decay: 权重衰减率\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    concat_axis = 1 if K.image_data_format() == 'channel_first' else -1  # 确定格式\n",
    "\n",
    "    x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5)(input_tensor)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # if bottleneck:\n",
    "    #     # 使用bottleneck进行降维\n",
    "    #     inter_channel = filters\n",
    "    #     x = Conv2D(inter_channel, (1, 1),\n",
    "    #                kernel_initializer='he_normal',\n",
    "    #                padding='same', use_bias=False,\n",
    "    #                kernel_regularizer=l2(weight_decay))(x)\n",
    "    #     x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5)(x)\n",
    "    #     x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters, (3, 3), kernel_initializer='he_normal', padding='same', use_bias=False)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def dens_block(input_tensor, nb_filter):\n",
    "    x1 = Conv_Block(input_tensor, nb_filter)\n",
    "    add1 = concatenate([x1, input_tensor], axis=-1)\n",
    "    x2 = Conv_Block(add1, nb_filter)\n",
    "    add2 = concatenate([x1, input_tensor, x2], axis=-1)\n",
    "    x3 = Conv_Block(add2, nb_filter)\n",
    "    return x3\n",
    "\n",
    "# model definition\n",
    "def unet(input_shape=(128, 128, 2)):\n",
    "    # tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "    inputs = Input(input_shape)\n",
    "    # input1=Input(shape=(128,128,1))\n",
    "    # input2=Input(shape=(128,128,1))\n",
    "    # inputs=Concatenate(axis=-1)([input1,input2])\n",
    "    # inputs = Input(shape\n",
    "    # x  = Conv2D(32, 1, strides=1, activation='relu', padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    x = Conv2D(32, 7, kernel_initializer='he_normal', padding='same', strides=1, use_bias=False,\n",
    "               kernel_regularizer=l2(1e-4))(inputs)\n",
    "    # down first\n",
    "    down1 = dens_block(x, nb_filter=64)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(down1)  # 256\n",
    "    # down second\n",
    "    down2 = dens_block(pool1, nb_filter=64)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(down2)  # 128\n",
    "    # down third\n",
    "    down3 = dens_block(pool2, nb_filter=128)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(down3)  # 64\n",
    "    # down four\n",
    "    down4 = dens_block(pool3, nb_filter=256)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(down4)  # 32\n",
    "    # center\n",
    "    conv5 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool4)\n",
    "    conv5 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    # up first\n",
    "    up6 = UpSampling2D(size=(2, 2))(drop5)\n",
    "    # up6 = UpSampling2D(size=(2, 2))(drop5)\n",
    "    add6 = concatenate([down4, up6], axis=3)\n",
    "    up6 = dens_block(add6, nb_filter=256)\n",
    "    # up second\n",
    "    up7 = UpSampling2D(size=(2, 2))(up6)\n",
    "    # up7 = UpSampling2D(size=(2, 2))(conv6)\n",
    "    add7 = concatenate([down3, up7], axis=3)\n",
    "    up7 = dens_block(add7, nb_filter=128)\n",
    "    # up third\n",
    "    up8 = UpSampling2D(size=(2, 2))(up7)\n",
    "    # up8 = UpSampling2D(size=(2, 2))(conv7)\n",
    "    add8 = concatenate([down2, up8], axis=-1)\n",
    "    up8 = dens_block(add8, nb_filter=64)\n",
    "    # up four\n",
    "    up9 = UpSampling2D(size=(2, 2))(up8)\n",
    "    add9 = concatenate([down1, up9], axis=-1)\n",
    "    up9 = dens_block(add9, nb_filter=64)\n",
    "    # output\n",
    "    conv10 = Conv2D(32, 7, strides=1, activation='relu', padding='same', kernel_initializer='he_normal')(up9)\n",
    "    conv10 = Conv2D(2, 1, activation='softmax')(conv10)\n",
    "    # model = Model(inputs=[input1,input2], outputs=conv10)\n",
    "    model = Model(inputs=inputs, outputs=conv10)\n",
    "    # print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "# define Huber loss\n",
    "def huber_loss(y_true, y_pred):\n",
    "    return tf.losses.huber_loss(y_true, y_pred, delta=0.01)\n",
    "\n",
    "\n",
    "def simm_loss(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_pred - y_true)) + 0.01 * K.mean(K.abs(y_pred))\n",
    "    # return tf.abs(tf.norm(y_pred - y_true))/tf.norm(y_true)\n",
    "    \n",
    "# smooth = 1. # 用于防止分母为0.\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true) # 将 y_true 拉伸为一维.\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection) / (K.sum(y_true_f * y_true_f) + K.sum(y_pred_f * y_pred_f))\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1. - dice_coef(y_true, y_pred)\n",
    "\n",
    "# Define the learning rate attenuation value\n",
    "def scheduler(epoch):\n",
    "    if epoch % 10 == 0 and epoch != 0:\n",
    "        lr = K.get_value(model.optimizer.lr)\n",
    "        K.set_value(model.optimizer.lr, lr * 0.1)\n",
    "        print(\"lr change to {}\".format(lr * 0.1))\n",
    "    return K.get_value(model.optimizer.lr)\n",
    "\n",
    "import random\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "  0.001,\n",
    "  decay_steps=1000,\n",
    "  decay_rate=1,\n",
    "  staircase=False)\n",
    "        \n",
    "def make_zip(source_dir, output_name):\n",
    "    zipf = zipfile.ZipFile(output_name, 'w')\n",
    "    prelen = len(os.path.dirname(source_dir))\n",
    "    for parent, _, filenames in os.walk(source_dir):\n",
    "        for filename in filenames:\n",
    "            pathfile = os.path.join(parent, filename)\n",
    "            arcname = pathfile[prelen:].strip(os.path.sep)     #相对路径\n",
    "            zipf.write(pathfile, arcname)\n",
    "        zipf.close()\n",
    "\n",
    "def train(model, model_savename, train_dataset, validation_dataset):\n",
    "    # model.compile(optimizer=Adam(learning_rate=lr_schedule), loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "    # reduce_lr = LearningRateScheduler(scheduler)\n",
    "    # reduce_lr = LearningRateScheduler(lschedule)\n",
    "    # model.load_weights('/code/save_model/exped/'+model_savename)\n",
    "    model_checkpoint = ModelCheckpoint('/code/save_model/exped/'+model_savename, monitor='loss', verbose=0,\n",
    "                                       save_best_only=True)\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=200)\n",
    "    Board = tf.keras.callbacks.TensorBoard(log_dir=\"/output/logs\")\n",
    "    \n",
    "    history = model.fit(train_dataset,\n",
    "                                  steps_per_epoch=100,\n",
    "                                  epochs=1000,\n",
    "                                  validation_data=validation_dataset,\n",
    "                                #   validation_steps=50,\n",
    "                                  callbacks=[model_checkpoint,\n",
    "                                             early_stop,Board\n",
    "                                             # Board\n",
    "                                             ])\n",
    "    plot_history(history, '/code/loss_picture/extended/',prefix='_exped')\n",
    "    return model\n",
    "    \n",
    "def test(model, test_dataset, output_path, length):\n",
    "    mask = model.predict(test_dataset)\n",
    "    mask = tf.argmax(mask, axis=-1)\n",
    "    mask = tf.keras.backend.eval(mask)\n",
    "    mask = (mask * 255).astype(np.uint8)\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    for j in range(0, length):\n",
    "        cv2.imwrite(output_path+ '%d.tif' %(j+1), mask[j])\n",
    "    print('model predict OK')\n",
    "        \n",
    "BATCH_SIZE=128\n",
    "plot_path = '/code/loss_picture/extended/'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    output_path = '/data/caizhizheng/2D/v2/exped_output/'\n",
    "    label_path = '/data/caizhizheng/2D/v2/label/label/'\n",
    "    img_path = '/data/caizhizheng/2D/v2/data1/data/'\n",
    "    img1_list = getimg(img_path,num=2901, j=0)\n",
    "    img_path = '/data/caizhizheng/2D/v2/data2/data/'\n",
    "    img2_list = getimg(img_path,num=2901, j=900)\n",
    "    label_list = getimg(label_path,num=2901, j=0)\n",
    "    img_list = np.stack((img1_list,img2_list),axis = -1)\n",
    "    \n",
    "    img_path2 = '/data/caizhizheng/2D/v2/test/predicted_input_data/'\n",
    "    label_path2 = '/data/caizhizheng/2D/v2/test/predicted_label/'\n",
    "    # output_path = '/data/caizhizheng/2D/v2/test/outputv2/'\n",
    "    img1_list = getimg(img_path2,num=101, j =0)\n",
    "    img2_list = getimg(img_path2,num=101, j =100)\n",
    "    test_img_list = np.stack((img1_list,img2_list),axis = -1)\n",
    "    test_label_list = getimg(label_path2, num=101, j=0)\n",
    "    \n",
    "    img_list = np.vstack((img_list, test_img_list))\n",
    "    label_list = np.vstack((label_list, test_label_list))\n",
    "    \n",
    "    image_dataset = tf.data.Dataset.from_tensor_slices(img_list)\n",
    "    label_dataset = tf.data.Dataset.from_tensor_slices(label_list)\n",
    "    label_dataset = label_dataset.map(onehot)\n",
    "    # 使用事先保存的数据集会造成内存泄漏？？\n",
    "    # image_dataset = tf.data.experimental.load(\"dataset/image_exped\")\n",
    "    # label_dataset = tf.data.experimental.load(\"dataset/label_exped\")\n",
    "    # x = next(iter(image_dataset.batch(image_dataset.cardinality().numpy())))\n",
    "    # y = next(iter(label_dataset.batch(label_dataset.cardinality().numpy())))\n",
    "    origin_dataset = tf.data.Dataset.zip((image_dataset, label_dataset))\n",
    "    # origin_dataset = origin_dataset.shuffle(origin_dataset.cardinality(), seed=73)\n",
    "\n",
    "    # validation_dataset = origin_dataset.take(100).batch(BATCH_SIZE)\n",
    "    # validation_dataset = validation_dataset.cache().prefetch(tf.data.AUTOTUNE)\n",
    "    train_dataset = origin_dataset\n",
    "    train_dataset = train_dataset.cache().shuffle(train_dataset.cardinality(),seed=1919810, reshuffle_each_iteration=True).repeat().batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    test_path = '/data/caizhizheng/2D/v2/test/test2/data/'\n",
    "    label_path = '/data/caizhizheng/2D/v2/test/test2/label/'\n",
    "    test_output_path = '/data/caizhizheng/2D/v2/test/outputv2/'\n",
    "    img1_list = getimg(test_path,num=201, j =0)\n",
    "    img2_list = getimg(test_path,num=201, j =200)\n",
    "    img_list = np.stack((img1_list,img2_list),axis = -1)\n",
    "    label_list = getimg(label_path,num=201,j=0)\n",
    "    tv_imgset = tf.data.Dataset.from_tensor_slices(img_list)\n",
    "    tv_labelset = tf.data.Dataset.from_tensor_slices(label_list)\n",
    "    tv_labelset = tv_labelset.map(onehot)\n",
    "    tv_set = tf.data.Dataset.zip((tv_imgset, tv_labelset))\n",
    "    validation_dataset = tv_set.take(100).batch(BATCH_SIZE)\n",
    "\n",
    "\n",
    "    mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "    with mirrored_strategy.scope():\n",
    "        model = unet(input_shape=(128, 128, 2))\n",
    "        model.compile(optimizer=Adam(learning_rate=lr_schedule), loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "        # model.load_weights('/code/save_model/exped/cross_ini_277.keras')\n",
    "    model_savename = 'tv2.keras'\n",
    "    train = train(model, model_savename, train_dataset, validation_dataset)\n",
    "    \n",
    "    image_dataset = image_dataset.batch(BATCH_SIZE)\n",
    "    test(model, image_dataset, output_path, 3000)\n",
    "    # name = 'error2' + '_postCrV' + '.csv'\n",
    "    error_path = './errors/'\n",
    "    name = 'error_expedv2' + '.csv'\n",
    "    name = os.path.join(error_path, name)\n",
    "    evalu.main(output_dir=output_path, label_dir=label_path, name=name, oder=np.array(0))\n",
    "    \n",
    "#     测试集\n",
    "\n",
    "    # test_dataset = tf.data.Dataset.from_tensor_slices(img_list)\n",
    "    test_dataset = tv_imgset.batch(1)\n",
    "    model.load_weights('/code/save_model/exped/'+model_savename)\n",
    "    test(model, test_dataset, test_output_path,200)\n",
    "    name = 'error_exped_testv2.csv'\n",
    "    name = os.path.join(error_path, name)\n",
    "    evalu.main(output_dir=test_output_path, label_dir=label_path, name=name, oder=np.array(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b83f700-0503-4603-ac11-9db0c6dd0f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平均汉明距离为：0.2893\n",
      "平均杰卡德相似系数为：0.9999\n",
      "平均Dice相似系数为：0.9999\n",
      "平均误差为：0.0922%\n",
      "model predict OK\n",
      "平均汉明距离为：97.7400\n",
      "平均杰卡德相似系数为：0.6782\n",
      "平均Dice相似系数为：0.7296\n",
      "平均误差为：47.4440%\n"
     ]
    }
   ],
   "source": [
    "# %load unet_tv.py\n",
    "import os\n",
    "# import shutil\n",
    "# import skimage.io as io\n",
    "# import skimage.transform as trans\n",
    "from keras.optimizer_v2.adam import Adam\n",
    "# from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "# from keras import backend as keras\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import imageio.v2 as imageio\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.layers import BatchNormalization, Conv2D, Activation, Dropout, AveragePooling2D, concatenate, \\\n",
    "    GlobalAveragePooling2D, MaxPooling2D, Dense, Input\n",
    "from keras.regularizers import l2\n",
    "import keras.backend as K\n",
    "import zipfile\n",
    "import evalu\n",
    "# import time\n",
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "# tf.config.experimental.set_memory_growth(gpus[1], True)\n",
    "# tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "# Read datasets\n",
    "def getimg(dir,num,j):\n",
    "    input_dir = dir\n",
    "    img_list = []\n",
    "    for i in range(1+j,num+j):\n",
    "        filename = f'{i}.tif'\n",
    "        img_path = os.path.join(input_dir,filename)\n",
    "        img = imageio.imread(img_path)\n",
    "        img = img.astype(np.float32)\n",
    "        img = img / 255.0\n",
    "        img_list.append(img)\n",
    "    return img_list\n",
    "\n",
    "def onehot(img):\n",
    "    img = tf.cast(img, tf.int32)\n",
    "    img_one_hot = tf.one_hot(img, depth=2, on_value=1.0, off_value=0.0, axis=-1)\n",
    "    return img_one_hot\n",
    "\n",
    "# def add_channel(img): \n",
    "#     img = tf.expand_dims(img, axis=-1) \n",
    "#     return img\n",
    "# Draw loss curve\n",
    "def plot_history(history, result_dir, prefix):\n",
    "    \"\"\"\n",
    "    将训练与验证的accuracy与loss画出来\n",
    "    \"\"\"\n",
    "    plt.plot(history.history['accuracy'], marker='.')\n",
    "    plt.plot(history.history['val_accuracy'], marker='.')\n",
    "    plt.title('model accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.grid()\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend(['acc', 'val_acc'], loc='upper right')\n",
    "    # plt.show()\n",
    "    plt.savefig(result_dir + 'unet_ace' + prefix + '.png')\n",
    "    plt.close()\n",
    "\n",
    "    plt.plot(history.history['loss'], marker='.')\n",
    "    plt.plot(history.history['val_loss'], marker='.')\n",
    "    plt.title('model loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.yscale(\"log\")\n",
    "    plt.grid()\n",
    "    plt.legend(['loss', 'val_loss'], loc='upper right')\n",
    "    # plt.show()\n",
    "    plt.savefig(result_dir + 'unet_loss'+ prefix +'.png')\n",
    "    plt.close()\n",
    "\n",
    "    # x = history.history['loss']\n",
    "    # np.savetxt('D:/pycharm/up_down_code/loss_picture/unet_train_loss.txt', x, fmt='%f')\n",
    "    # y = history.history['val_loss']\n",
    "    # np.savetxt('D:/pycharm/up_down_code/loss_picture/unet_val_loss.txt', y, fmt='%f')hb\n",
    "\n",
    "\n",
    "def Conv_Block(input_tensor, filters, bottleneck=False, weight_decay=1e-4):\n",
    "    \"\"\"    封装卷积层\n",
    "\n",
    "    :param input_tensor: 输入张量\n",
    "    :param filters: 卷积核数目\n",
    "    :param bottleneck: 是否使用bottleneck\n",
    "    :param dropout_rate: dropout比率\n",
    "    :param weight_decay: 权重衰减率\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    concat_axis = 1 if K.image_data_format() == 'channel_first' else -1  # 确定格式\n",
    "\n",
    "    x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5)(input_tensor)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # if bottleneck:\n",
    "    #     # 使用bottleneck进行降维\n",
    "    #     inter_channel = filters\n",
    "    #     x = Conv2D(inter_channel, (1, 1),\n",
    "    #                kernel_initializer='he_normal',\n",
    "    #                padding='same', use_bias=False,\n",
    "    #                kernel_regularizer=l2(weight_decay))(x)\n",
    "    #     x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5)(x)\n",
    "    #     x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters, (3, 3), kernel_initializer='he_normal', padding='same', use_bias=False)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def dens_block(input_tensor, nb_filter):\n",
    "    x1 = Conv_Block(input_tensor, nb_filter)\n",
    "    add1 = concatenate([x1, input_tensor], axis=-1)\n",
    "    x2 = Conv_Block(add1, nb_filter)\n",
    "    add2 = concatenate([x1, input_tensor, x2], axis=-1)\n",
    "    x3 = Conv_Block(add2, nb_filter)\n",
    "    return x3\n",
    "\n",
    "# model definition\n",
    "def unet(input_shape=(128, 128, 2)):\n",
    "    # tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "    inputs = Input(input_shape)\n",
    "    # input1=Input(shape=(128,128,1))\n",
    "    # input2=Input(shape=(128,128,1))\n",
    "    # inputs=Concatenate(axis=-1)([input1,input2])\n",
    "    # inputs = Input(shape\n",
    "    # x  = Conv2D(32, 1, strides=1, activation='relu', padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    x = Conv2D(32, 7, kernel_initializer='he_normal', padding='same', strides=1, use_bias=False,\n",
    "               kernel_regularizer=l2(1e-4))(inputs)\n",
    "    # down first\n",
    "    down1 = dens_block(x, nb_filter=64)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(down1)  # 256\n",
    "    # down second\n",
    "    down2 = dens_block(pool1, nb_filter=64)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(down2)  # 128\n",
    "    # down third\n",
    "    down3 = dens_block(pool2, nb_filter=128)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(down3)  # 64\n",
    "    # down four\n",
    "    down4 = dens_block(pool3, nb_filter=256)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(down4)  # 32\n",
    "    # center\n",
    "    conv5 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool4)\n",
    "    conv5 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    # up first\n",
    "    up6 = UpSampling2D(size=(2, 2))(drop5)\n",
    "    # up6 = UpSampling2D(size=(2, 2))(drop5)\n",
    "    add6 = concatenate([down4, up6], axis=3)\n",
    "    up6 = dens_block(add6, nb_filter=256)\n",
    "    # up second\n",
    "    up7 = UpSampling2D(size=(2, 2))(up6)\n",
    "    # up7 = UpSampling2D(size=(2, 2))(conv6)\n",
    "    add7 = concatenate([down3, up7], axis=3)\n",
    "    up7 = dens_block(add7, nb_filter=128)\n",
    "    # up third\n",
    "    up8 = UpSampling2D(size=(2, 2))(up7)\n",
    "    # up8 = UpSampling2D(size=(2, 2))(conv7)\n",
    "    add8 = concatenate([down2, up8], axis=-1)\n",
    "    up8 = dens_block(add8, nb_filter=64)\n",
    "    # up four\n",
    "    up9 = UpSampling2D(size=(2, 2))(up8)\n",
    "    add9 = concatenate([down1, up9], axis=-1)\n",
    "    up9 = dens_block(add9, nb_filter=64)\n",
    "    # output\n",
    "    conv10 = Conv2D(32, 7, strides=1, activation='relu', padding='same', kernel_initializer='he_normal')(up9)\n",
    "    conv10 = Conv2D(2, 1, activation='softmax')(conv10)\n",
    "    # model = Model(inputs=[input1,input2], outputs=conv10)\n",
    "    model = Model(inputs=inputs, outputs=conv10)\n",
    "    # print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "# define Huber loss\n",
    "def huber_loss(y_true, y_pred):\n",
    "    return tf.losses.huber_loss(y_true, y_pred, delta=0.01)\n",
    "\n",
    "\n",
    "def simm_loss(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_pred - y_true)) + 0.01 * K.mean(K.abs(y_pred))\n",
    "    # return tf.abs(tf.norm(y_pred - y_true))/tf.norm(y_true)\n",
    "    \n",
    "# smooth = 1. # 用于防止分母为0.\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true) # 将 y_true 拉伸为一维.\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection) / (K.sum(y_true_f * y_true_f) + K.sum(y_pred_f * y_pred_f))\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1. - dice_coef(y_true, y_pred)\n",
    "\n",
    "# Define the learning rate attenuation value\n",
    "def scheduler(epoch):\n",
    "    if epoch % 10 == 0 and epoch != 0:\n",
    "        lr = K.get_value(model.optimizer.lr)\n",
    "        K.set_value(model.optimizer.lr, lr * 0.1)\n",
    "        print(\"lr change to {}\".format(lr * 0.1))\n",
    "    return K.get_value(model.optimizer.lr)\n",
    "\n",
    "import random\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "  0.001,\n",
    "  decay_steps=1000,\n",
    "  decay_rate=1,\n",
    "  staircase=False)\n",
    "        \n",
    "def make_zip(source_dir, output_name):\n",
    "    zipf = zipfile.ZipFile(output_name, 'w')\n",
    "    prelen = len(os.path.dirname(source_dir))\n",
    "    for parent, _, filenames in os.walk(source_dir):\n",
    "        for filename in filenames:\n",
    "            pathfile = os.path.join(parent, filename)\n",
    "            arcname = pathfile[prelen:].strip(os.path.sep)     #相对路径\n",
    "            zipf.write(pathfile, arcname)\n",
    "        zipf.close()\n",
    "\n",
    "def train(model, model_savename, train_dataset, validation_dataset):\n",
    "    # model.compile(optimizer=Adam(learning_rate=lr_schedule), loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "    # reduce_lr = LearningRateScheduler(scheduler)\n",
    "    # reduce_lr = LearningRateScheduler(lschedule)\n",
    "    # model.load_weights('/code/save_model/exped/'+model_savename)\n",
    "    model_checkpoint = ModelCheckpoint('/code/save_model/exped/'+model_savename, monitor='loss', verbose=0,\n",
    "                                       save_best_only=True)\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=200)\n",
    "    Board = tf.keras.callbacks.TensorBoard(log_dir=\"/output/logs\")\n",
    "    \n",
    "    history = model.fit(train_dataset,\n",
    "                                  steps_per_epoch=100,\n",
    "                                  epochs=1000,\n",
    "                                  validation_data=validation_dataset,\n",
    "                                #   validation_steps=50,\n",
    "                                  callbacks=[model_checkpoint,\n",
    "                                             early_stop,Board\n",
    "                                             # Board\n",
    "                                             ])\n",
    "    plot_history(history, '/code/loss_picture/extended/',prefix='_exped')\n",
    "    return model\n",
    "    \n",
    "def test(model, test_dataset, output_path, length):\n",
    "    mask = model.predict(test_dataset)\n",
    "    mask = tf.argmax(mask, axis=-1)\n",
    "    mask = tf.keras.backend.eval(mask)\n",
    "    mask = (mask * 255).astype(np.uint8)\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    for j in range(0, length):\n",
    "        cv2.imwrite(output_path+ '%d.tif' %(j+1), mask[j])\n",
    "    print('model predict OK')\n",
    "        \n",
    "BATCH_SIZE=128\n",
    "plot_path = '/code/loss_picture/extended/'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    output_path = '/data/caizhizheng/2D/v2/exped_output/'\n",
    "    label_path = '/data/caizhizheng/2D/v2/label/label/'\n",
    "    img_path = '/data/caizhizheng/2D/v2/data1/data/'\n",
    "    img1_list = getimg(img_path,num=2901, j=0)\n",
    "    img_path = '/data/caizhizheng/2D/v2/data2/data/'\n",
    "    img2_list = getimg(img_path,num=2901, j=900)\n",
    "    label_list = getimg(label_path,num=2901, j=0)\n",
    "    img_list = np.stack((img1_list,img2_list),axis = -1)\n",
    "    \n",
    "    img_path2 = '/data/caizhizheng/2D/v2/test/predicted_input_data/'\n",
    "    label_path2 = '/data/caizhizheng/2D/v2/test/predicted_label/'\n",
    "    # output_path = '/data/caizhizheng/2D/v2/test/outputv2/'\n",
    "    img1_list = getimg(img_path2,num=101, j =0)\n",
    "    img2_list = getimg(img_path2,num=101, j =100)\n",
    "    test_img_list = np.stack((img1_list,img2_list),axis = -1)\n",
    "    test_label_list = getimg(label_path2, num=101, j=0)\n",
    "    \n",
    "    img_list = np.vstack((img_list, test_img_list))\n",
    "    label_list = np.vstack((label_list, test_label_list))\n",
    "    \n",
    "    image_dataset = tf.data.Dataset.from_tensor_slices(img_list)\n",
    "    label_dataset = tf.data.Dataset.from_tensor_slices(label_list)\n",
    "    label_dataset = label_dataset.map(onehot)\n",
    "    # 使用事先保存的数据集会造成内存泄漏？？\n",
    "    # image_dataset = tf.data.experimental.load(\"dataset/image_exped\")\n",
    "    # label_dataset = tf.data.experimental.load(\"dataset/label_exped\")\n",
    "    # x = next(iter(image_dataset.batch(image_dataset.cardinality().numpy())))\n",
    "    # y = next(iter(label_dataset.batch(label_dataset.cardinality().numpy())))\n",
    "    origin_dataset = tf.data.Dataset.zip((image_dataset, label_dataset))\n",
    "    # origin_dataset = origin_dataset.shuffle(origin_dataset.cardinality(), seed=73)\n",
    "\n",
    "    # validation_dataset = origin_dataset.take(100).batch(BATCH_SIZE)\n",
    "    # validation_dataset = validation_dataset.cache().prefetch(tf.data.AUTOTUNE)\n",
    "    train_dataset = origin_dataset\n",
    "    train_dataset = train_dataset.cache().shuffle(train_dataset.cardinality(),seed=1919810, reshuffle_each_iteration=True).repeat().batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    test_path = '/data/caizhizheng/2D/v2/test/test2/data/'\n",
    "    test_label_path = '/data/caizhizheng/2D/v2/test/test2/label/'\n",
    "    test_output_path = '/data/caizhizheng/2D/v2/test/outputv2/'\n",
    "    img1_list = getimg(test_path,num=201, j =0)\n",
    "    img2_list = getimg(test_path,num=201, j =200)\n",
    "    img_list = np.stack((img1_list,img2_list),axis = -1)\n",
    "    label_list = getimg(test_label_path,num=201,j=0)\n",
    "    tv_imgset = tf.data.Dataset.from_tensor_slices(img_list)\n",
    "    tv_labelset = tf.data.Dataset.from_tensor_slices(label_list)\n",
    "    tv_labelset = tv_labelset.map(onehot)\n",
    "    tv_set = tf.data.Dataset.zip((tv_imgset, tv_labelset))\n",
    "    validation_dataset = tv_set.take(100).batch(BATCH_SIZE)\n",
    "\n",
    "\n",
    "    # mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "    # with mirrored_strategy.scope():\n",
    "    model = unet(input_shape=(128, 128, 2))\n",
    "    #     model.compile(optimizer=Adam(learning_rate=lr_schedule), loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "    #     # model.load_weights('/code/save_model/exped/cross_ini_277.keras')\n",
    "    model_savename = 'tv2.keras'\n",
    "    # train = train(model, model_savename, train_dataset, validation_dataset)\n",
    "    \n",
    "    # image_dataset = image_dataset.batch(BATCH_SIZE)\n",
    "    # test(model, image_dataset, output_path, 3000)\n",
    "    # name = 'error2' + '_postCrV' + '.csv'\n",
    "    error_path = './errors/'\n",
    "    name = 'error_expedv2' + '.csv'\n",
    "    name = os.path.join(error_path, name)\n",
    "    evalu.main(output_dir=output_path, label_dir=label_path, name=name, oder=np.array(0))\n",
    "    \n",
    "#     测试集\n",
    "\n",
    "    # test_dataset = tf.data.Dataset.from_tensor_slices(img_list)\n",
    "    test_dataset = tv_imgset.batch(1)\n",
    "    model.load_weights('/code/save_model/exped/'+model_savename)\n",
    "    test(model, test_dataset, test_output_path,200)\n",
    "    name = 'error_exped_testv2.csv'\n",
    "    name = os.path.join(error_path, name)\n",
    "    evalu.main(output_dir=test_output_path, label_dir=test_label_path, name=name, oder=np.array(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf26db5-4092-48a8-996a-4585ce68fd59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.8.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
