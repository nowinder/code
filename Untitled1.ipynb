{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aafc30e-b52e-4b09-be02-ec9e9236347c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## resnet测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e054daf-c536-43f6-bbd1-afa7a245d6b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The input must have 3 channels; got `input_shape=(128, 128, 2)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-6145907f56f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 加载预训练的 ResNet50 模型，去掉最后一层分类层，指定输入尺寸和通道数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mbase_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResNet152\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# 在模型的最后添加一个上采样层，将特征图放大到 128×128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUpSampling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/applications/resnet.py\u001b[0m in \u001b[0;36mResNet152\u001b[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes, **kwargs)\u001b[0m\n\u001b[1;32m    498\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstack1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'conv5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m   return ResNet(stack_fn, False, True, 'resnet152', include_top, weights,\n\u001b[0m\u001b[1;32m    501\u001b[0m                 input_tensor, input_shape, pooling, classes, **kwargs)\n\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/applications/resnet.py\u001b[0m in \u001b[0;36mResNet\u001b[0;34m(stack_fn, preact, use_bias, model_name, include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m   \u001b[0;31m# Determine proper input shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m   input_shape = imagenet_utils.obtain_input_shape(\n\u001b[0m\u001b[1;32m    136\u001b[0m       \u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m       \u001b[0mdefault_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/applications/imagenet_utils.py\u001b[0m in \u001b[0;36mobtain_input_shape\u001b[0;34m(input_shape, default_size, min_size, data_format, require_flatten, weights)\u001b[0m\n\u001b[1;32m    363\u001b[0m           \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`input_shape` must be a tuple of three integers.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'imagenet'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m           raise ValueError('The input must have 3 channels; got '\n\u001b[0m\u001b[1;32m    366\u001b[0m                            '`input_shape=' + str(input_shape) + '`')\n\u001b[1;32m    367\u001b[0m         if ((input_shape[0] is not None and input_shape[0] < min_size) or\n",
      "\u001b[0;31mValueError\u001b[0m: The input must have 3 channels; got `input_shape=(128, 128, 2)`"
     ]
    }
   ],
   "source": [
    "from keras.applications import resnet\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import models\n",
    "# 加载预训练的 ResNet50 模型，去掉最后一层分类层，指定输入尺寸和通道数\n",
    "base_model = resnet.ResNet152(include_top=False, input_shape=(128, 128, 2))\n",
    "# 在模型的最后添加一个上采样层，将特征图放大到 128×128\n",
    "x = layers.UpSampling2D(size=(4, 4))(base_model.input)\n",
    "# 在模型的最后添加一个卷积层，将特征图转换为输出图像，激活函数为 tanh\n",
    "x = layers.Conv2D(2, (3, 3), padding='same', activation='tanh')(x)\n",
    "x.summary()\n",
    "# resnet.ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e129e16-a2c8-476f-8177-d31109a40857",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/applications/imagenet_utils.py:331: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 2 input channels.\n",
      "  warnings.warn('This model usually expects 1 or 3 input channels. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 128, 128, 2) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 134, 134, 2)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 64, 64, 64)   6336        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 64, 64, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 64, 64, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 66, 66, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 32, 32, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 32, 32, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 32, 32, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 32, 32, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 32, 32, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 32, 32, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 32, 32, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 32, 32, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 32, 32, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 32, 32, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 32, 32, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 32, 32, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 32, 32, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 32, 32, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 32, 32, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 32, 32, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 32, 32, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 32, 32, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 32, 32, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 32, 32, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 32, 32, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 32, 32, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 32, 32, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 32, 32, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 32, 32, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 32, 32, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 32, 32, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 16, 16, 128)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 16, 16, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 16, 16, 128)  147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 16, 16, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 16, 16, 512)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 16, 16, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 16, 16, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 16, 16, 512)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 16, 16, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 16, 16, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 16, 16, 128)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 16, 16, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 16, 16, 128)  147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 16, 16, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 16, 16, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 16, 16, 512)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 16, 16, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 16, 16, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 16, 16, 128)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 16, 16, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 16, 16, 128)  147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 16, 16, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 16, 16, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 16, 16, 512)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 16, 16, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 16, 16, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 16, 16, 128)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 16, 16, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 16, 16, 128)  147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 16, 16, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 16, 16, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 16, 16, 512)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 16, 16, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 16, 16, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 8, 8, 256)    131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 8, 8, 256)    0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 8, 8, 256)    590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 8, 8, 256)    0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 8, 8, 1024)   525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 8, 8, 1024)   0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 8, 8, 1024)   0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 8, 8, 256)    262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 8, 8, 256)    0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 8, 8, 256)    590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 8, 8, 256)    0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 8, 8, 1024)   0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 8, 8, 1024)   0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 8, 8, 256)    262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 8, 8, 256)    0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 8, 8, 256)    590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 8, 8, 256)    0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 8, 8, 1024)   0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 8, 8, 1024)   0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 8, 8, 256)    262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 8, 8, 256)    0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 8, 8, 256)    590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 8, 8, 256)    0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 8, 8, 1024)   0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 8, 8, 1024)   0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 8, 8, 256)    262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 8, 8, 256)    0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 8, 8, 256)    590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 8, 8, 256)    0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 8, 8, 1024)   0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 8, 8, 1024)   0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 8, 8, 256)    262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 8, 8, 256)    0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 8, 8, 256)    590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 8, 8, 256)    0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 8, 8, 1024)   4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 8, 8, 1024)   0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 8, 8, 1024)   0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 4, 4, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 4, 4, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 4, 4, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 4, 4, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 4, 4, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 4, 4, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 4, 4, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 4, 4, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 4, 4, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 4, 4, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 4, 4, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 4, 4, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 4, 4, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 4, 4, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 4, 4, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 4, 4, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 4, 4, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 4, 4, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 4, 4, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 4, 4, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 4, 4, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 4, 4, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 4, 4, 2048)   0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 4, 4, 2)      262146      conv5_block3_out[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 23,846,722\n",
      "Trainable params: 23,793,602\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 导入 keras 库\n",
    "import keras\n",
    "from keras.applications import resnet\n",
    "from keras import layers\n",
    "from keras import models\n",
    "# 加载预训练的 ResNet50 模型，去掉最后一层分类层\n",
    "base_model = resnet.ResNet50(weights=None, include_top=False,input_shape=(128, 128, 2))\n",
    "# 在模型的最后添加一个上采样层，将特征图放大到 128×128\n",
    "# x = layers.UpSampling2D(size=(4, 4))(base_model.output)\n",
    "\n",
    "# 在模型的最后添加一个卷积层，将特征图转换为输出图像，激活函数为 tanh\n",
    "# output = layers.Conv2D(2, (3, 3), padding='same', activation='tanh')(x)\n",
    "output = layers.Conv2D(2, (8, 8), padding='same', activation='tanh')(base_model.output)\n",
    "\n",
    "# 构建图像图像映射的网络，输入和输出都是 128×128×2 的图像\n",
    "model = models.Model(inputs=base_model.input, outputs=output)\n",
    "# 修改第一层卷积的输入通道数\n",
    "# base_model.layers[1] = keras.layers.Conv2D(2, (7, 7), strides=(2, 2), padding='same', name='conv1_pad')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08a52516-bb4b-4e56-9823-8d53e2278840",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/applications/imagenet_utils.py:331: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 2 input channels.\n",
      "  warnings.warn('This model usually expects 1 or 3 input channels. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平均汉明距离为：1171.5942\n",
      "平均杰卡德相似系数为：0.4347\n",
      "平均Dice相似系数为：0.5176\n",
      "平均误差为：75.2858%\n"
     ]
    }
   ],
   "source": [
    "# %load resnet.py\n",
    "# 用自带的来改似乎有许多问题\n",
    "import os\n",
    "import shutil\n",
    "from unittest import result\n",
    "import skimage.io as io\n",
    "import skimage.transform as trans\n",
    "from keras.optimizer_v2.adam import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "import keras \n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.saving.utils_v1.mode_keys import is_train\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import imageio.v2 as imageio\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import models\n",
    "from keras import layers\n",
    "# from keras.layers import BatchNormalization, Conv2D, Activation, Dropout, AveragePooling2D, concatenate, \\\n",
    "    # GlobalAveragePooling2D, MaxPooling2D, Dense, Input\n",
    "from keras.regularizers import l2\n",
    "import keras.backend as K\n",
    "import evalu\n",
    "import time\n",
    "import zipfile\n",
    "from keras.applications import resnet\n",
    "from keras.regularizers import l2\n",
    "import random\n",
    "\n",
    "def getimg(dir, j):\n",
    "    input_dir = dir\n",
    "    img_list = []\n",
    "    for i in range(1,1201):\n",
    "        filename = f'{i+j}.tif'\n",
    "        img_path = os.path.join(input_dir,filename)\n",
    "        img = imageio.imread(img_path)\n",
    "        img = img.astype(np.float32)\n",
    "        img = img / 255.0\n",
    "        img_list.append(img)\n",
    "        # print(i)\n",
    "    return img_list\n",
    "\n",
    "def onehot(img):\n",
    "    img = tf.cast(img, tf.int32)\n",
    "    img_one_hot = tf.one_hot(img, depth=2, on_value=1.0, off_value=0.0, axis=-1)\n",
    "    return img_one_hot\n",
    "\n",
    "# def add_channel(img): \n",
    "#     img = tf.expand_dims(img, axis=-1) \n",
    "#     return img\n",
    "# Draw loss curve\n",
    "def plot_history(history, result_dir, prefix):\n",
    "    \"\"\"\n",
    "    将训练与验证的accuracy与loss画出来\n",
    "    \"\"\"\n",
    "    plt.plot(history.history['accuracy'], marker='.')\n",
    "    plt.plot(history.history['val_accuracy'], marker='.')\n",
    "    plt.title('model accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.grid()\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend(['acc'], loc='upper right')\n",
    "\t# plt.show()\n",
    "    plt.savefig(result_dir + 'ace' + prefix + '.png')\n",
    "    plt.close()\n",
    "\n",
    "    plt.plot(history.history['loss'], marker='.')\n",
    "    plt.plot(history.history['val_loss'], marker='.')\n",
    "    plt.title('model loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.yscale(\"log\")\n",
    "    plt.grid()\n",
    "    plt.legend(['loss'], loc='upper right')\n",
    "    # plt.show()\n",
    "    plt.savefig(result_dir + 'loss'+ prefix +'.png')\n",
    "    plt.close()\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "  0.001,\n",
    "  decay_steps=100,\n",
    "  decay_rate=1,\n",
    "  staircase=False)\n",
    "        \n",
    "def make_zip(source_dir, output_name):\n",
    "    zipf = zipfile.ZipFile(output_name, 'w')\n",
    "    prelen = len(os.path.dirname(source_dir))\n",
    "    for parent, _, filenames in os.walk(source_dir):\n",
    "        for filename in filenames:\n",
    "            pathfile = os.path.join(parent, filename)\n",
    "            arcname = pathfile[prelen:].strip(os.path.sep)     #相对路径\n",
    "            zipf.write(pathfile, arcname)\n",
    "        zipf.close()\n",
    "\n",
    "def train(model, model_savename, train_dataset, validation_dataset):\n",
    "    model.compile(optimizer=Adam(learning_rate=lr_schedule), loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "    # reduce_lr = LearningRateScheduler(scheduler)\n",
    "    # reduce_lr = LearningRateScheduler(lschedule)\n",
    "    model_checkpoint = ModelCheckpoint('./save_model/'+model_savename, monitor='loss', verbose=1,\n",
    "                                       save_best_only=True)\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=100)\n",
    "    Board = tf.keras.callbacks.TensorBoard(log_dir=\"/output/logs\")\n",
    "\n",
    "    history = model.fit(train_dataset.repeat(),\n",
    "                                  steps_per_epoch=180,\n",
    "                                  epochs=500,\n",
    "                                  validation_data=validation_dataset,\n",
    "                                #   validation_steps=50,\n",
    "                                  callbacks=[model_checkpoint,\n",
    "                                             early_stop,Board\n",
    "                                             # Board\n",
    "                                             ])\n",
    "    plot_history(history, './loss_picture/', '_resnet')\n",
    "    return model\n",
    "\n",
    "        \n",
    "BATCH_SIZE=8\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    save_path = '/data/caizhizheng/2D/newnet/'\n",
    "    label_path = '/data/caizhizheng/2D/v2/label/label'\n",
    "    # data_path = save_path + 'data/'\n",
    "    output_path = save_path + 'predict/'\n",
    "    # output_path = '/data/caizhizheng/2D/v2/exped_output/'\n",
    "    label_path = '/data/caizhizheng/2D/v2/label/label/'\n",
    "    img_path = '/data/caizhizheng/2D/v2/data1/data/'\n",
    "    img1_list = getimg(img_path, j=0)\n",
    "    img_path = '/data/caizhizheng/2D/v2/data2/data/'\n",
    "    # img2_list = getimg(img_path, j=900)\n",
    "    # label_list = getimg(label_path,j=0)\n",
    "    # img_list = np.stack((img1_list,img2_list),axis = -1)\n",
    "#     image_dataset = tf.data.Dataset.from_tensor_slices(img_list)\n",
    "#     label_dataset = tf.data.Dataset.from_tensor_slices(label_list)\n",
    "#     label_dataset = label_dataset.map(onehot)\n",
    "#     train_dataset = tf.data.Dataset.zip((image_dataset, label_dataset))\n",
    "#     train_dataset = train_dataset.cache().shuffle(train_dataset.cardinality())\n",
    "\n",
    "#     validation_dataset = train_dataset.take(50).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "#     train_dataset = train_dataset.skip(50).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    # 加载 ResNet50 模型，去掉最后一层分类层，指定输入尺寸和通道数\n",
    "    base_model = resnet.ResNet50(weights=None, include_top=False, input_shape=(128, 128, 2))\n",
    "    # 修改第一层卷积的输入通道数\n",
    "    # base_model.layers[1] = keras.layers.Conv2D(2, (7, 7), strides=(2, 2), padding='same', name='conv1_pad')\n",
    "    # 在模型的最后添加一个上采样层，将特征图放大到 128×128\n",
    "    x = layers.UpSampling2D(size=(32, 32))(base_model.output)\n",
    "    # 在模型的最后添加一个卷积层，将特征图转换为输出图像，激活函数为 tanh\n",
    "    output = layers.Conv2D(2, (3, 3), padding='same', activation='softmax')(x)\n",
    "    # 构建图像图像映射的网络，输入和输出都是 128×128×2 的图像\n",
    "    model = models.Model(inputs=base_model.input, outputs=output)\n",
    "    # model.summary()\n",
    "    model_savename = 'resnet.keras'\n",
    "    # train = train(model, model_savename, train_dataset, validation_dataset)\n",
    "    \n",
    "    # image_dataset = image_dataset.batch(BATCH_SIZE)\n",
    "    # mask = model.predict(image_dataset)\n",
    "    # mask = tf.argmax(mask, axis=-1)\n",
    "    # mask = tf.keras.backend.eval(mask)\n",
    "    # mask = (mask * 255).astype(np.uint8)\n",
    "    # os.makedirs(output_path, exist_ok=True)\n",
    "    # for j in range(0, 1200):\n",
    "    #     cv2.imwrite(output_path+ '%d.tif' %(j+1), mask[j])\n",
    "\n",
    "    # name = 'error2' + '_postCrV' + '.csv'\n",
    "    name = 'error' + '.csv'\n",
    "    name = os.path.join(save_path, name)\n",
    "    evalu.main(output_dir=output_path, label_dir=label_path, name=name, oder=np.array(0))\n",
    "\n",
    "    # output_dir = '/data/caizhizheng/2D/validataset/predict2/'\n",
    "    # output_name = '/data/caizhizheng/2D/validataset/' + 'vali-filterd2.zip'\n",
    "    # output_name = output_path + 'predict.zip'\n",
    "    # make_zip(source_dir=output_path, output_name=output_name)\n",
    "    # print(\"zip OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff8a565-1f02-4590-a8b1-1901d6134aec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 压缩、onehot等等小函数与整合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ff1c67f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.865458"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试交叉熵误差函数\n",
    "import tensorflow as tf\n",
    "y_true = [0, 1, 0, 0]\n",
    "y_pred = [-18.6, 0.51, 2.94, -12.8]\n",
    "bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "bce(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ec83534-73ff-4e58-be34-ee86c9721bc5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zip OK\n"
     ]
    }
   ],
   "source": [
    "# 压缩完整程序\n",
    "import zipfile\n",
    "import os\n",
    "def make_zip(source_dir, output_name):\n",
    "    zipf = zipfile.ZipFile(output_name, 'w')\n",
    "    prelen = len(os.path.dirname(source_dir))\n",
    "    for parent, _, filenames in os.walk(source_dir):\n",
    "        for filename in filenames:\n",
    "            pathfile = os.path.join(parent, filename)\n",
    "            arcname = pathfile[prelen:].strip(os.path.sep)     #相对路径\n",
    "            zipf.write(pathfile, arcname)\n",
    "        zipf.close()\n",
    "        \n",
    "output_dir = '/data/caizhizheng/2D/v2/test/outputv2/'\n",
    "output_name = '/data/caizhizheng/2D/v2/test/' + 'unet_outv2.zip'\n",
    "make_zip(source_dir=output_dir, output_name=output_name)\n",
    "print(\"zip OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55112aa8-782f-4b2d-b796-2cdc12678a7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_dir1 = input_dir2 = '/data/caizhizheng/2D/v2/predicted_input_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "155e81c5-7494-4e3d-8ef7-6a9c20f63255",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "n = len(os.listdir(input_dir1))\n",
    "n = n/2\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "041ec459-34c1-4e3f-80f5-97af79fd567c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add(a, b):\n",
    "   res = a + b\n",
    "   return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf8b820-1b6f-4fbc-a06f-c6b7635e6a62",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = add(1, 2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74ecca1f-0887-41eb-b218-fc233acc7524",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 54.22369     15.94079     12.248195   ... 194.81082    231.64977\n",
      "  184.08852   ]\n",
      " [ 26.041986     9.746332     4.870913   ... 175.84555    209.67267\n",
      "  137.96419   ]\n",
      " [ 11.731605     3.6755352    2.2074535  ... 115.24107    135.55334\n",
      "  120.20413   ]\n",
      " ...\n",
      " [  5.0215597    0.9845804    0.49632177 ... 174.94247    196.01437\n",
      "  156.45685   ]\n",
      " [  7.059702     1.619245     0.75938165 ... 199.66898    212.0699\n",
      "  199.86003   ]\n",
      " [ 12.550756     4.496557     3.1235843  ... 221.34952    206.03761\n",
      "  210.4963    ]]\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "img = cv2.imread('/data/caizhizheng/2D/result/99.tif', cv2.IMREAD_UNCHANGED) # 读取图片，保持原始通道\n",
    "print(img)\n",
    "img1 = cv2.imread('/data/caizhizheng/2D/predicted_label/99.tif', cv2.IMREAD_UNCHANGED) # 读取图片，保持原始通道\n",
    "print(img1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b628da7-9813-4649-a61f-e0c4076bf9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "img = Image.open('/data/caizhizheng/2D/result/1.tif') # 读取图片\n",
    "width, channels = img.size # 获取图片的宽度和高度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12ebbe8f-c829-4b84-80a5-6a91c2ac0d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "577b1f09-8db1-4deb-ab39-082771a22ebc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2, 2), dtype=float32, numpy=\n",
       "array([[[255.,   0.],\n",
       "        [  0.,   0.]],\n",
       "\n",
       "       [[255.,   0.],\n",
       "        [255.,   0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "indices = [[0, 255], [0, 0]]\n",
    "tf.one_hot(indices, depth=2, on_value=255.0, off_value=0.0, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14fefcd8-7151-4afc-a41e-b6f84b2888c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
      "1.0004814\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "# y_true and y_pred are both 2D tensors of shape (batch_size, 128, 128)\n",
    "y_true = tf.random.uniform(shape=(32, 128, 128), minval=0, maxval=2, dtype=tf.int32)\n",
    "y_pred = tf.random.uniform(shape=(32, 128, 128), minval=0, maxval=1)\n",
    "\n",
    "# Create a loss function\n",
    "bce = BinaryCrossentropy()\n",
    "\n",
    "# Compute the loss\n",
    "loss = bce(y_true, y_pred)\n",
    "print(loss.numpy()) # 0.6931472\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3358cf3f-ce7f-457c-8d1b-bc3bd12038d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %load onehot.py\n",
    "import tensorflow as tf\n",
    "import imageio.v2 as imageio\n",
    "import os\n",
    "import numpy as np\n",
    "input_dir = '/data/caizhizheng/2D/v2/label/label'\n",
    "for i in range(1,901):\n",
    "    filename = f'{i}.tif'\n",
    "    img_path = os.path.join(input_dir,filename)\n",
    "    img = imageio.imread(img_path)\n",
    "    img = img.astype(np.float32)\n",
    "    img = img / 255.0\n",
    "    img_one_hot = tf.one_hot(img, depth=2, on_value=255.0, off_value=0.0, axis=-1)\n",
    "    # print(img_one_hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ff5d0d3-b40a-44e4-97b1-592975e72100",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zip OK\n"
     ]
    }
   ],
   "source": [
    "# 压缩测试图片方便下载\n",
    "import zipfile\n",
    "import os\n",
    "def make_zip(source_dir, output_name):\n",
    "    zipf = zipfile.ZipFile(output_name, 'w')\n",
    "    prelen = len(os.path.dirname(source_dir))\n",
    "    for parent, _, filenames in os.walk(source_dir):\n",
    "        for filename in filenames:\n",
    "            pathfile = os.path.join(parent, filename)\n",
    "            arcname = pathfile[prelen:].strip(os.path.sep)     #相对路径\n",
    "            zipf.write(pathfile, arcname)\n",
    "        zipf.close()\n",
    "source_dir = \"/data/caizhizheng/2D/v2/result_test\"\n",
    "output_name = \"/data/caizhizheng/2D/v2/result1.zip\"\n",
    "make_zip(source_dir=source_dir, output_name=output_name)\n",
    "print(\"zip OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53b2e133-50a9-4184-9de6-289e9136f485",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import imageio.v2 as imageio\n",
    "import numpy as np\n",
    "\n",
    "label_dir = '/data/caizhizheng/2D/v2/label/label'\n",
    "label_paths = [os.path.join(label_dir, f'{i}.tif') for i in range(1, 901)]\n",
    "label_dataset = tf.data.Dataset.from_tensor_slices(label_paths)\n",
    "label_dataset = label_dataset.map(lambda x: tf.io.decode_image(x, channels=1))\n",
    "# def process_image(img_path):\n",
    "#   # 读取图片并转换为numpy数组\n",
    "#   img = imageio.imread(img_path)\n",
    "#   img = img.astype(np.float32)\n",
    "#   img = img / 255.0\n",
    "#   # 将矩阵转换为one-hot编码的张量，depth为2，on_value为255.0，off_value为0.0\n",
    "#   img_one_hot = tf.one_hot(img, depth=2, on_value=255.0, off_value=0.0, axis=-1)\n",
    "#   return img_one_hot\n",
    "\n",
    "# label_dataset = label_dataset.map(process_image)\n",
    "# it = iter(label_dataset)\n",
    "# print(next(it).numpy())\n",
    "# for x in label_dataset:\n",
    "#     print(x.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3bc59037-6a5c-402f-b0d4-ff801e559dc1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((TensorSpec(shape=(8, 128, 128, 1), dtype=tf.float32, name=None),\n",
       "  TensorSpec(shape=(8, 128, 128, 1), dtype=tf.float32, name=None)),\n",
       " TensorSpec(shape=(8, 128, 128, 2), dtype=tf.float32, name=None))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %load onehot.py\n",
    "import tensorflow as tf\n",
    "import imageio.v2 as imageio\n",
    "import os\n",
    "import numpy as np\n",
    "from keras import backend as keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "def getimg():\n",
    "    input_dir = '/data/caizhizheng/2D/v2/label/label'\n",
    "    img_list = []\n",
    "    for i in range(1,901):\n",
    "        filename = f'{i}.tif'\n",
    "        img_path = os.path.join(input_dir,filename)\n",
    "        img = imageio.imread(img_path)\n",
    "        img = img.astype(np.float32)\n",
    "        img = img / 255.0\n",
    "        img_list.append(img)\n",
    "    return img_list\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def onehot(img):\n",
    "    img = tf.cast(img, tf.int32)\n",
    "    img_one_hot = tf.one_hot(img, depth=2, on_value=255.0, off_value=0.0, axis=-1)\n",
    "    return img_one_hot\n",
    "\n",
    "def train_generator(label_dataset, batch_size=32):\n",
    "    image1_datagen = ImageDataGenerator()\n",
    "    image2_datagen = ImageDataGenerator()\n",
    "    mask_datagen = ImageDataGenerator()\n",
    "    seed=1\n",
    "    def image_generator():\n",
    "        image_generator1 = image1_datagen.flow_from_directory(\n",
    "            '/data/caizhizheng/2D/v2/data1',\n",
    "            # The absolute path of the data set\n",
    "            class_mode=None,\n",
    "            batch_size=batch_size,\n",
    "            color_mode='grayscale',\n",
    "            target_size=(128, 128),\n",
    "            # save_to_dir='./data/gen/images',\n",
    "            shuffle=False,\n",
    "            seed=seed)\n",
    "        image_generator2 = image2_datagen.flow_from_directory(\n",
    "            '/data/caizhizheng/2D/v2/data2',\n",
    "            # The absolute path of the data set\n",
    "            class_mode=None,\n",
    "            batch_size=batch_size,\n",
    "            color_mode='grayscale',\n",
    "            target_size=(128, 128),\n",
    "            shuffle=False,\n",
    "            # save_to_dir='./data/gen/images',\n",
    "            seed=seed)\n",
    "        for imgs1, imgs2 in zip(image_generator1, image_generator2):\n",
    "            imgs1 = imgs1 / 255.0\n",
    "            imgs2 = imgs2 / 255.0\n",
    "            yield (imgs1, imgs2)\n",
    "            \n",
    "    def mask_generator():\n",
    "        for masks in mask_datagen.flow(label_dataset, batch_size=batch_size):\n",
    "            yield masks\n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: zip(image_generator(), mask_generator()),\n",
    "        output_types=((tf.float32, tf.float32), tf.float32),\n",
    "        output_shapes=(((batch_size, 128, 128, 1), (batch_size, 128, 128, 1)), (batch_size, 128, 128, 2))\n",
    "    )\n",
    "    \n",
    "    return train_dataset\n",
    "\n",
    "# def train_generator(label_dataset, batch_size=32):\n",
    "#     image1_datagen = ImageDataGenerator()\n",
    "#     image2_datagen = ImageDataGenerator()\n",
    "#     mask_datagen = ImageDataGenerator()\n",
    "#     seed=1\n",
    "#     image_generator = image1_datagen.flow_from_directory(\n",
    "#         '/data/caizhizheng/2D/v2/data1',\n",
    "#         # The absolute path of the data set\n",
    "#         class_mode=None,\n",
    "#         batch_size=batch_size,\n",
    "#         color_mode='grayscale',\n",
    "#         target_size=(128, 128),\n",
    "#         # save_to_dir='./data/gen/images',\n",
    "#         shuffle=False,\n",
    "#         seed=seed)\n",
    "#     image_generator2 = image2_datagen.flow_from_directory(\n",
    "#         '/data/caizhizheng/2D/v2/data2',\n",
    "#         # The absolute path of the data set\n",
    "#         class_mode=None,\n",
    "#         batch_size=batch_size,\n",
    "#         color_mode='grayscale',\n",
    "#         target_size=(128, 128),\n",
    "#         shuffle=False,\n",
    "#         # save_to_dir='./data/gen/images',\n",
    "#         seed=seed)\n",
    "#     mask_generator = mask_datagen.flow(label_dataset, batch_size=batch_size)\n",
    "#     train_generator = zip(image_generator, image_generator2, mask_generator)\n",
    "\n",
    "#     for (imgs1, imgs2, masks) in train_generator:\n",
    "#         imgs1 = imgs1 / 255.0\n",
    "#         imgs2 = imgs2 / 255.0\n",
    "#         # print(masks.shape)\n",
    "#         # masks = cv2.cvtColor(masks,cv2.COLOR_RGB2GRAY)\n",
    "#         # masks = masks / 255.0\n",
    "#         yield (imgs1, imgs2), masks\n",
    "        \n",
    "img_list = getimg()\n",
    "label_dataset = tf.data.Dataset.from_tensor_slices(img_list)\n",
    "label_dataset = label_dataset.map(onehot)\n",
    "label_dataset\n",
    "# dataset_to_numpy=list(label_dataset.as_numpy_iterator())\n",
    "# tf.shape(dataset_to_numpy)\n",
    "gen=train_generator(batch_size=8, label_dataset=label_dataset)\n",
    "gen.element_spec\n",
    "gen_data=gen.batch(4)\n",
    "for batch in gen_data.take(1):\n",
    "    print([arr.numpy() for arr in batch])\n",
    "# dataset_to_numpy1=list(gen)\n",
    "# tf.shape(dataset_to_numpy1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6eb1a0-d651-4871-a86c-5ef81f8d5e43",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 误差等后处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a36a7746-492a-4851-9d44-71bcf6637f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平均汉明距离为：0.0000\n",
      "平均杰卡德相似系数为：1.0000\n",
      "平均Dice相似系数为：1.0000\n",
      "平均误差为：0.0000%\n"
     ]
    }
   ],
   "source": [
    "import evalu\n",
    "import os\n",
    "import numpy as np\n",
    "label_path = '/data/caizhizheng/2D/v2/test/predicted_label/'\n",
    "output_path = '/data/caizhizheng/2D/v2/exped_output/'\n",
    "error_path = './errors/'\n",
    "name = 'error_expedv2_remain' + '.csv'\n",
    "name = os.path.join(error_path, name)\n",
    "evalu.main(output_dir=output_path, label_dir=label_path, name=name, oder=np.array(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "937c2ee2-7077-4a43-80ae-38f677b5c74f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平均汉明距离为：646.0800\n",
      "平均杰卡德相似系数为：0.8069\n",
      "平均Dice相似系数为：0.8626\n",
      "平均误差为：40.3881%\n"
     ]
    }
   ],
   "source": [
    "# %load data_post.py\n",
    "import cv2\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "# 定义一个函数计算两个图片矩阵的误差\n",
    "def error(img1, img2):\n",
    "    # 将图片矩阵转换为一维向量\n",
    "    vec1 = img1.flatten()\n",
    "    vec2 = img2.flatten()\n",
    "    # 计算两个向量的差值的二范数的绝对值\n",
    "    diff = np.linalg.norm(vec1 - vec2)\n",
    "    # 计算真值向量的二范数\n",
    "    norm = np.linalg.norm(vec2)\n",
    "    # 返回误差，即差值除以真值\n",
    "        # 判断真值向量是否为0或接近0\n",
    "    if norm < 1e-6:\n",
    "        # 返回一个特殊的值，表示无法计算误差\n",
    "        return -1\n",
    "    else:\n",
    "        # 返回误差，即差值除以真值乘以100\n",
    "        return diff / norm * 100\n",
    "# 定义一个函数计算两个二值化图片矩阵的汉明距离\n",
    "def hamming_distance(img1, img2):\n",
    "    # 将图片矩阵转换为一维向量\n",
    "    vec1 = img1.flatten()\n",
    "    vec2 = img2.flatten()\n",
    "    # 计算两个向量的异或，然后统计非零元素的个数\n",
    "    diff = np.count_nonzero(vec1 ^ vec2)\n",
    "    # 返回汉明距离\n",
    "    return diff\n",
    "\n",
    "# 定义一个函数计算两个二值化图片矩阵的杰卡德相似系数\n",
    "def jaccard_similarity(img1, img2):\n",
    "    # 将图片矩阵转换为一维向量\n",
    "    vec1 = img1.flatten()\n",
    "    vec2 = img2.flatten()\n",
    "    # 计算两个向量的逻辑与，然后统计非零元素的个数（重叠区域）\n",
    "    intersection = np.count_nonzero(vec1 & vec2)\n",
    "    # 计算两个向量的逻辑或，然后统计非零元素的个数（总区域）\n",
    "    union = np.count_nonzero(vec1 | vec2)\n",
    "    # 返回杰卡德相似系数，即重叠区域除以总区域\n",
    "        # 判断总区域是否为0或接近0\n",
    "    if union < 1e-6:\n",
    "        # 返回一个特殊的值，表示无法计算相似系数\n",
    "        return -1\n",
    "    else:\n",
    "        # 返回杰卡德相似系数\n",
    "        return intersection / union\n",
    "\n",
    "# 定义一个函数计算两个二值化图片矩阵的Dice相似系数\n",
    "def dice_similarity(img1, img2):\n",
    "    # 将图片矩阵转换为一维向量\n",
    "    vec1 = img1.flatten()\n",
    "    vec2 = img2.flatten()\n",
    "    # 计算两个向量的逻辑与，然后统计非零元素的个数（重叠区域）\n",
    "    intersection = np.count_nonzero(vec1 & vec2)\n",
    "    # 计算两个向量各自的非零元素的个数之和（平均区域）\n",
    "    sum = np.count_nonzero(vec1) + np.count_nonzero(vec2)\n",
    "    # 返回Dice相似系数，即重叠区域乘以2除以平均区域\n",
    "        # 判断平均区域是否为0或接近0\n",
    "    if sum < 1e-6:\n",
    "        # 返回一个特殊的值，表示无法计算相似系数\n",
    "        return -1\n",
    "    else:\n",
    "        # 返回Dice相似系数\n",
    "        return (2 * intersection) / sum\n",
    "# 定义一个空列表存储100个误差\n",
    "errors = []\n",
    "# 定义三个空列表存储100个汉明距离、杰卡德相似系数和Dice相似系数\n",
    "hamming_distances = []\n",
    "jaccard_similarities = []\n",
    "dice_similarities = []\n",
    "# 用一个for循环遍历100张图像\n",
    "for i in range(1, 101):\n",
    "    output = cv2.imread(f'/data/caizhizheng/2D/v2/result/{i}.tif', cv2.IMREAD_UNCHANGED)\n",
    "    label = cv2.imread(f'/data/caizhizheng/2D/v2/predicted_label/{i}.tif', cv2.IMREAD_UNCHANGED)\n",
    "    output = output / 255.0\n",
    "    label = label / 255.0\n",
    "    output = output.astype(int)\n",
    "    label = label.astype(int)\n",
    "    # 调用error函数计算两个图像之间的误差，并添加到列表中\n",
    "    errors.append(error(output, label))\n",
    "    # 调用三个函数，计算两个图像之间的三个参数，并添加到对应的列表中\n",
    "    hamming_distances.append(hamming_distance(output, label))\n",
    "    jaccard_similarities.append(jaccard_similarity(output, label))\n",
    "    dice_similarities.append(dice_similarity(output, label))\n",
    "# 计算100个误差的平均值，忽略-1的值\n",
    "mean_error = np.mean([e for e in errors if e != -1])\n",
    "# 计算三个参数的平均值，忽略-1的值，并打印出来\n",
    "mean_hamming_distance = np.mean([h for h in hamming_distances if h != -1])\n",
    "mean_jaccard_similarity = np.mean([j for j in jaccard_similarities if j != -1])\n",
    "mean_dice_similarity = np.mean([d for d in dice_similarities if d != -1])\n",
    "print(f'平均汉明距离为：{mean_hamming_distance:.4f}')\n",
    "print(f'平均杰卡德相似系数为：{mean_jaccard_similarity:.4f}')\n",
    "print(f'平均Dice相似系数为：{mean_dice_similarity:.4f}')\n",
    "# 打印平均误差\n",
    "print(f'平均误差为：{mean_error:.4f}%')\n",
    "\n",
    "# 将100个误差写入csv文件，假设文件名为errors.csv，位于当前目录下\n",
    "with open('errors.csv', 'w') as f:\n",
    "    # 创建一个csv写入对象\n",
    "    writer = csv.writer(f)\n",
    "    # 写入一行表头，表示图像编号和误差\n",
    "    writer.writerow(['Image', 'Error', 'Hamming Distance', 'Jaccard Similarity', 'Dice Similarity'])\n",
    "    writer.writerow(['Mean', mean_error, mean_hamming_distance, mean_jaccard_similarity, mean_dice_similarity])\n",
    "    # 用一个for循环遍历100个误差，并写入一行数据，表示第i张图像的误差\n",
    "    for i, (e,h,j,d) in enumerate(zip(errors,hamming_distances, jaccard_similarities, dice_similarities)):\n",
    "        writer.writerow([i + 1, e,h,j,d])\n",
    "    \n",
    "    # writer.writerow(['Image', 'Error'])\n",
    "    # # 用一个for循环遍历100个误差，并写入一行数据，表示第i张图像的误差\n",
    "    # for i, e in enumerate(errors):\n",
    "    #     writer.writerow([i + 1, e])\n",
    "# with open('errors.csv', 'w') as f:\n",
    "#     # 创建一个csv写入对象，指定表头为Image和Error\n",
    "#     writer = csv.DictWriter(f, fieldnames=['Image', 'Error'])\n",
    "#     # 写入表头\n",
    "#     writer.writeheader()\n",
    "#     # 用一个for循环遍历100个误差，并写入一行数据，表示第i张图像的误差\n",
    "#     for i, e in enumerate(errors, start=1):\n",
    "#         writer.writerow({'Image': i, 'Error': e})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6fa4b6c-e37a-49c6-aedf-c88652476576",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 128, 128, 2)  0           input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 128, 128, 32) 3136        concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 128, 128, 32) 128         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 128, 128, 32) 0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 128, 128, 64) 18432       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 128, 128, 96) 0           conv2d_30[0][0]                  \n",
      "                                                                 conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 128, 128, 96) 384         concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 128, 128, 96) 0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 128, 128, 64) 55296       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 128, 128, 160 0           conv2d_30[0][0]                  \n",
      "                                                                 conv2d_29[0][0]                  \n",
      "                                                                 conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 128, 128, 160 640         concatenate_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 128, 128, 160 0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 128, 128, 64) 92160       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 64, 64, 64)   0           conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 64, 64, 64)   256         max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 64, 64, 64)   0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 64, 64, 64)   36864       activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 64, 64, 128)  0           conv2d_33[0][0]                  \n",
      "                                                                 max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 64, 64, 128)  512         concatenate_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 64, 64, 128)  0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 64, 64, 64)   73728       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_25 (Concatenate)    (None, 64, 64, 192)  0           conv2d_33[0][0]                  \n",
      "                                                                 max_pooling2d_4[0][0]            \n",
      "                                                                 conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 64, 64, 192)  768         concatenate_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 64, 64, 192)  0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 64, 64, 64)   110592      activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 32, 32, 64)   0           conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 32, 32, 64)   256         max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 32, 32, 64)   0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 32, 32, 128)  73728       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_26 (Concatenate)    (None, 32, 32, 192)  0           conv2d_36[0][0]                  \n",
      "                                                                 max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 32, 32, 192)  768         concatenate_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 32, 32, 192)  0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 32, 32, 128)  221184      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_27 (Concatenate)    (None, 32, 32, 320)  0           conv2d_36[0][0]                  \n",
      "                                                                 max_pooling2d_5[0][0]            \n",
      "                                                                 conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 32, 32, 320)  1280        concatenate_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 32, 32, 320)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 32, 32, 128)  368640      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 16, 16, 128)  0           conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 16, 16, 128)  512         max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 16, 16, 128)  0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 16, 16, 256)  294912      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_28 (Concatenate)    (None, 16, 16, 384)  0           conv2d_39[0][0]                  \n",
      "                                                                 max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 16, 16, 384)  1536        concatenate_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 16, 16, 384)  0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 16, 16, 256)  884736      activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_29 (Concatenate)    (None, 16, 16, 640)  0           conv2d_39[0][0]                  \n",
      "                                                                 max_pooling2d_6[0][0]            \n",
      "                                                                 conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 16, 16, 640)  2560        concatenate_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 16, 16, 640)  0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 16, 16, 256)  1474560     activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 8, 8, 256)    0           conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 8, 8, 512)    1180160     max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 8, 8, 512)    2359808     conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 8, 8, 512)    0           conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2D)  (None, 16, 16, 512)  0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_30 (Concatenate)    (None, 16, 16, 768)  0           conv2d_41[0][0]                  \n",
      "                                                                 up_sampling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 16, 16, 768)  3072        concatenate_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 16, 16, 768)  0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 16, 16, 256)  1769472     activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_31 (Concatenate)    (None, 16, 16, 1024) 0           conv2d_44[0][0]                  \n",
      "                                                                 concatenate_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 16, 16, 1024) 4096        concatenate_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 16, 16, 1024) 0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 16, 16, 256)  2359296     activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_32 (Concatenate)    (None, 16, 16, 1280) 0           conv2d_44[0][0]                  \n",
      "                                                                 concatenate_30[0][0]             \n",
      "                                                                 conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 16, 16, 1280) 5120        concatenate_32[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 16, 16, 1280) 0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 16, 16, 256)  2949120     activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2D)  (None, 32, 32, 256)  0           conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_33 (Concatenate)    (None, 32, 32, 384)  0           conv2d_38[0][0]                  \n",
      "                                                                 up_sampling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 32, 32, 384)  1536        concatenate_33[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 32, 32, 384)  0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 32, 32, 128)  442368      activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_34 (Concatenate)    (None, 32, 32, 512)  0           conv2d_47[0][0]                  \n",
      "                                                                 concatenate_33[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 32, 32, 512)  2048        concatenate_34[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 32, 32, 512)  0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 32, 32, 128)  589824      activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_35 (Concatenate)    (None, 32, 32, 640)  0           conv2d_47[0][0]                  \n",
      "                                                                 concatenate_33[0][0]             \n",
      "                                                                 conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 32, 32, 640)  2560        concatenate_35[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 32, 32, 640)  0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 32, 32, 128)  737280      activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_6 (UpSampling2D)  (None, 64, 64, 128)  0           conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_36 (Concatenate)    (None, 64, 64, 192)  0           conv2d_35[0][0]                  \n",
      "                                                                 up_sampling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 64, 64, 192)  768         concatenate_36[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 64, 64, 192)  0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 64, 64, 64)   110592      activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_37 (Concatenate)    (None, 64, 64, 256)  0           conv2d_50[0][0]                  \n",
      "                                                                 concatenate_36[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 64, 64, 256)  1024        concatenate_37[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 64, 64, 256)  0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 64, 64, 64)   147456      activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_38 (Concatenate)    (None, 64, 64, 320)  0           conv2d_50[0][0]                  \n",
      "                                                                 concatenate_36[0][0]             \n",
      "                                                                 conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 64, 64, 320)  1280        concatenate_38[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 64, 64, 320)  0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 64, 64, 64)   184320      activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_7 (UpSampling2D)  (None, 128, 128, 64) 0           conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_39 (Concatenate)    (None, 128, 128, 128 0           conv2d_32[0][0]                  \n",
      "                                                                 up_sampling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 128, 128, 128 512         concatenate_39[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 128, 128, 128 0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 128, 128, 64) 73728       activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_40 (Concatenate)    (None, 128, 128, 192 0           conv2d_53[0][0]                  \n",
      "                                                                 concatenate_39[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 128, 128, 192 768         concatenate_40[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 128, 128, 192 0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 128, 128, 64) 110592      activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_41 (Concatenate)    (None, 128, 128, 256 0           conv2d_53[0][0]                  \n",
      "                                                                 concatenate_39[0][0]             \n",
      "                                                                 conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 128, 128, 256 1024        concatenate_41[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 128, 128, 256 0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 128, 128, 64) 147456      activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 128, 128, 32) 100384      conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 128, 128, 2)  66          conv2d_56[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 17,003,298\n",
      "Trainable params: 16,986,594\n",
      "Non-trainable params: 16,704\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "zip OK\n",
      "平均汉明距离为：533.8200\n",
      "平均杰卡德相似系数为：0.8408\n",
      "平均Dice相似系数为：0.8867\n",
      "平均误差为：37.0598%\n"
     ]
    }
   ],
   "source": [
    "# 全自动测试\n",
    "# %load 2.5Dv3.py\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from unittest import result\n",
    "import skimage.io as io\n",
    "import skimage.transform as trans\n",
    "from keras.optimizer_v2.adam import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as keras\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.saving.utils_v1.mode_keys import is_train\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import imageio.v2 as imageio\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.layers import BatchNormalization, Conv2D, Activation, Dropout, AveragePooling2D, concatenate, \\\n",
    "    GlobalAveragePooling2D, MaxPooling2D, Dense, Input\n",
    "from keras.regularizers import l2\n",
    "import keras.backend as K\n",
    "import zipfile\n",
    "import evalu\n",
    "import time\n",
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "# tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "# Read datasets\n",
    "def getimg(dir,j):\n",
    "    input_dir = dir\n",
    "    img_list = []\n",
    "    for i in range(1+j,901+j):\n",
    "        filename = f'{i}.tif'\n",
    "        img_path = os.path.join(input_dir,filename)\n",
    "        img = imageio.imread(img_path)\n",
    "        img = img.astype(np.float32)\n",
    "        img = img / 255.0\n",
    "        img_list.append(img)\n",
    "    return img_list\n",
    "\n",
    "def onehot(img):\n",
    "    img = tf.cast(img, tf.int32)\n",
    "    img_one_hot = tf.one_hot(img, depth=2, on_value=1.0, off_value=0.0, axis=-1)\n",
    "    return img_one_hot\n",
    "\n",
    "def add_channel(img): \n",
    "    img = tf.expand_dims(img, axis=-1) \n",
    "    return img\n",
    "# def train_generator(label_dataset, batch_size=32):\n",
    "#     image1_datagen = ImageDataGenerator()\n",
    "#     image2_datagen = ImageDataGenerator()\n",
    "#     mask_datagen = ImageDataGenerator()\n",
    "#     seed=1\n",
    "#     def image_generator():\n",
    "#         image_generator1 = image1_datagen.flow_from_directory(\n",
    "#             '/data/caizhizheng/2D/v2/data1',\n",
    "#             # The absolute path of the data set\n",
    "#             class_mode=None,\n",
    "#             batch_size=batch_size,\n",
    "#             color_mode='grayscale',\n",
    "#             target_size=(128, 128),\n",
    "#             # save_to_dir='./data/gen/images',\n",
    "#             shuffle=False,\n",
    "#             seed=seed)\n",
    "#         image_generator2 = image2_datagen.flow_from_directory(\n",
    "#             '/data/caizhizheng/2D/v2/data2',\n",
    "#             # The absolute path of the data set\n",
    "#             class_mode=None,\n",
    "#             batch_size=batch_size,\n",
    "#             color_mode='grayscale',\n",
    "#             target_size=(128, 128),\n",
    "#             shuffle=False,\n",
    "#             # save_to_dir='./data/gen/images',\n",
    "#             seed=seed)\n",
    "#         for imgs1, imgs2 in zip(image_generator1, image_generator2):\n",
    "#             imgs1 = imgs1 / 255.0\n",
    "#             imgs2 = imgs2 / 255.0\n",
    "#             yield (imgs1, imgs2)\n",
    "            \n",
    "#     def mask_generator():\n",
    "#         for masks in mask_datagen.flow(label_dataset, batch_size=batch_size):\n",
    "#             yield masks\n",
    "    \n",
    "#     train_dataset = tf.data.Dataset.from_generator(\n",
    "#         lambda: zip(image_generator(), mask_generator()),\n",
    "#         output_types=((tf.float32, tf.float32), tf.float32),\n",
    "#         output_shapes=(((batch_size, 128, 128, 1), (batch_size, 128, 128, 1)), (batch_size, 128, 128, 2))\n",
    "#     )\n",
    "    \n",
    "#     return train_dataset\n",
    "\n",
    "\n",
    "# Draw loss curve\n",
    "def plot_history(history, result_dir, prefix):\n",
    "    \"\"\"\n",
    "    将训练与验证的accuracy与loss画出来\n",
    "    \"\"\"\n",
    "    plt.plot(history.history['accuracy'], marker='.')\n",
    "    plt.plot(history.history['val_accuracy'], marker='.')\n",
    "    plt.title('model accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.grid()\n",
    "    plt.legend(['acc', 'val_acc'], loc='upper right')\n",
    "\t# plt.show()\n",
    "    plt.savefig('/code/loss_picture/unet_val_ace.png')\n",
    "    plt.close()\n",
    "\n",
    "    plt.plot(history.history['loss'], marker='.')\n",
    "    plt.plot(history.history['val_loss'], marker='.')\n",
    "    plt.title('model loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.grid()\n",
    "    plt.legend(['loss', 'val_loss'], loc='upper right')\n",
    "    # plt.show()\n",
    "    plt.savefig('/code/loss_picture/denseunet_loss.png')\n",
    "    plt.close()\n",
    "\n",
    "    # x = history.history['loss']\n",
    "    # np.savetxt('D:/pycharm/up_down_code/loss_picture/unet_train_loss.txt', x, fmt='%f')\n",
    "    # y = history.history['val_loss']\n",
    "    # np.savetxt('D:/pycharm/up_down_code/loss_picture/unet_val_loss.txt', y, fmt='%f')hb\n",
    "\n",
    "\n",
    "def Conv_Block(input_tensor, filters, bottleneck=False, weight_decay=1e-4):\n",
    "    \"\"\"    封装卷积层\n",
    "\n",
    "    :param input_tensor: 输入张量\n",
    "    :param filters: 卷积核数目\n",
    "    :param bottleneck: 是否使用bottleneck\n",
    "    :param dropout_rate: dropout比率\n",
    "    :param weight_decay: 权重衰减率\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    concat_axis = 1 if K.image_data_format() == 'channel_first' else -1  # 确定格式\n",
    "\n",
    "    x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5)(input_tensor)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # if bottleneck:\n",
    "    #     # 使用bottleneck进行降维\n",
    "    #     inter_channel = filters\n",
    "    #     x = Conv2D(inter_channel, (1, 1),\n",
    "    #                kernel_initializer='he_normal',\n",
    "    #                padding='same', use_bias=False,\n",
    "    #                kernel_regularizer=l2(weight_decay))(x)\n",
    "    #     x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5)(x)\n",
    "    #     x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters, (3, 3), kernel_initializer='he_normal', padding='same', use_bias=False)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def dens_block(input_tensor, nb_filter):\n",
    "    x1 = Conv_Block(input_tensor, nb_filter)\n",
    "    add1 = concatenate([x1, input_tensor], axis=-1)\n",
    "    x2 = Conv_Block(add1, nb_filter)\n",
    "    add2 = concatenate([x1, input_tensor, x2], axis=-1)\n",
    "    x3 = Conv_Block(add2, nb_filter)\n",
    "    return x3\n",
    "\n",
    "\n",
    "from keras.regularizers import l2\n",
    "\n",
    "\n",
    "# model definition\n",
    "def unet(input_shape=(128, 128, 2)):\n",
    "    # tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "    # inputs = Input(input_shape)\n",
    "    input1=Input(shape=(128,128,1))\n",
    "    input2=Input(shape=(128,128,1))\n",
    "    inputs=Concatenate(axis=-1)([input1,input2])\n",
    "    # x  = Conv2D(32, 1, strides=1, activation='relu', padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    x = Conv2D(32, 7, kernel_initializer='he_normal', padding='same', strides=1, use_bias=False,\n",
    "               kernel_regularizer=l2(1e-4))(inputs)\n",
    "    # down first\n",
    "    down1 = dens_block(x, nb_filter=64)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(down1)  # 256\n",
    "    # down second\n",
    "    down2 = dens_block(pool1, nb_filter=64)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(down2)  # 128\n",
    "    # down third\n",
    "    down3 = dens_block(pool2, nb_filter=128)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(down3)  # 64\n",
    "    # down four\n",
    "    down4 = dens_block(pool3, nb_filter=256)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(down4)  # 32\n",
    "    # center\n",
    "    conv5 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool4)\n",
    "    conv5 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    # up first\n",
    "    up6 = UpSampling2D(size=(2, 2))(drop5)\n",
    "    # up6 = UpSampling2D(size=(2, 2))(drop5)\n",
    "    add6 = concatenate([down4, up6], axis=3)\n",
    "    up6 = dens_block(add6, nb_filter=256)\n",
    "    # up second\n",
    "    up7 = UpSampling2D(size=(2, 2))(up6)\n",
    "    # up7 = UpSampling2D(size=(2, 2))(conv6)\n",
    "    add7 = concatenate([down3, up7], axis=3)\n",
    "    up7 = dens_block(add7, nb_filter=128)\n",
    "    # up third\n",
    "    up8 = UpSampling2D(size=(2, 2))(up7)\n",
    "    # up8 = UpSampling2D(size=(2, 2))(conv7)\n",
    "    add8 = concatenate([down2, up8], axis=-1)\n",
    "    up8 = dens_block(add8, nb_filter=64)\n",
    "    # up four\n",
    "    up9 = UpSampling2D(size=(2, 2))(up8)\n",
    "    add9 = concatenate([down1, up9], axis=-1)\n",
    "    up9 = dens_block(add9, nb_filter=64)\n",
    "    # output\n",
    "    conv10 = Conv2D(32, 7, strides=1, activation='relu', padding='same', kernel_initializer='he_normal')(up9)\n",
    "    conv10 = Conv2D(2, 1, activation='softmax')(conv10)\n",
    "    model = Model(inputs=[input1,input2], outputs=conv10)\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "# define Huber loss\n",
    "def huber_loss(y_true, y_pred):\n",
    "    return tf.losses.huber_loss(y_true, y_pred, delta=0.01)\n",
    "\n",
    "\n",
    "def simm_loss(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_pred - y_true)) + 0.01 * K.mean(K.abs(y_pred))\n",
    "    # return tf.abs(tf.norm(y_pred - y_true))/tf.norm(y_true)\n",
    "    \n",
    "# smooth = 1. # 用于防止分母为0.\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true) # 将 y_true 拉伸为一维.\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection) / (K.sum(y_true_f * y_true_f) + K.sum(y_pred_f * y_pred_f))\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1. - dice_coef(y_true, y_pred)\n",
    "\n",
    "# Define the learning rate attenuation value\n",
    "def scheduler(epoch):\n",
    "    if epoch % 10 == 0 and epoch != 0:\n",
    "        lr = K.get_value(model.optimizer.lr)\n",
    "        K.set_value(model.optimizer.lr, lr * 0.1)\n",
    "        print(\"lr change to {}\".format(lr * 0.1))\n",
    "    return K.get_value(model.optimizer.lr)\n",
    "\n",
    "\n",
    "# ssim psnr\n",
    "from ssim import compute_ssim\n",
    "import math\n",
    "\n",
    "\n",
    "def psnr(img1, img2):\n",
    "    mse = np.mean((img1 / 255. - img2 / 255.) ** 2)\n",
    "    if mse < 1.0e-10:\n",
    "        return 100\n",
    "    PIXEL_MAX = 1\n",
    "    return 20 * math.log10(PIXEL_MAX / math.sqrt(mse))\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "def add_gaussian_nois(image_in, mean=0, var=0.01):\n",
    "    \"\"\"\n",
    "    给图片添加高斯噪声\n",
    "    \"\"\"\n",
    "    img = image_in.astype(np.int16)\n",
    "    mu = 0\n",
    "    sigma = 40\n",
    "    for i in range(img.shape[0]):\n",
    "        for j in range(img.shape[1]):\n",
    "            for k in range(img.shape[2]):\n",
    "                img[i, j, k] = img[i, j, k] + random.gauss(mu=mu, sigma=sigma)\n",
    "    img[img > 255] = 255\n",
    "    img[img < 0] = 0\n",
    "    img_out = img.astype(np.uint8)\n",
    "\n",
    "    # cv2.imshow(\"noise_image\",img_out)\n",
    "    # cv2.waitKey(0)\n",
    "    return img_out\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "  0.001,\n",
    "  decay_steps=10,\n",
    "  decay_rate=1,\n",
    "  staircase=False)\n",
    "def train(model):\n",
    "    # train\n",
    "    # tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "    # no shutil and shutil module\n",
    "    # logdir = pathlib.Path(tempfile.mkdtemp())/\"tensorboard_logs\"\n",
    "    # shutil.rmtree(logdir, ignore_errors=True)\n",
    "    # use tf.io.gfile\n",
    "    # logdir = tf.io.gfile.mkdir('/tensorboard_logs')\n",
    "    # tf.io.gfile.rmtree (logdir)\n",
    "    model.compile(optimizer=Adam(learning_rate=lr_schedule), loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "    # reduce_lr = LearningRateScheduler(scheduler)\n",
    "    # reduce_lr = LearningRateScheduler(lschedule)\n",
    "    model_checkpoint = ModelCheckpoint('/code/save_model/'+model_savename, monitor='loss', verbose=1,\n",
    "                                       save_best_only=True)\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "    Board = tf.keras.callbacks.TensorBoard(log_dir=\"/output/logs\")\n",
    "\n",
    "    history = model.fit(train_dataset.repeat(),\n",
    "                                  steps_per_epoch=100,\n",
    "                                  epochs=400,\n",
    "                                  validation_data=validation_dataset,\n",
    "                                #   validation_steps=50,\n",
    "                                  callbacks=[model_checkpoint,\n",
    "                                             early_stop,Board\n",
    "                                             ])\n",
    "    plot_history(history, '.results/', 'Unet')\n",
    "    return model\n",
    "\n",
    "def test(model):\n",
    "    # test\n",
    "    input_dir1 = input_dir2 = '/data/caizhizheng/2D/v2/predicted_input_data'\n",
    "    # output_dir = '/data/caizhizheng/2D/v2/result/'\n",
    "    n = len(os.listdir(input_dir1))\n",
    "    # n = n/2\n",
    "    for i in range(1,n//2+1):\n",
    "        # x = cv2.imread('/data/caizhizheng/data/test/%d.tif' % (i))  # #The absolute path of the testsets\n",
    "        # x = add_gaussian_nois(x)\n",
    "        # x = cv2.cvtColor(x,cv2.COLOR_BGR2GRAY)\n",
    "        filename1 = f'{i}.tif'\n",
    "        img_path1 = os.path.join(input_dir1,filename1)\n",
    "        x1 = cv2.imread(img_path1, cv2.IMREAD_GRAYSCALE)\n",
    "        x1 = x1 / 255.0\n",
    "        x1 = np.array([x1])\n",
    "        filename2 = f'{i+n//2}.tif'\n",
    "        img_path2 = os.path.join(input_dir2,filename2)\n",
    "        x2 = cv2.imread(img_path2, cv2.IMREAD_GRAYSCALE)\n",
    "        x2 = x2 / 255.0\n",
    "        x2 = np.array([x2])\n",
    "        # xt = np.stack([x1, x2], axis=-1)\n",
    "        mask_tensor = model.predict([x1,x2], batch_size=None, verbose=0, steps=None)\n",
    "        mask = mask_tensor[0]\n",
    "        mask = tf.argmax(mask, axis=-1)\n",
    "        mask = tf.keras.backend.eval(mask)\n",
    "        mask = (mask * 255).astype(np.uint8)\n",
    "        cv2.imwrite(output_dir+'%d.tif' % (i), mask)\n",
    "        \n",
    "def make_zip(source_dir, output_name):\n",
    "    zipf = zipfile.ZipFile(output_name, 'w')\n",
    "    prelen = len(os.path.dirname(source_dir))\n",
    "    for parent, _, filenames in os.walk(source_dir):\n",
    "        for filename in filenames:\n",
    "            pathfile = os.path.join(parent, filename)\n",
    "            arcname = pathfile[prelen:].strip(os.path.sep)     #相对路径\n",
    "            zipf.write(pathfile, arcname)\n",
    "        zipf.close()\n",
    "BATCH_SIZE=128\n",
    "if __name__ == '__main__':\n",
    "    # is_train = False # you can change this to False if you want to test only\n",
    "    model = unet(input_shape=(128, 128, 2))\n",
    "    # model = tf.keras.models.load_model('/code/save_model/dens_2.5Dv3.keras')\n",
    "    # if is_train:\n",
    "    label_list=getimg(dir='/data/caizhizheng/2D/v2/label/label',j=0)\n",
    "    label_dataset = tf.data.Dataset.from_tensor_slices(label_list)\n",
    "    label_dataset = label_dataset.map(onehot)\n",
    "    img1_list=getimg(dir='/data/caizhizheng/2D/v2/data1/data',j=0)\n",
    "    img1_dataset = tf.data.Dataset.from_tensor_slices(img1_list)\n",
    "    # img1_dataset = img1_dataset.map(add_channel)\n",
    "    img2_list=getimg(dir='/data/caizhizheng/2D/v2/data2/data',j=900)\n",
    "    img2_dataset = tf.data.Dataset.from_tensor_slices(img2_list)\n",
    "    # img2_dataset = img2_dataset.map(add_channel)\n",
    "    image_dataset = tf.data.Dataset.zip((img1_dataset, img2_dataset))\n",
    "    train_dataset = tf.data.Dataset.zip((image_dataset, label_dataset))\n",
    "    #4th modifation\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=len(train_dataset))\n",
    "    validation_dataset = train_dataset.take(32).cache().batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    train_dataset = train_dataset.skip(32).cache()\n",
    "    \n",
    "    # train_dataset = train_dataset.cache()\n",
    "    train_dataset = train_dataset.shuffle(BATCH_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    # train_dataset = train_dataset.batch(32)\n",
    "\n",
    "    time1 = time.strftime('%m%d%H')\n",
    "    model_savename = 'dens_2.5Dv3' + time1 +'.keras'\n",
    "    result_name = 'result' + time1 + '.zip'\n",
    "    output_dir = \"/data/caizhizheng/2D/v2/result\"\n",
    "    output_name = \"/data/caizhizheng/2D/v2/\"+result_name\n",
    "    \n",
    "    # model = train(model) # train the model and save the best weights\n",
    "    # else:\n",
    "    model.load_weights('/code/save_model/dens_2.5Dv3.keras') # load the saved weights\n",
    "    test(model) # test the model and save the results\n",
    "    \n",
    "    make_zip(source_dir=output_dir, output_name=output_name)\n",
    "    print(\"zip OK\")\n",
    "    evalu.main(output_dir=output_dir, label_dir='/data/caizhizheng/2D/v2/predicted_label/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bac9f3-7594-4e98-9a30-193bfca0c1b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f257ec0-a9d9-46df-ab32-67bfc1433816",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model predict OK\n",
      "平均汉明距离为：158.4500\n",
      "平均杰卡德相似系数为：0.6010\n",
      "平均Dice相似系数为：0.6573\n",
      "平均误差为：59.1376%\n"
     ]
    }
   ],
   "source": [
    "# %load unet_alltrain.py\n",
    "import os\n",
    "import shutil\n",
    "from unittest import result\n",
    "import skimage.io as io\n",
    "import skimage.transform as trans\n",
    "from keras.optimizer_v2.adam import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as keras\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.saving.utils_v1.mode_keys import is_train\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import imageio.v2 as imageio\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.layers import BatchNormalization, Conv2D, Activation, Dropout, AveragePooling2D, concatenate, \\\n",
    "    GlobalAveragePooling2D, MaxPooling2D, Dense, Input\n",
    "from keras.regularizers import l2\n",
    "import keras.backend as K\n",
    "import zipfile\n",
    "import evalu\n",
    "import time\n",
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "# tf.config.experimental.set_memory_growth(gpus[1], True)\n",
    "# tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "# Read datasets\n",
    "def getimg(dir,num,j):\n",
    "    input_dir = dir\n",
    "    img_list = []\n",
    "    for i in range(1+j,num+j):\n",
    "        filename = f'{i}.tif'\n",
    "        img_path = os.path.join(input_dir,filename)\n",
    "        img = imageio.imread(img_path)\n",
    "        img = img.astype(np.float32)\n",
    "        img = img / 255.0\n",
    "        img_list.append(img)\n",
    "    return img_list\n",
    "\n",
    "def onehot(img):\n",
    "    img = tf.cast(img, tf.int32)\n",
    "    img_one_hot = tf.one_hot(img, depth=2, on_value=1.0, off_value=0.0, axis=-1)\n",
    "    return img_one_hot\n",
    "\n",
    "# def add_channel(img): \n",
    "#     img = tf.expand_dims(img, axis=-1) \n",
    "#     return img\n",
    "# Draw loss curve\n",
    "def plot_history(history, result_dir, prefix):\n",
    "    \"\"\"\n",
    "    将训练与验证的accuracy与loss画出来\n",
    "    \"\"\"\n",
    "    plt.plot(history.history['accuracy'], marker='.')\n",
    "    plt.plot(history.history['val_accuracy'], marker='.')\n",
    "    plt.title('model accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.grid()\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend(['acc', 'val_acc'], loc='upper right')\n",
    "\t# plt.show()\n",
    "    plt.savefig(result_dir + 'unet_ace' + prefix + '.png')\n",
    "    plt.close()\n",
    "\n",
    "    plt.plot(history.history['loss'], marker='.')\n",
    "    plt.plot(history.history['val_loss'], marker='.')\n",
    "    plt.title('model loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.yscale(\"log\")\n",
    "    plt.grid()\n",
    "    plt.legend(['loss', 'val_loss'], loc='upper right')\n",
    "    # plt.show()\n",
    "    plt.savefig(result_dir + 'unet_loss'+ prefix +'.png')\n",
    "    plt.close()\n",
    "\n",
    "    # x = history.history['loss']\n",
    "    # np.savetxt('D:/pycharm/up_down_code/loss_picture/unet_train_loss.txt', x, fmt='%f')\n",
    "    # y = history.history['val_loss']\n",
    "    # np.savetxt('D:/pycharm/up_down_code/loss_picture/unet_val_loss.txt', y, fmt='%f')hb\n",
    "\n",
    "\n",
    "def Conv_Block(input_tensor, filters, bottleneck=False, weight_decay=1e-4):\n",
    "    \"\"\"    封装卷积层\n",
    "\n",
    "    :param input_tensor: 输入张量\n",
    "    :param filters: 卷积核数目\n",
    "    :param bottleneck: 是否使用bottleneck\n",
    "    :param dropout_rate: dropout比率\n",
    "    :param weight_decay: 权重衰减率\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    concat_axis = 1 if K.image_data_format() == 'channel_first' else -1  # 确定格式\n",
    "\n",
    "    x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5)(input_tensor)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # if bottleneck:\n",
    "    #     # 使用bottleneck进行降维\n",
    "    #     inter_channel = filters\n",
    "    #     x = Conv2D(inter_channel, (1, 1),\n",
    "    #                kernel_initializer='he_normal',\n",
    "    #                padding='same', use_bias=False,\n",
    "    #                kernel_regularizer=l2(weight_decay))(x)\n",
    "    #     x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5)(x)\n",
    "    #     x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters, (3, 3), kernel_initializer='he_normal', padding='same', use_bias=False)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def dens_block(input_tensor, nb_filter):\n",
    "    x1 = Conv_Block(input_tensor, nb_filter)\n",
    "    add1 = concatenate([x1, input_tensor], axis=-1)\n",
    "    x2 = Conv_Block(add1, nb_filter)\n",
    "    add2 = concatenate([x1, input_tensor, x2], axis=-1)\n",
    "    x3 = Conv_Block(add2, nb_filter)\n",
    "    return x3\n",
    "\n",
    "\n",
    "from keras.regularizers import l2\n",
    "\n",
    "\n",
    "# model definition\n",
    "def unet(input_shape=(128, 128, 2)):\n",
    "    # tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "    inputs = Input(input_shape)\n",
    "    # input1=Input(shape=(128,128,1))\n",
    "    # input2=Input(shape=(128,128,1))\n",
    "    # inputs=Concatenate(axis=-1)([input1,input2])\n",
    "    # inputs = Input(shape\n",
    "    # x  = Conv2D(32, 1, strides=1, activation='relu', padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    x = Conv2D(32, 7, kernel_initializer='he_normal', padding='same', strides=1, use_bias=False,\n",
    "               kernel_regularizer=l2(1e-4))(inputs)\n",
    "    # down first\n",
    "    down1 = dens_block(x, nb_filter=64)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(down1)  # 256\n",
    "    # down second\n",
    "    down2 = dens_block(pool1, nb_filter=64)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(down2)  # 128\n",
    "    # down third\n",
    "    down3 = dens_block(pool2, nb_filter=128)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(down3)  # 64\n",
    "    # down four\n",
    "    down4 = dens_block(pool3, nb_filter=256)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(down4)  # 32\n",
    "    # center\n",
    "    conv5 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool4)\n",
    "    conv5 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    # up first\n",
    "    up6 = UpSampling2D(size=(2, 2))(drop5)\n",
    "    # up6 = UpSampling2D(size=(2, 2))(drop5)\n",
    "    add6 = concatenate([down4, up6], axis=3)\n",
    "    up6 = dens_block(add6, nb_filter=256)\n",
    "    # up second\n",
    "    up7 = UpSampling2D(size=(2, 2))(up6)\n",
    "    # up7 = UpSampling2D(size=(2, 2))(conv6)\n",
    "    add7 = concatenate([down3, up7], axis=3)\n",
    "    up7 = dens_block(add7, nb_filter=128)\n",
    "    # up third\n",
    "    up8 = UpSampling2D(size=(2, 2))(up7)\n",
    "    # up8 = UpSampling2D(size=(2, 2))(conv7)\n",
    "    add8 = concatenate([down2, up8], axis=-1)\n",
    "    up8 = dens_block(add8, nb_filter=64)\n",
    "    # up four\n",
    "    up9 = UpSampling2D(size=(2, 2))(up8)\n",
    "    add9 = concatenate([down1, up9], axis=-1)\n",
    "    up9 = dens_block(add9, nb_filter=64)\n",
    "    # output\n",
    "    conv10 = Conv2D(32, 7, strides=1, activation='relu', padding='same', kernel_initializer='he_normal')(up9)\n",
    "    conv10 = Conv2D(2, 1, activation='softmax')(conv10)\n",
    "    # model = Model(inputs=[input1,input2], outputs=conv10)\n",
    "    model = Model(inputs=inputs, outputs=conv10)\n",
    "    # print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "# define Huber loss\n",
    "def huber_loss(y_true, y_pred):\n",
    "    return tf.losses.huber_loss(y_true, y_pred, delta=0.01)\n",
    "\n",
    "\n",
    "def simm_loss(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_pred - y_true)) + 0.01 * K.mean(K.abs(y_pred))\n",
    "    # return tf.abs(tf.norm(y_pred - y_true))/tf.norm(y_true)\n",
    "    \n",
    "# smooth = 1. # 用于防止分母为0.\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true) # 将 y_true 拉伸为一维.\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection) / (K.sum(y_true_f * y_true_f) + K.sum(y_pred_f * y_pred_f))\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1. - dice_coef(y_true, y_pred)\n",
    "\n",
    "# Define the learning rate attenuation value\n",
    "def scheduler(epoch):\n",
    "    if epoch % 10 == 0 and epoch != 0:\n",
    "        lr = K.get_value(model.optimizer.lr)\n",
    "        K.set_value(model.optimizer.lr, lr * 0.1)\n",
    "        print(\"lr change to {}\".format(lr * 0.1))\n",
    "    return K.get_value(model.optimizer.lr)\n",
    "\n",
    "import random\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "  0.00001,\n",
    "  decay_steps=1000,\n",
    "  decay_rate=1,\n",
    "  staircase=False)\n",
    "        \n",
    "def make_zip(source_dir, output_name):\n",
    "    zipf = zipfile.ZipFile(output_name, 'w')\n",
    "    prelen = len(os.path.dirname(source_dir))\n",
    "    for parent, _, filenames in os.walk(source_dir):\n",
    "        for filename in filenames:\n",
    "            pathfile = os.path.join(parent, filename)\n",
    "            arcname = pathfile[prelen:].strip(os.path.sep)     #相对路径\n",
    "            zipf.write(pathfile, arcname)\n",
    "        zipf.close()\n",
    "\n",
    "def train(model, model_savename, train_dataset, validation_dataset):\n",
    "    # model.compile(optimizer=Adam(learning_rate=lr_schedule), loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "    # reduce_lr = LearningRateScheduler(scheduler)\n",
    "    # reduce_lr = LearningRateScheduler(lschedule)\n",
    "    # model.load_weights('/code/save_model/exped/'+model_savename)\n",
    "    model_checkpoint = ModelCheckpoint('/code/save_model/exped/'+model_savename, monitor='loss', verbose=1,\n",
    "                                       save_best_only=True)\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=500)\n",
    "    Board = tf.keras.callbacks.TensorBoard(log_dir=\"/output/logs\")\n",
    "    \n",
    "    history = model.fit(train_dataset.repeat(),\n",
    "                                  steps_per_epoch=100,\n",
    "                                  epochs=20000,\n",
    "                                  validation_data=validation_dataset,\n",
    "                                #   validation_steps=50,\n",
    "                                  callbacks=[model_checkpoint,\n",
    "                                             early_stop,Board\n",
    "                                             # Board\n",
    "                                             ])\n",
    "    plot_history(history, '/code/loss_picture/extended/',prefix='_exped')\n",
    "    return model\n",
    "    \n",
    "def test(model, test_dataset, output_path, length):\n",
    "    mask = model.predict(test_dataset)\n",
    "    mask = tf.argmax(mask, axis=-1)\n",
    "    mask = tf.keras.backend.eval(mask)\n",
    "    mask = (mask * 255).astype(np.uint8)\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    for j in range(0, length):\n",
    "        cv2.imwrite(output_path+ '%d.tif' %(j+1), mask[j])\n",
    "    print('model predict OK')\n",
    "        \n",
    "BATCH_SIZE=128\n",
    "plot_path = '/code/loss_picture/extended/'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # is_train = False # you can change this to False if you want to test only\n",
    "    output_path = '/data/caizhizheng/2D/v2/exped_output/'\n",
    "    label_path = '/data/caizhizheng/2D/v2/label/label/'\n",
    "    img_path = '/data/caizhizheng/2D/v2/data1/data/'\n",
    "    img1_list = getimg(img_path,num=1201, j=0)\n",
    "    img_path = '/data/caizhizheng/2D/v2/data2/data/'\n",
    "    img2_list = getimg(img_path,num=1201, j=900)\n",
    "    label_list = getimg(label_path,num=1201, j=0)\n",
    "    img_list = np.stack((img1_list,img2_list),axis = -1)\n",
    "    \n",
    "    test_path = '/data/caizhizheng/2D/v2/test/predicted_input_data/'\n",
    "    test_label_path = '/data/caizhizheng/2D/v2/test/predicted_label/'\n",
    "    # output_path = '/data/caizhizheng/2D/v2/test/outputv2/'\n",
    "    img1_list = getimg(test_path,num=101, j =0)\n",
    "    img2_list = getimg(test_path,num=101, j =100)\n",
    "    test_img_list = np.stack((img1_list,img2_list),axis = -1)\n",
    "    test_label_list = getimg(test_label_path, num=101, j=0)\n",
    "    \n",
    "    img_list = np.vstack((img_list, test_img_list))\n",
    "    label_list = np.vstack((label_list, test_label_list))\n",
    "    \n",
    "    image_dataset = tf.data.Dataset.from_tensor_slices(img_list)\n",
    "    label_dataset = tf.data.Dataset.from_tensor_slices(label_list)\n",
    "    label_dataset = label_dataset.map(onehot)\n",
    "    # 使用事先保存的数据集会造成内存泄漏？？这是为什么，以前使用并不会\n",
    "    # image_dataset = tf.data.experimental.load(\"dataset/image_exped\")\n",
    "    # label_dataset = tf.data.experimental.load(\"dataset/label_exped\")\n",
    "    # x = next(iter(image_dataset.batch(image_dataset.cardinality().numpy())))\n",
    "    # y = next(iter(label_dataset.batch(label_dataset.cardinality().numpy())))\n",
    "    # train_dataset = tf.data.Dataset.zip((image_dataset, label_dataset))\n",
    "    # train_dataset = train_dataset.cache().shuffle(train_dataset.cardinality())\n",
    "\n",
    "    # validation_dataset = train_dataset.take(50).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    # train_dataset = train_dataset.skip(50).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    # mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "    # with mirrored_strategy.scope():\n",
    "    model = unet(input_shape=(128, 128, 2))\n",
    "    #     model.compile(optimizer=Adam(learning_rate=lr_schedule), loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "    #     model.load_weights('/code/save_model/exped/cross_ini_274.keras')\n",
    "    model_savename = 'expedv2.keras'\n",
    "    model.load_weights('/code/save_model/exped/'+model_savename)\n",
    "    # train = train(model, model_savename, train_dataset, validation_dataset)\n",
    "    \n",
    "    # image_dataset = image_dataset.batch(BATCH_SIZE)\n",
    "    # test(model, image_dataset, output_path, 1300)\n",
    "    # # name = 'error2' + '_postCrV' + '.csv'\n",
    "    error_path = './errors/'\n",
    "    name = 'error_expedv2' + '.csv'\n",
    "    # name = os.path.join(error_path, name)\n",
    "    # evalu.main(output_dir=output_path, label_dir=label_path, name=name, oder=np.array(0))\n",
    "    \n",
    "#     测试集\n",
    "    test_path = '/data/caizhizheng/2D/v2/test/test2/data/'\n",
    "    test_label_path = '/data/caizhizheng/2D/v2/test/test2/label/'\n",
    "    output_path = '/data/caizhizheng/2D/v2/test/outputv2/'\n",
    "    img1_list = getimg(test_path,num=201, j =0)\n",
    "    img2_list = getimg(test_path,num=201, j =200)\n",
    "    img_list = np.stack((img1_list,img2_list),axis = -1)\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices(img_list)\n",
    "    test_dataset = test_dataset.batch(1)\n",
    "    # model.load_weights('/code/save_model/exped/'+model_savename)\n",
    "    test(model, test_dataset, output_path,200)\n",
    "    name = 'error_exped_testv2.csv'\n",
    "    name = os.path.join(error_path, name)\n",
    "    evalu.main(output_dir=output_path, label_dir=test_label_path, name=name, oder=np.array(0))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6939926-e573-4774-80a1-e20cfd316714",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model predict OK\n",
      "平均汉明距离为：152.7400\n",
      "平均杰卡德相似系数为：0.5601\n",
      "平均Dice相似系数为：0.6170\n",
      "平均误差为：62.0764%\n"
     ]
    }
   ],
   "source": [
    "# %load unet_iou.py\n",
    "import os\n",
    "import shutil\n",
    "from unittest import result\n",
    "import skimage.io as io\n",
    "import skimage.transform as trans\n",
    "from keras.optimizer_v2.adam import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as keras\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.saving.utils_v1.mode_keys import is_train\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import imageio.v2 as imageio\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.layers import BatchNormalization, Conv2D, Activation, Dropout, AveragePooling2D, concatenate, \\\n",
    "    GlobalAveragePooling2D, MaxPooling2D, Dense, Input\n",
    "from keras.regularizers import l2\n",
    "import keras.backend as K\n",
    "import zipfile\n",
    "import evalu\n",
    "import time\n",
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "# tf.config.experimental.set_memory_growth(gpus[1], True)\n",
    "# tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "# Read datasets\n",
    "def getimg(dir,num,j):\n",
    "    input_dir = dir\n",
    "    img_list = []\n",
    "    for i in range(1+j,num+j):\n",
    "        filename = f'{i}.tif'\n",
    "        img_path = os.path.join(input_dir,filename)\n",
    "        img = imageio.imread(img_path)\n",
    "        img = img.astype(np.float32)\n",
    "        img = img / 255.0\n",
    "        img_list.append(img)\n",
    "    return img_list\n",
    "\n",
    "def onehot(img):\n",
    "    img = tf.cast(img, tf.int32)\n",
    "    img_one_hot = tf.one_hot(img, depth=2, on_value=1.0, off_value=0.0, axis=-1)\n",
    "    return img_one_hot\n",
    "\n",
    "# def add_channel(img): \n",
    "#     img = tf.expand_dims(img, axis=-1) \n",
    "#     return img\n",
    "# Draw loss curve\n",
    "def plot_history(history, result_dir, prefix):\n",
    "    \"\"\"\n",
    "    将训练与验证的accuracy与loss画出来\n",
    "    \"\"\"\n",
    "    plt.plot(history.history['accuracy'], marker='.')\n",
    "    plt.plot(history.history['val_accuracy'], marker='.')\n",
    "    plt.title('model accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.grid()\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend(['acc', 'val_acc'], loc='upper right')\n",
    "\t# plt.show()\n",
    "    plt.savefig(result_dir + 'unet_ace' + prefix + '.png')\n",
    "    plt.close()\n",
    "\n",
    "    plt.plot(history.history['loss'], marker='.')\n",
    "    plt.plot(history.history['val_loss'], marker='.')\n",
    "    plt.title('model loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.yscale(\"log\")\n",
    "    plt.grid()\n",
    "    plt.legend(['loss', 'val_loss'], loc='upper right')\n",
    "    # plt.show()\n",
    "    plt.savefig(result_dir + 'unet_loss'+ prefix +'.png')\n",
    "    plt.close()\n",
    "\n",
    "    # x = history.history['loss']\n",
    "    # np.savetxt('D:/pycharm/up_down_code/loss_picture/unet_train_loss.txt', x, fmt='%f')\n",
    "    # y = history.history['val_loss']\n",
    "    # np.savetxt('D:/pycharm/up_down_code/loss_picture/unet_val_loss.txt', y, fmt='%f')hb\n",
    "\n",
    "\n",
    "def Conv_Block(input_tensor, filters, bottleneck=False, weight_decay=1e-4):\n",
    "    \"\"\"    封装卷积层\n",
    "\n",
    "    :param input_tensor: 输入张量\n",
    "    :param filters: 卷积核数目\n",
    "    :param bottleneck: 是否使用bottleneck\n",
    "    :param dropout_rate: dropout比率\n",
    "    :param weight_decay: 权重衰减率\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    concat_axis = 1 if K.image_data_format() == 'channel_first' else -1  # 确定格式\n",
    "\n",
    "    x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5)(input_tensor)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # if bottleneck:\n",
    "    #     # 使用bottleneck进行降维\n",
    "    #     inter_channel = filters\n",
    "    #     x = Conv2D(inter_channel, (1, 1),\n",
    "    #                kernel_initializer='he_normal',\n",
    "    #                padding='same', use_bias=False,\n",
    "    #                kernel_regularizer=l2(weight_decay))(x)\n",
    "    #     x = BatchNormalization(axis=concat_axis, epsilon=1.1e-5)(x)\n",
    "    #     x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters, (3, 3), kernel_initializer='he_normal', padding='same', use_bias=False)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def dens_block(input_tensor, nb_filter):\n",
    "    x1 = Conv_Block(input_tensor, nb_filter)\n",
    "    add1 = concatenate([x1, input_tensor], axis=-1)\n",
    "    x2 = Conv_Block(add1, nb_filter)\n",
    "    add2 = concatenate([x1, input_tensor, x2], axis=-1)\n",
    "    x3 = Conv_Block(add2, nb_filter)\n",
    "    return x3\n",
    "\n",
    "\n",
    "from keras.regularizers import l2\n",
    "\n",
    "\n",
    "# model definition\n",
    "def unet(input_shape=(128, 128, 2)):\n",
    "    # tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "    inputs = Input(input_shape)\n",
    "    # input1=Input(shape=(128,128,1))\n",
    "    # input2=Input(shape=(128,128,1))\n",
    "    # inputs=Concatenate(axis=-1)([input1,input2])\n",
    "    # inputs = Input(shape\n",
    "    # x  = Conv2D(32, 1, strides=1, activation='relu', padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    x = Conv2D(32, 7, kernel_initializer='he_normal', padding='same', strides=1, use_bias=False,\n",
    "               kernel_regularizer=l2(1e-4))(inputs)\n",
    "    # down first\n",
    "    down1 = dens_block(x, nb_filter=64)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(down1)  # 256\n",
    "    # down second\n",
    "    down2 = dens_block(pool1, nb_filter=64)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(down2)  # 128\n",
    "    # down third\n",
    "    down3 = dens_block(pool2, nb_filter=128)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(down3)  # 64\n",
    "    # down four\n",
    "    down4 = dens_block(pool3, nb_filter=256)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(down4)  # 32\n",
    "    # center\n",
    "    conv5 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool4)\n",
    "    conv5 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    # up first\n",
    "    up6 = UpSampling2D(size=(2, 2))(drop5)\n",
    "    # up6 = UpSampling2D(size=(2, 2))(drop5)\n",
    "    add6 = concatenate([down4, up6], axis=3)\n",
    "    up6 = dens_block(add6, nb_filter=256)\n",
    "    # up second\n",
    "    up7 = UpSampling2D(size=(2, 2))(up6)\n",
    "    # up7 = UpSampling2D(size=(2, 2))(conv6)\n",
    "    add7 = concatenate([down3, up7], axis=3)\n",
    "    up7 = dens_block(add7, nb_filter=128)\n",
    "    # up third\n",
    "    up8 = UpSampling2D(size=(2, 2))(up7)\n",
    "    # up8 = UpSampling2D(size=(2, 2))(conv7)\n",
    "    add8 = concatenate([down2, up8], axis=-1)\n",
    "    up8 = dens_block(add8, nb_filter=64)\n",
    "    # up four\n",
    "    up9 = UpSampling2D(size=(2, 2))(up8)\n",
    "    add9 = concatenate([down1, up9], axis=-1)\n",
    "    up9 = dens_block(add9, nb_filter=64)\n",
    "    # output\n",
    "    conv10 = Conv2D(32, 7, strides=1, activation='relu', padding='same', kernel_initializer='he_normal')(up9)\n",
    "    conv10 = Conv2D(2, 1, activation='softmax')(conv10)\n",
    "    # model = Model(inputs=[input1,input2], outputs=conv10)\n",
    "    model = Model(inputs=inputs, outputs=conv10)\n",
    "    # print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "# define Huber loss\n",
    "def huber_loss(y_true, y_pred):\n",
    "    return tf.losses.huber_loss(y_true, y_pred, delta=0.01)\n",
    "\n",
    "\n",
    "def simm_loss(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_pred - y_true)) + 0.01 * K.mean(K.abs(y_pred))\n",
    "    # return tf.abs(tf.norm(y_pred - y_true))/tf.norm(y_true)\n",
    "    \n",
    "# smooth = 1. # 用于防止分母为0.\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true) # 将 y_true 拉伸为一维.\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection) / (K.sum(y_true_f * y_true_f) + K.sum(y_pred_f * y_pred_f))\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1. - dice_coef(y_true, y_pred)\n",
    "\n",
    "# Define the learning rate attenuation value\n",
    "def scheduler(epoch):\n",
    "    if epoch % 10 == 0 and epoch != 0:\n",
    "        lr = K.get_value(model.optimizer.lr)\n",
    "        K.set_value(model.optimizer.lr, lr * 0.1)\n",
    "        print(\"lr change to {}\".format(lr * 0.1))\n",
    "    return K.get_value(model.optimizer.lr)\n",
    "\n",
    "import random\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "  0.001,\n",
    "  decay_steps=1000,\n",
    "  decay_rate=1,\n",
    "  staircase=False)\n",
    "        \n",
    "def make_zip(source_dir, output_name):\n",
    "    zipf = zipfile.ZipFile(output_name, 'w')\n",
    "    prelen = len(os.path.dirname(source_dir))\n",
    "    for parent, _, filenames in os.walk(source_dir):\n",
    "        for filename in filenames:\n",
    "            pathfile = os.path.join(parent, filename)\n",
    "            arcname = pathfile[prelen:].strip(os.path.sep)     #相对路径\n",
    "            zipf.write(pathfile, arcname)\n",
    "        zipf.close()\n",
    "\n",
    "def train(model, model_savename, train_dataset, validation_dataset):\n",
    "    model.compile(optimizer=Adam(learning_rate=lr_schedule), loss=dice_coef_loss, metrics=['accuracy'])\n",
    "    # reduce_lr = LearningRateScheduler(scheduler)\n",
    "    # reduce_lr = LearningRateScheduler(lschedule)\n",
    "    model_checkpoint = ModelCheckpoint('/code/save_model/exped/'+model_savename, monitor='loss', verbose=1,\n",
    "                                       save_best_only=True)\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=100)\n",
    "    Board = tf.keras.callbacks.TensorBoard(log_dir=\"/output/logs\")\n",
    "    history = model.fit(train_dataset.repeat(),\n",
    "                                  steps_per_epoch=100,\n",
    "                                  epochs=1000,\n",
    "                                  validation_data=validation_dataset,\n",
    "                                #   validation_steps=50,\n",
    "                                  callbacks=[model_checkpoint,\n",
    "                                             early_stop,Board\n",
    "                                             # Board\n",
    "                                             ])\n",
    "    plot_history(history, '/code/loss_picture/extended/',prefix='_exped_iou')\n",
    "    return model\n",
    "    \n",
    "def test(model, test_dataset, output_path, length):\n",
    "    mask = model.predict(test_dataset)\n",
    "    mask = tf.argmax(mask, axis=-1)\n",
    "    mask = tf.keras.backend.eval(mask)\n",
    "    mask = (mask * 255).astype(np.uint8)\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    for j in range(0, length):\n",
    "        cv2.imwrite(output_path+ '%d.tif' %(j+1), mask[j])\n",
    "    print('model predict OK')\n",
    "        \n",
    "BATCH_SIZE=128\n",
    "plot_path = '/code/loss_picture/extended/'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # is_train = False # you can change this to False if you want to test only\n",
    "    output_path = '/data/caizhizheng/2D/v2/exped_iou_output/'\n",
    "    label_path = '/data/caizhizheng/2D/v2/label/label/'\n",
    "    model = unet(input_shape=(128, 128, 2))\n",
    "    img_path = '/data/caizhizheng/2D/v2/data1/data/'\n",
    "    img1_list = getimg(img_path,num=1201, j=0)\n",
    "    img_path = '/data/caizhizheng/2D/v2/data2/data/'\n",
    "    img2_list = getimg(img_path,num=1201, j=900)\n",
    "    label_list = getimg(label_path,num=1201, j=0)\n",
    "    img_list = np.stack((img1_list,img2_list),axis = -1)\n",
    "    \n",
    "    test_path = '/data/caizhizheng/2D/v2/test/predicted_input_data/'\n",
    "    test_label_path = '/data/caizhizheng/2D/v2/test/predicted_label/'\n",
    "    # output_path = '/data/caizhizheng/2D/v2/test/outputv2/'\n",
    "    img1_list = getimg(test_path,num=101, j =0)\n",
    "    img2_list = getimg(test_path,num=101, j =100)\n",
    "    test_img_list = np.stack((img1_list,img2_list),axis = -1)\n",
    "    test_label_list = getimg(test_label_path, num=101, j=0)\n",
    "    \n",
    "#     img_list = np.vstack((img_list, test_img_list))\n",
    "#     label_list = np.vstack((label_list, test_label_list))\n",
    "    \n",
    "#     image_dataset = tf.data.Dataset.from_tensor_slices(img_list)\n",
    "#     label_dataset = tf.data.Dataset.from_tensor_slices(label_list)\n",
    "#     label_dataset = label_dataset.map(onehot)\n",
    "    # 使用事先保存的数据集会造成内存泄漏？？这是为什么，以前使用并不会\n",
    "    # image_dataset = tf.data.experimental.load(\"dataset/image_exped\")\n",
    "    # label_dataset = tf.data.experimental.load(\"dataset/label_exped\")\n",
    "    # x = next(iter(image_dataset.batch(image_dataset.cardinality().numpy())))\n",
    "    # y = next(iter(label_dataset.batch(label_dataset.cardinality().numpy())))\n",
    "#     train_dataset = tf.data.Dataset.zip((image_dataset, label_dataset))\n",
    "#     train_dataset = train_dataset.cache().shuffle(train_dataset.cardinality())\n",
    "\n",
    "#     validation_dataset = train_dataset.take(50).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "#     train_dataset = train_dataset.skip(50).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    model_savename = 'exped_iou.keras'\n",
    "    # train = train(model, model_savename, train_dataset, validation_dataset)\n",
    "    \n",
    "    # image_dataset = image_dataset.batch(BATCH_SIZE)\n",
    "    # test(model, image_dataset, output_path, 1300)\n",
    "    # name = 'error2' + '_postCrV' + '.csv'\n",
    "    error_path = './errors/'\n",
    "    # name = 'error_exped_iou' + '.csv'\n",
    "    # name = os.path.join(error_path, name)\n",
    "    # evalu.main(output_dir=output_path, label_dir=label_path, name=name, oder=np.array(0))\n",
    "    \n",
    "#     测试集\n",
    "    test_path = '/data/caizhizheng/2D/v2/test/test2/data/'\n",
    "    test_label_path = '/data/caizhizheng/2D/v2/test/test2/label/'\n",
    "    output_path = '/data/caizhizheng/2D/v2/test/outputv2/'\n",
    "    img1_list = getimg(test_path,num=201, j =0)\n",
    "    img2_list = getimg(test_path,num=201, j =200)\n",
    "    img_list = np.stack((img1_list,img2_list),axis = -1)\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices(img_list)\n",
    "    test_dataset = test_dataset.batch(1)\n",
    "    model.load_weights('/code/save_model/exped/'+model_savename)\n",
    "    test(model, test_dataset, output_path,200)\n",
    "    name = 'error_exped_iou_testv2.csv'\n",
    "    name = os.path.join(error_path, name)\n",
    "    evalu.main(output_dir=output_path, label_dir=test_label_path, name=name, oder=np.array(0))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a1e1af-190f-451e-a168-25669478757d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
